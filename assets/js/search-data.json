{
  
    
        "post0": {
            "title": "Working with string methods in pandas",
            "content": "One of the most useful features of pandas is its ability to take a bit of code that looks like it operates on a single bit of data, and have it operate on a whole bunch of different values. . . Tip: For a much more detailed explanation vectorization and lots more string processing examples, checkout the chapter on working with groups in the Drawing from Data book. . To see an example, let&#39;s load a dataset that shows fuel efficiency for different car models: . import pandas as pd pd.options.display.max_rows = 10 df = pd.read_csv( &quot;https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mpg.csv&quot; ) df . mpg cylinders displacement horsepower weight acceleration model_year origin name . 0 18.0 | 8 | 307.0 | 130.0 | 3504 | 12.0 | 70 | usa | chevrolet chevelle malibu | . 1 15.0 | 8 | 350.0 | 165.0 | 3693 | 11.5 | 70 | usa | buick skylark 320 | . 2 18.0 | 8 | 318.0 | 150.0 | 3436 | 11.0 | 70 | usa | plymouth satellite | . 3 16.0 | 8 | 304.0 | 150.0 | 3433 | 12.0 | 70 | usa | amc rebel sst | . 4 17.0 | 8 | 302.0 | 140.0 | 3449 | 10.5 | 70 | usa | ford torino | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 393 27.0 | 4 | 140.0 | 86.0 | 2790 | 15.6 | 82 | usa | ford mustang gl | . 394 44.0 | 4 | 97.0 | 52.0 | 2130 | 24.6 | 82 | europe | vw pickup | . 395 32.0 | 4 | 135.0 | 84.0 | 2295 | 11.6 | 82 | usa | dodge rampage | . 396 28.0 | 4 | 120.0 | 79.0 | 2625 | 18.6 | 82 | usa | ford ranger | . 397 31.0 | 4 | 119.0 | 82.0 | 2720 | 19.4 | 82 | usa | chevy s-10 | . 398 rows × 9 columns . Each row is a car, and for each car we have miles per gallon (mpg), plus a bunch of information about the engine and car, and finally the name. Say we want to convert the horsepower column into Kilowatts (kW). A quick trip to Google shows that one horsepower equals 0.7457 kW, so we might just iterate over the values and convert them like this: . kilowatts = [] for horsepower in df[&#39;horsepower&#39;]: kilowatts.append(horsepower * 0.7457) kilowatts[:10] . [96.941, 123.04050000000001, 111.855, 111.855, 104.39800000000001, 147.64860000000002, 164.054, 160.3255, 167.7825, 141.683] . However, the magic of pandas allows us to do this instead: . df[&#39;horsepower&#39;] * 0.7457 . 0 96.9410 1 123.0405 2 111.8550 3 111.8550 4 104.3980 ... 393 64.1302 394 38.7764 395 62.6388 396 58.9103 397 61.1474 Name: horsepower, Length: 398, dtype: float64 . We just write the expression as if we were trying to convert a single number, and pandas takes care of applying it to the whole series. This is called vectorization, and it&#39;s generally faster and more convenient than writing for loops. . However, we often run into problems when we try to use the same technique with text data. Let&#39;s get a list of the manufacturers for each car, which we can do just by getting the word before the first space in the name field. . First we&#39;ll use a loop: . manufacturers = [] for model in df[&#39;name&#39;]: manufacturers.append(model.split(&#39; &#39;)[0]) manufacturers[:10] . [&#39;chevrolet&#39;, &#39;buick&#39;, &#39;plymouth&#39;, &#39;amc&#39;, &#39;ford&#39;, &#39;ford&#39;, &#39;chevrolet&#39;, &#39;plymouth&#39;, &#39;pontiac&#39;, &#39;amc&#39;] . So far so good. Now, can we use the same trick as we did earlier and write the expression as if we&#39;re calculating it for a single value? . df[&#39;name&#39;].split(&#39; &#39;)[0] . AttributeError Traceback (most recent call last) &lt;ipython-input-5-453444d0ed9d&gt; in &lt;module&gt; -&gt; 1 df[&#39;name&#39;].split(&#39; &#39;)[0] ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/generic.py in __getattr__(self, name) 5128 if self._info_axis._can_hold_identifiers_and_holds_name(name): 5129 return self[name] -&gt; 5130 return object.__getattribute__(self, name) 5131 5132 def __setattr__(self, name: str, value) -&gt; None: AttributeError: &#39;Series&#39; object has no attribute &#39;split&#39; . The magic doesn&#39;t work. To put it in simple terms, a pandas Series object &quot;knows&quot; about the * operator, but it doesn&#39;t know anything about split(). If we think about it, we can see why this must be the case. A Series object has to be able to hold either numbers or text, so it doesn&#39;t make sense for it to have methods like split() that only work on text. . One way round this is to switch to apply(), which lets us run arbitrary code on values in a series. We can write a function that takes a single name and returns the manufacturer: . def get_manufacturer(name): return name.split(&#39; &#39;)[0] . Then apply it to the name column: . df[&#39;name&#39;].apply(get_manufacturer) . 0 chevrolet 1 buick 2 plymouth 3 amc 4 ford ... 393 ford 394 vw 395 dodge 396 ford 397 chevy Name: name, Length: 398, dtype: object . But a better way is to use the series str attribute. This attribute has nearly all of the string methods that we might be used to, plus a bunch more functionality specific to pandas. Let&#39;s start with a simpler example - say we want to change all the names to upper case. Here it is with apply(): . df[&#39;name&#39;].apply(str.upper) . 0 CHEVROLET CHEVELLE MALIBU 1 BUICK SKYLARK 320 2 PLYMOUTH SATELLITE 3 AMC REBEL SST 4 FORD TORINO ... 393 FORD MUSTANG GL 394 VW PICKUP 395 DODGE RAMPAGE 396 FORD RANGER 397 CHEVY S-10 Name: name, Length: 398, dtype: object . And here is is using the str attribute: . df[&#39;name&#39;].str.upper() . 0 CHEVROLET CHEVELLE MALIBU 1 BUICK SKYLARK 320 2 PLYMOUTH SATELLITE 3 AMC REBEL SST 4 FORD TORINO ... 393 FORD MUSTANG GL 394 VW PICKUP 395 DODGE RAMPAGE 396 FORD RANGER 397 CHEVY S-10 Name: name, Length: 398, dtype: object . Going back to our original problem, we can split the name into a list of strings like this: . df[&#39;name&#39;].str.split(&#39; &#39;) . 0 [chevrolet, chevelle, malibu] 1 [buick, skylark, 320] 2 [plymouth, satellite] 3 [amc, rebel, sst] 4 [ford, torino] ... 393 [ford, mustang, gl] 394 [vw, pickup] 395 [dodge, rampage] 396 [ford, ranger] 397 [chevy, s-10] Name: name, Length: 398, dtype: object . Now there&#39;s one final complication before we can get just the first word - the result of the expression above is itself a series. So we have to access its str attribute again before we can use square brackets to get just the first word: . df[&#39;name&#39;].str.split(&#39; &#39;).str[0] . 0 chevrolet 1 buick 2 plymouth 3 amc 4 ford ... 393 ford 394 vw 395 dodge 396 ford 397 chevy Name: name, Length: 398, dtype: object . Now we have a series which contains the data we want. At this point we could store it in a new column: . df[&#39;manufacturer&#39;] = df[&#39;name&#39;].str.split(&#39; &#39;).str[0] df . mpg cylinders displacement horsepower weight acceleration model_year origin name manufacturer . 0 18.0 | 8 | 307.0 | 130.0 | 3504 | 12.0 | 70 | usa | chevrolet chevelle malibu | chevrolet | . 1 15.0 | 8 | 350.0 | 165.0 | 3693 | 11.5 | 70 | usa | buick skylark 320 | buick | . 2 18.0 | 8 | 318.0 | 150.0 | 3436 | 11.0 | 70 | usa | plymouth satellite | plymouth | . 3 16.0 | 8 | 304.0 | 150.0 | 3433 | 12.0 | 70 | usa | amc rebel sst | amc | . 4 17.0 | 8 | 302.0 | 140.0 | 3449 | 10.5 | 70 | usa | ford torino | ford | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 393 27.0 | 4 | 140.0 | 86.0 | 2790 | 15.6 | 82 | usa | ford mustang gl | ford | . 394 44.0 | 4 | 97.0 | 52.0 | 2130 | 24.6 | 82 | europe | vw pickup | vw | . 395 32.0 | 4 | 135.0 | 84.0 | 2295 | 11.6 | 82 | usa | dodge rampage | dodge | . 396 28.0 | 4 | 120.0 | 79.0 | 2625 | 18.6 | 82 | usa | ford ranger | ford | . 397 31.0 | 4 | 119.0 | 82.0 | 2720 | 19.4 | 82 | usa | chevy s-10 | chevy | . 398 rows × 10 columns . Or do any other type of series processing: . df[&#39;name&#39;].str.split(&#39; &#39;).str[0].value_counts() . ford 51 chevrolet 43 plymouth 31 amc 28 dodge 28 .. hi 1 chevroelt 1 nissan 1 vokswagen 1 toyouta 1 Name: name, Length: 37, dtype: int64 . This technique is also very useful for filtering. Say we want to find just the cars made by Ford. We can use the startswith method of the str attribute to get a series of boolean values: . df[&#39;name&#39;].str.startswith(&#39;ford&#39;) . 0 False 1 False 2 False 3 False 4 True ... 393 True 394 False 395 False 396 True 397 False Name: name, Length: 398, dtype: bool . Which we can then use as a filter mask: . df[df[&#39;name&#39;].str.startswith(&#39;ford&#39;)] . mpg cylinders displacement horsepower weight acceleration model_year origin name manufacturer . 4 17.0 | 8 | 302.0 | 140.0 | 3449 | 10.5 | 70 | usa | ford torino | ford | . 5 15.0 | 8 | 429.0 | 198.0 | 4341 | 10.0 | 70 | usa | ford galaxie 500 | ford | . 17 21.0 | 6 | 200.0 | 85.0 | 2587 | 16.0 | 70 | usa | ford maverick | ford | . 25 10.0 | 8 | 360.0 | 215.0 | 4615 | 14.0 | 70 | usa | ford f250 | ford | . 32 25.0 | 4 | 98.0 | NaN | 2046 | 19.0 | 71 | usa | ford pinto | ford | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 365 20.2 | 6 | 200.0 | 88.0 | 3060 | 17.1 | 81 | usa | ford granada gl | ford | . 373 24.0 | 4 | 140.0 | 92.0 | 2865 | 16.4 | 82 | usa | ford fairmont futura | ford | . 389 22.0 | 6 | 232.0 | 112.0 | 2835 | 14.7 | 82 | usa | ford granada l | ford | . 393 27.0 | 4 | 140.0 | 86.0 | 2790 | 15.6 | 82 | usa | ford mustang gl | ford | . 396 28.0 | 4 | 120.0 | 79.0 | 2625 | 18.6 | 82 | usa | ford ranger | ford | . 51 rows × 10 columns . There are many useful methods hiding in the str attribute. Let&#39;s find all the cars with names longer than 30 characters, which we can do with the len() method: . long_name_cars = df[df[&#39;name&#39;].str.len() &gt; 30] long_name_cars . mpg cylinders displacement horsepower weight acceleration model_year origin name manufacturer . 73 13.0 | 8 | 307.0 | 130.0 | 4098 | 14.0 | 72 | usa | chevrolet chevelle concours (sw) | chevrolet | . 133 16.0 | 6 | 250.0 | 100.0 | 3781 | 17.0 | 74 | usa | chevrolet chevelle malibu classic | chevrolet | . 187 17.5 | 8 | 305.0 | 140.0 | 4215 | 13.0 | 76 | usa | chevrolet chevelle malibu classic | chevrolet | . 244 43.1 | 4 | 90.0 | 48.0 | 1985 | 21.5 | 78 | europe | volkswagen rabbit custom diesel | volkswagen | . 249 19.9 | 8 | 260.0 | 110.0 | 3365 | 15.5 | 78 | usa | oldsmobile cutlass salon brougham | oldsmobile | . 263 17.7 | 6 | 231.0 | 165.0 | 3445 | 13.4 | 78 | usa | buick regal sport coupe (turbo) | buick | . 292 18.5 | 8 | 360.0 | 150.0 | 3940 | 13.0 | 79 | usa | chrysler lebaron town @ country (sw) | chrysler | . 300 23.9 | 8 | 260.0 | 90.0 | 3420 | 22.2 | 79 | usa | oldsmobile cutlass salon brougham | oldsmobile | . 387 38.0 | 6 | 262.0 | 85.0 | 3015 | 17.0 | 82 | usa | oldsmobile cutlass ciera (diesel) | oldsmobile | . Here&#39;s a common annoyance when making charts with long axis labels: . import matplotlib.pyplot as plt plt.figure(figsize=(8,8)) long_name_cars.set_index(&#39;name&#39;)[&#39;horsepower&#39;].plot(kind=&#39;barh&#39;) None . The long labels take up a bunch of space on the left hand side of the chart. We can fix this by using the .wrap() method to split the names over multiple lines: . plt.figure(figsize=(8,8)) long_name_cars[&#39;display_name&#39;] = long_name_cars[&#39;name&#39;].str.wrap(15) long_name_cars.set_index(&#39;display_name&#39;)[&#39;horsepower&#39;].plot(kind=&#39;barh&#39;) None . One last example: a bunch of the car names end with the string &quot;(sw)&quot;. Let&#39;s find them: . df[df[&#39;name&#39;].str.endswith(&#39;(sw)&#39;)] . mpg cylinders displacement horsepower weight acceleration model_year origin name manufacturer . 13 14.0 | 8 | 455.0 | 225.0 | 3086 | 10.0 | 70 | usa | buick estate wagon (sw) | buick | . 42 12.0 | 8 | 383.0 | 180.0 | 4955 | 11.5 | 71 | usa | dodge monaco (sw) | dodge | . 43 13.0 | 8 | 400.0 | 170.0 | 4746 | 12.0 | 71 | usa | ford country squire (sw) | ford | . 44 13.0 | 8 | 400.0 | 175.0 | 5140 | 12.0 | 71 | usa | pontiac safari (sw) | pontiac | . 45 18.0 | 6 | 258.0 | 110.0 | 2962 | 13.5 | 71 | usa | amc hornet sportabout (sw) | amc | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 289 16.9 | 8 | 350.0 | 155.0 | 4360 | 14.9 | 79 | usa | buick estate wagon (sw) | buick | . 290 15.5 | 8 | 351.0 | 142.0 | 4054 | 14.3 | 79 | usa | ford country squire (sw) | ford | . 291 19.2 | 8 | 267.0 | 125.0 | 3605 | 15.0 | 79 | usa | chevrolet malibu classic (sw) | chevrolet | . 292 18.5 | 8 | 360.0 | 150.0 | 3940 | 13.0 | 79 | usa | chrysler lebaron town @ country (sw) | chrysler | . 340 25.8 | 4 | 156.0 | 92.0 | 2620 | 14.4 | 81 | usa | dodge aries wagon (sw) | dodge | . 28 rows × 10 columns . The &quot;sw&quot; stands for &quot;Station Wagon&quot;. Let&#39;s replace the abbreviation with the full name: . df[&#39;name&#39;] = df[&#39;name&#39;].str.replace(&quot; (sw )&quot;, &quot;(Station Wagon)&quot;) df[40:50] . mpg cylinders displacement horsepower weight acceleration model_year origin name manufacturer . 40 14.0 | 8 | 351.0 | 153.0 | 4154 | 13.5 | 71 | usa | ford galaxie 500 | ford | . 41 14.0 | 8 | 318.0 | 150.0 | 4096 | 13.0 | 71 | usa | plymouth fury iii | plymouth | . 42 12.0 | 8 | 383.0 | 180.0 | 4955 | 11.5 | 71 | usa | dodge monaco (Station Wagon) | dodge | . 43 13.0 | 8 | 400.0 | 170.0 | 4746 | 12.0 | 71 | usa | ford country squire (Station Wagon) | ford | . 44 13.0 | 8 | 400.0 | 175.0 | 5140 | 12.0 | 71 | usa | pontiac safari (Station Wagon) | pontiac | . 45 18.0 | 6 | 258.0 | 110.0 | 2962 | 13.5 | 71 | usa | amc hornet sportabout (Station Wagon) | amc | . 46 22.0 | 4 | 140.0 | 72.0 | 2408 | 19.0 | 71 | usa | chevrolet vega (Station Wagon) | chevrolet | . 47 19.0 | 6 | 250.0 | 100.0 | 3282 | 15.0 | 71 | usa | pontiac firebird | pontiac | . 48 18.0 | 6 | 250.0 | 88.0 | 3139 | 14.5 | 71 | usa | ford mustang | ford | . 49 23.0 | 4 | 122.0 | 86.0 | 2220 | 14.0 | 71 | usa | mercury capri 2000 | mercury | . Notice that we have to escape the parentheses in the &quot;(sw)&quot; string, as the replace() method works on regular expressions. . There are many other useful string processing methods in the pandas str attribute. The documentation page has a bunch of examples, and a method summary at the end. So before you set out to process strings either by writing a loop, or by using apply(), check to see if there&#39;s a method there that will do what you want! .",
            "url": "https://drawingfromdata.com/pandas/strings/2020/12/01/working-with-pandas-string-methods.html",
            "relUrl": "/pandas/strings/2020/12/01/working-with-pandas-string-methods.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Setting the size of a figure in matplotlib and seaborn",
            "content": "TL;DR . if you&#39;re using plot() on a pandas Series or Dataframe, use the figsize keyword | if you&#39;re using matplotlib directly, use matplotlib.pyplot.figure with the figsize keyword | if you&#39;re using a seaborn function that draws a single plot, use matplotlib.pyplot.figure with the figsize keyword | if you&#39;re using a seaborn function that draws multiple plots, use the height and aspect keyword arguments | . Introduction . Setting figure sizes is one of those things that feels like it should be very straightforward. However, it still manages to show up on the first page of stackoverflow questions for both matplotlib and seaborn. Part of the confusion arises because there are so many ways to do the same thing - this highly upvoted question has six suggested solutions: . manually create an Axes object with the desired size | pass some configuration paramteters to seaborn so that the size you want is the default | call a method on the figure once it&#39;s been created | pass hight and aspect keywords to the seaborn plotting function | use the matplotlib.pyplot interface and call the figure() function | use the matplotlib.pyplot interface to get the current figure then set its size using a method | . each of which will work in some circumstances but not others! . . Tip: For a full overview of the difference between figure and axis level charts, checkout the chapters on styles and small multiples in the Drawing from Data book. . Drawing a figure using pandas . Let&#39;s jump in. As an example we&#39;ll use the olympic medal dataset, which we can load directly from a URL:: . import pandas as pd data = pd.read_csv(&quot;https://raw.githubusercontent.com/mojones/binders/master/olympics.csv&quot;, sep=&quot; t&quot;) data . City Year Sport ... Medal Country Int Olympic Committee code . 0 Athens | 1896 | Aquatics | ... | Gold | Hungary | HUN | . 1 Athens | 1896 | Aquatics | ... | Silver | Austria | AUT | . 2 Athens | 1896 | Aquatics | ... | Bronze | Greece | GRE | . 3 Athens | 1896 | Aquatics | ... | Gold | Greece | GRE | . 4 Athens | 1896 | Aquatics | ... | Silver | Greece | GRE | . ... ... | ... | ... | ... | ... | ... | ... | . 29211 Beijing | 2008 | Wrestling | ... | Silver | Germany | GER | . 29212 Beijing | 2008 | Wrestling | ... | Bronze | Lithuania | LTU | . 29213 Beijing | 2008 | Wrestling | ... | Bronze | Armenia | ARM | . 29214 Beijing | 2008 | Wrestling | ... | Gold | Cuba | CUB | . 29215 Beijing | 2008 | Wrestling | ... | Silver | Russia | RUS | . 29216 rows × 12 columns . For our first figure, we&#39;ll count how many medals have been won in total by each country, then take the top thirty: . data[&#39;Country&#39;].value_counts().head(30) . United States 4335 Soviet Union 2049 United Kingdom 1594 France 1314 Italy 1228 ... Spain 377 Switzerland 376 Brazil 372 Bulgaria 331 Czechoslovakia 329 Name: Country, Length: 30, dtype: int64 . And turn it into a bar chart: . data[&#39;Country&#39;].value_counts().head(30).plot(kind=&#39;barh&#39;) . &lt;AxesSubplot:&gt; . Ignoring other asthetic aspects of the plot, it&#39;s obvious that we need to change the size - or rather the shape. Part of the confusion over sizes in plotting is that sometimes we need to just make the chart bigger or smaller, and sometimes we need to make it thinner or fatter. If we just scaled up this plot so that it was big enough to read the names on the vertical axis, then it would also be very wide. We can set the size by adding a figsize keyword argument to our pandas plot() function. The value has to be a tuple of sizes - it&#39;s actually the horizontal and vertical size in inches, but for most purposes we can think of them as arbirary units. . Here&#39;s what happens if we make the plot bigger, but keep the original shape: . data[&#39;Country&#39;].value_counts().head(30).plot(kind=&#39;barh&#39;, figsize=(20,10)) . &lt;AxesSubplot:&gt; . And here&#39;s a version that keeps the large vertical size but shrinks the chart horizontally so it doesn&#39;t take up so much space: . data[&#39;Country&#39;].value_counts().head(30).plot(kind=&#39;barh&#39;, figsize=(6,10)) . &lt;AxesSubplot:&gt; . Drawing a figure using matplotlib . OK, but what if we aren&#39;t using pandas&#39; convenient plot() method but drawing the chart using matplotlib directly? Let&#39;s look at the number of medals awarded in each year: . plt.plot(data[&#39;Year&#39;].value_counts().sort_index()) . [&lt;matplotlib.lines.Line2D at 0x7fb1bd16de20&gt;] . This time, we&#39;ll say that we want to make the plot longer in the horizontal direction, to better see the pattern over time. If we search the documentation for the matplotlib plot() funtion, we won&#39;t find any mention of size or shape. This actually makes sense in the design of matplotlib - plots don&#39;t really have a size, figures do. So to change it we have to call the figure() function: . import matplotlib.pyplot as plt plt.figure(figsize=(15,4)) plt.plot(data[&#39;Year&#39;].value_counts().sort_index()) . [&lt;matplotlib.lines.Line2D at 0x7fb1bd5f3dc0&gt;] . Notice that with the figure() function we have to call it before we make the call to plot(), otherwise it won&#39;t take effect: . plt.plot(data[&#39;Year&#39;].value_counts().sort_index()) # no effect, the plot has already been drawn plt.figure(figsize=(15,4)) . &lt;Figure size 1080x288 with 0 Axes&gt; . &lt;Figure size 1080x288 with 0 Axes&gt; . Drawing a figure with seaborn . OK, now what if we&#39;re using seaborn rather than matplotlib? Well, happily the same technique will work. We know from our first plot which countries have won the most medals overall, but now let&#39;s look at how this varies by year. We&#39;ll create a summary table to show the number of medals per year for all countries that have won at least 500 medals total. . (ignore this panda stuff if it seems confusing, and just look at the final table) . summary = ( data .groupby(&#39;Country&#39;) .filter(lambda x : len(x) &gt; 500) .groupby([&#39;Country&#39;, &#39;Year&#39;]) .size() .to_frame(&#39;medal count&#39;) .reset_index() ) # wrap long country names summary[&#39;Country&#39;] = summary[&#39;Country&#39;].str.replace(&#39; &#39;, &#39; n&#39;) summary . Country Year medal count . 0 Australia | 1896 | 2 | . 1 Australia | 1900 | 5 | . 2 Australia | 1920 | 6 | . 3 Australia | 1924 | 10 | . 4 Australia | 1928 | 4 | . ... ... | ... | ... | . 309 United nStates | 1992 | 224 | . 310 United nStates | 1996 | 260 | . 311 United nStates | 2000 | 248 | . 312 United nStates | 2004 | 264 | . 313 United nStates | 2008 | 315 | . 314 rows × 3 columns . Now we can do a box plot to show the distribution of yearly medal totals for each country: . import seaborn as sns sns.boxplot( data=summary, x=&#39;Country&#39;, y=&#39;medal count&#39;, color=&#39;red&#39;) . &lt;AxesSubplot:xlabel=&#39;Country&#39;, ylabel=&#39;medal count&#39;&gt; . This is hard to read because of all the names, so let&#39;s space them out a bit: . plt.figure(figsize=(20,5)) sns.boxplot( data=summary, x=&#39;Country&#39;, y=&#39;medal count&#39;, color=&#39;red&#39;) . &lt;AxesSubplot:xlabel=&#39;Country&#39;, ylabel=&#39;medal count&#39;&gt; . Now we come to the final complication; let&#39;s say we want to look at the distributions of the different medal types separately. We&#39;ll make a new summary table - again, ignore the pandas stuff if it&#39;s confusing, and just look at the final table: . summary_by_medal = ( data .groupby(&#39;Country&#39;) .filter(lambda x : len(x) &gt; 500) .groupby([&#39;Country&#39;, &#39;Year&#39;, &#39;Medal&#39;]) .size() .to_frame(&#39;medal count&#39;) .reset_index() ) summary_by_medal[&#39;Country&#39;] = summary_by_medal[&#39;Country&#39;].str.replace(&#39; &#39;, &#39; n&#39;) summary_by_medal . Country Year Medal medal count . 0 Australia | 1896 | Gold | 2 | . 1 Australia | 1900 | Bronze | 3 | . 2 Australia | 1900 | Gold | 2 | . 3 Australia | 1920 | Bronze | 1 | . 4 Australia | 1920 | Silver | 5 | . ... ... | ... | ... | ... | . 881 United nStates | 2004 | Gold | 116 | . 882 United nStates | 2004 | Silver | 75 | . 883 United nStates | 2008 | Bronze | 81 | . 884 United nStates | 2008 | Gold | 125 | . 885 United nStates | 2008 | Silver | 109 | . 886 rows × 4 columns . Now we will switch from boxplot() to the higher level catplot(), as this makes it easy to switch between different plot types. But notice that now our call to plt.figure() gets ignored: . plt.figure(figsize=(20,5)) sns.catplot( data=summary_by_medal, x=&#39;Country&#39;, y=&#39;medal count&#39;, hue=&#39;Medal&#39;, kind=&#39;box&#39; ) . &lt;seaborn.axisgrid.FacetGrid at 0x7fb1bcc23ac0&gt; . &lt;Figure size 1440x360 with 0 Axes&gt; . The reason for this is that the higher level plotting functions in seaborn (what the documentation calls Figure-level interfaces) have a different way of managing size, largely due to the fact that the often produce multiple subplots. To set the size when using catplot() or relplot() (also pairplot(), lmplot() and jointplot()), use the height keyword to control the size and the aspect keyword to control the shape: . sns.catplot( data=summary_by_medal, x=&#39;Country&#39;, y=&#39;medal count&#39;, hue=&#39;Medal&#39;, kind=&#39;box&#39;, height=5, # make the plot 5 units high aspect=3) # height should be three times width . &lt;seaborn.axisgrid.FacetGrid at 0x7fb1bc40d850&gt; . Because we often end up drawing small multiples with catplot() and relplot(), being able to control the shape separately from the size is very convenient. The height and aspect keywords apply to each subplot separately, not to the figure as a whole. So if we put each medal on a separate row rather than using hue, we&#39;ll end up with three subplots, so we&#39;ll want to set the height to be smaller, but the aspect ratio to be bigger: . sns.catplot( data=summary_by_medal, x=&#39;Country&#39;, y=&#39;medal count&#39;, row=&#39;Medal&#39;, kind=&#39;box&#39;, height=3, aspect=4, color=&#39;blue&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7fb1bc04edf0&gt; . Printing a figure . Finally, a word about printing. If the reason that you need to change the size of a plot, rather than the shape, is because you need to print it, then don&#39;t worry about the size - get the shape that you want, then use savefig() to make the plot in SVG format: . plt.savefig(&#39;medals.svg&#39;) . &lt;Figure size 432x288 with 0 Axes&gt; . This will give you a plot in Scalable Vector Graphics format, which stores the actual lines and shapes of the chart so that you can print it at any size - even a giant poster - and it will look sharp. As a nice bonus, you can also edit individual bits of the chart using a graphical SVG editor (Inkscape is free and powerful, though takes a bit of effort to learn). .",
            "url": "https://drawingfromdata.com/pandas/seaborn/matplotlib/visualization/2020/12/01/setting-figure-size-matplotlib-seaborn.html",
            "relUrl": "/pandas/seaborn/matplotlib/visualization/2020/12/01/setting-figure-size-matplotlib-seaborn.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Rotating axis labels in matplotlib and seaborn",
            "content": "There&#39;s a common pattern which often occurs when working with charting libraries: drawing charts with all the defaults seems very straightforward, but when we want to change some aspect of the chart things get complicated. This pattern is even more noticable when working with a high-level library like seaborn - the library does all sorts of clever things to make our life easier, and lets us draw sophisticated, beautiful charts, so it&#39;s frustrating when we want to change something that feels like it should be simple. . . Tip: For a full overview of styling charts, check out the chapters on styles and color in the Drawing from Data book. . In this article, we&#39;ll take a look at the classic example of this phenomenon - rotating axis tick labels. This seems like such a common thing that it should be easy, but it&#39;s one of the most commonly asked questions on StackOverflow for both seaborn and matplotlib. As an example dataset, we&#39;ll look at a table of Olympic medal winners. We can load it into pandas directly from a URL: . import pandas as pd data = pd.read_csv(&quot;https://raw.githubusercontent.com/mojones/binders/master/olympics.csv&quot;, sep=&quot; t&quot;) data . City Year Sport ... Medal Country Int Olympic Committee code . 0 Athens | 1896 | Aquatics | ... | Gold | Hungary | HUN | . 1 Athens | 1896 | Aquatics | ... | Silver | Austria | AUT | . 2 Athens | 1896 | Aquatics | ... | Bronze | Greece | GRE | . 3 Athens | 1896 | Aquatics | ... | Gold | Greece | GRE | . 4 Athens | 1896 | Aquatics | ... | Silver | Greece | GRE | . ... ... | ... | ... | ... | ... | ... | ... | . 29211 Beijing | 2008 | Wrestling | ... | Silver | Germany | GER | . 29212 Beijing | 2008 | Wrestling | ... | Bronze | Lithuania | LTU | . 29213 Beijing | 2008 | Wrestling | ... | Bronze | Armenia | ARM | . 29214 Beijing | 2008 | Wrestling | ... | Gold | Cuba | CUB | . 29215 Beijing | 2008 | Wrestling | ... | Silver | Russia | RUS | . 29216 rows × 12 columns . Each row is a single medal, and we have a bunch of different information like where and when the event took place, the classification of the event, and the name of the athlete that won. . We&#39;ll start with something simple; let&#39;s grab all the events for the 1980 games and see how many fall into each type of sport: . import seaborn as sns import matplotlib.pyplot as plt # set the figure size plt.figure(figsize=(10,5)) # draw the chart chart = sns.countplot( data=data[data[&#39;Year&#39;] == 1980], x=&#39;Sport&#39;, palette=&#39;Set1&#39; ) . Here we have the classic problem with categorical data: we need to display all the labels and because some of them are quite long, they overlap. How are we going to rotate them? The key is to look at what type of object we&#39;ve created. What is the type of the return value from the countplot() function, which we have stored in chart? . type(chart) . matplotlib.axes._subplots.AxesSubplot . Looks like chart is a matplotlib AxesSubplot object. This actually doesn&#39;t help us very much - if we go searching for the documentation for AxesSubplot we won&#39;t find anything useful. Instead, we have to know that an AxesSubplot is a type of Axes object, and now we can go look up the documentation for Axes in which we find the set_xticklabels() method. . Looking at the documentation for set_xticklabels() we don&#39;t actually see any obvious reference to rotation. The clue we&#39;re looking for is in the &quot;Other parameters&quot; section at the end, where it tells us that we can supply a list of keyword arguments that are properties of Text objects. . Finally, in the documentation for Text objects we can see a list of the properties, including rotation. This was a long journey! but hopefully it will pay off - there are lots of other useful properties here as well. Now we can finally set the rotation: . plt.figure(figsize=(10,5)) chart = sns.countplot( data=data[data[&#39;Year&#39;] == 1980], x=&#39;Sport&#39;, palette=&#39;Set1&#39; ) chart.set_xticklabels(rotation=45) . TypeError Traceback (most recent call last) &lt;ipython-input-27-059eaf6ffa77&gt; in &lt;module&gt; 5 palette=&#39;Set1&#39; 6 ) -&gt; 7 chart.set_xticklabels(rotation=45) ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/matplotlib/axes/_base.py in wrapper(self, *args, **kwargs) 61 62 def wrapper(self, *args, **kwargs): &gt; 63 return get_method(self)(*args, **kwargs) 64 65 wrapper.__module__ = owner.__module__ ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs) 449 &#34;parameter will become keyword-only %(removal)s.&#34;, 450 name=name, obj_type=f&#34;parameter of {func.__name__}()&#34;) --&gt; 451 return func(*args, **kwargs) 452 453 return wrapper TypeError: _set_ticklabels() missing 1 required positional argument: &#39;labels&#39; . Disaster! We need to pass set_xticklabels() a list of the actual labels we want to use. Since we don&#39;t want to change the labels themselves, we can just call get_xticklabels(): . plt.figure(figsize=(10,5)) chart = sns.countplot( data=data[data[&#39;Year&#39;] == 1980], x=&#39;Sport&#39;, palette=&#39;Set1&#39; ) chart.set_xticklabels(chart.get_xticklabels(), rotation=45) None #don&#39;t show the label objects . This looks better, but notice how the &quot;Modern Pentathlon&quot; label is running into the &quot;Sailing&quot; label? That&#39;s because the labels have been rotated about their center - which also makes it hard to see which label belongs to which bar. We should also set the horizontal alignment to &quot;right&quot;: . plt.figure(figsize=(10,5)) chart = sns.countplot( data=data[data[&#39;Year&#39;] == 1980], x=&#39;Sport&#39;, palette=&#39;Set1&#39; ) chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) None #don&#39;t show the label objects . And just to show a few more things that we can do with set_xticklabels() we&#39;ll also set the font weight to be a bit lighter, and the font size to be a bit bigger: . plt.figure(figsize=(10,5)) chart = sns.countplot( data=data[data[&#39;Year&#39;] == 1980], x=&#39;Sport&#39;, palette=&#39;Set1&#39; ) chart.set_xticklabels( chart.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;, fontweight=&#39;light&#39;, fontsize=&#39;x-large&#39; ) None #don&#39;t show the label objects . In all of these examples, we&#39;ve been using the object-oriented interface to matplotlib - notice that we&#39;re calling set_xticklabels() directly on the chart object. . Another object is to use the pyplot interface. There&#39;s a method simply called xticks() which we could use like this: . import matplotlib.pyplot as plt plt.figure(figsize=(10,5)) chart = sns.countplot( data=data[data[&#39;Year&#39;] == 1980], x=&#39;Sport&#39;, palette=&#39;Set1&#39; ) plt.xticks( rotation=45, horizontalalignment=&#39;right&#39;, fontweight=&#39;light&#39;, fontsize=&#39;x-large&#39; ) None #don&#39;t show the label objects . Notice that when we do it this way the list of labels is optional, so we don&#39;t need to call get_xticklabels(). . Althought the pyplot interface is easier to use in this case, in general I find it clearer to use the object-oriented interface, as it tends to be more explicit. . Everything that we&#39;ve seen above applies if we&#39;re using matplotlib directly instead of seaborn: once we have an Axes object, we can call set_xticklabels() on it. Let&#39;s do the same thing using pandas&#39;s built in plotting function: . chart = data[data[&#39;Year&#39;] == 1980][&#39;Sport&#39;].value_counts().plot(kind=&#39;bar&#39;) chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) None . Dealing with multiple plots . Let&#39;s try another plot. One of the great features of seaborn is that it makes it very easy to draw multiple plots. Let&#39;s see how the distribution of medals in each sport changed between 1980 and 2008: . chart = sns.catplot( data=data[data[&#39;Year&#39;].isin([1980, 2008])], x=&#39;Sport&#39;, kind=&#39;count&#39;, palette=&#39;Set1&#39;, row=&#39;Year&#39;, aspect=3, height=3 ) . As before, the labels need to be rotated. Let&#39;s try the approach that we used before: . chart = sns.catplot( data=data[data[&#39;Year&#39;].isin([1980, 2008])], x=&#39;Sport&#39;, kind=&#39;count&#39;, palette=&#39;Set1&#39;, row=&#39;Year&#39;, aspect=3, height=3 ) chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) . AttributeError Traceback (most recent call last) &lt;ipython-input-8-69ed9d536d8c&gt; in &lt;module&gt; 8 height=3 9 ) &gt; 10 chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) AttributeError: &#39;FacetGrid&#39; object has no attribute &#39;get_xticklabels&#39; . We run into an error. Note that the missing attribute is not set_xticklabels() but get_xticklabels(). The reason why this approach worked for countplot() and not for factorplot() is that the output from countplot() is a single Axes object, as we saw above, but the output from factorplot() is a seaborn FacetGrid object: . type(chart) . seaborn.axisgrid.FacetGrid . whose job is to store a collection of multiple axes - two in this case. So how to rotate the labels? It turns out that FacetGrid has its own version of set_xticklabels that will take care of things: . chart = sns.catplot( data=data[data[&#39;Year&#39;].isin([1980, 2008])], x=&#39;Sport&#39;, kind=&#39;count&#39;, palette=&#39;Set1&#39;, row=&#39;Year&#39;, aspect=3, height=3 ) chart.set_xticklabels(rotation=65, horizontalalignment=&#39;right&#39;) None . The pyplot interface that we saw earlier also works fine: . chart = sns.catplot( data=data[data[&#39;Year&#39;].isin([1980, 2008])], x=&#39;Sport&#39;, kind=&#39;count&#39;, palette=&#39;Set1&#39;, row=&#39;Year&#39;, aspect=3, height=3 ) plt.xticks(rotation=65, horizontalalignment=&#39;right&#39;) None . And, of course, everything that we&#39;ve done here will work for y-axis labels as well - we typically don&#39;t need to change their rotation, but we might want to set their other properties. As an example, let&#39;s count how many medals were won at each Olypmic games for each country in each year. To keep the dataset managable, we&#39;ll just look at countries that have won more than 500 metals in total: . by_sport = (data .groupby(&#39;Country&#39;) .filter(lambda x : len(x) &gt; 500) .groupby([&#39;Country&#39;, &#39;Year&#39;]) .size() .unstack() ) by_sport . Year 1896 1900 1904 ... 2000 2004 2008 . Country . Australia 2.0 | 5.0 | NaN | ... | 183.0 | 157.0 | 149.0 | . Canada NaN | 2.0 | 35.0 | ... | 31.0 | 17.0 | 34.0 | . China NaN | NaN | NaN | ... | 79.0 | 94.0 | 184.0 | . East Germany NaN | NaN | NaN | ... | NaN | NaN | NaN | . France 11.0 | 185.0 | NaN | ... | 66.0 | 53.0 | 76.0 | . ... ... | ... | ... | ... | ... | ... | ... | . Russia NaN | NaN | NaN | ... | 188.0 | 192.0 | 143.0 | . Soviet Union NaN | NaN | NaN | ... | NaN | NaN | NaN | . Sweden NaN | 1.0 | NaN | ... | 32.0 | 12.0 | 7.0 | . United Kingdom 7.0 | 78.0 | 2.0 | ... | 55.0 | 57.0 | 77.0 | . United States 20.0 | 55.0 | 394.0 | ... | 248.0 | 264.0 | 315.0 | . 17 rows × 26 columns . If the use of two groupby() method calls is confusing, take a look at this article on grouping. The first one just gives us the rows belonging to countries that have won more than 500 medals; the second one does the aggregation and fills in missing data. The natural way to display a table like this is as a heatmap: . plt.figure(figsize=(10,10)) g = sns.heatmap( by_sport, square=True, # make cells square cbar_kws={&#39;fraction&#39; : 0.01}, # shrink colour bar cmap=&#39;OrRd&#39;, # use orange/red colour map linewidth=1 # space between cells ) . This example is perfectly readable, but by way of an example we&#39;ll rotate both the x and y axis labels: . plt.figure(figsize=(10,10)) g = sns.heatmap( by_sport, square=True, cbar_kws={&#39;fraction&#39; : 0.01}, cmap=&#39;OrRd&#39;, linewidth=1 ) g.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) g.set_yticklabels(g.get_yticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) None # prevent the list of label objects showing up annoyingly in the output . OK, I think that covers it. That was an agonizingly long article to read just about rotating labels, but hopefully it&#39;s given you an insight into what&#39;s going on. It all comes down to understanding what type of object you&#39;re working with - an Axes, a FacetGrid, or a PairGrid. . If you encounter a situation where none of these work, drop me an email at martin@drawingwithdata.com and I&#39;ll update this article! .",
            "url": "https://drawingfromdata.com/seaborn/matplotlib/visualization/2020/12/01/rotate-axis-labels-matplotlib-seaborn.html",
            "relUrl": "/seaborn/matplotlib/visualization/2020/12/01/rotate-axis-labels-matplotlib-seaborn.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "When to use aggreagate/filter/transform with pandas",
            "content": "I&#39;ve been teaching quite a lot of Pandas recently, and a lot of the recurring questions are about grouping. That&#39;s no surprise, as it&#39;s one of the most flexible features of Pandas. However, that flexibility also makes it sometimes confusing. . Tip: For a much more detailed explanation of grouping operations, checkout the chapter on working with groups in the Drawing from Data book. . I think that most of the confusion arises because the same grouping logic is used for (at least) three distinct operations in Pandas. In the order that we normally learn them, these are: . calculating some aggregate measurement for each group (size, mean, etc.) | filtering the rows on a property of the group they belong to | calculating a new value for each row based on a property of the group. | . This leads commonly to situations where we know that we need to use groupby() - and may even be able to easily figure out what the arguments to groupby() should be - but are unsure about what to do next. . Here&#39;s a trick that I&#39;ve found useful when teaching these ideas: think about the result you want, and work back from there. If you want to get a single value for each group, use aggregate() (or one of its shortcuts). If you want to get a subset of the original rows, use filter(). And if you want to get a new value for each original row, use transpose(). . Here&#39;s a minimal example of the three different situations, all of which require exactly the same call to groupby() but which do different things with the result. We&#39;ll use the well known tips dataset which we can load directly from the web: . import pandas as pd df = pd.read_csv(&quot;https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv&quot;) pd.options.display.max_rows = 10 df . total_bill tip sex smoker day time size . 0 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | . 1 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | . 2 21.01 | 3.50 | Male | No | Sun | Dinner | 3 | . 3 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | . 4 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | . ... ... | ... | ... | ... | ... | ... | ... | . 239 29.03 | 5.92 | Male | No | Sat | Dinner | 3 | . 240 27.18 | 2.00 | Female | Yes | Sat | Dinner | 2 | . 241 22.67 | 2.00 | Male | Yes | Sat | Dinner | 2 | . 242 17.82 | 1.75 | Male | No | Sat | Dinner | 2 | . 243 18.78 | 3.00 | Female | No | Thur | Dinner | 2 | . 244 rows × 7 columns . If you&#39;re not familiar with this dataset, all you need to know is that each row represents a meal at a restaurant, and the columns store the value of the total bill and the tip, plus some metadata about the customer - their sex, whether or not they were a smoker, what day and time they ate at, and the size of their party. Also, notice that we have 244 rows - this will be important later on. . What was the average total bill on each day? . To answer this, let&#39;s imagine that we have already figured out that we need to group by day: . df.groupby(&#39;day&#39;) . now what&#39;s the next step? Use the trick that I just described and start by imagining what we want the output to look like. We want a single value for each group, so we need to use aggregate(): . df.groupby(&#39;day&#39;).aggregate(&#39;mean&#39;) . total_bill tip size . day . Fri 17.151579 | 2.734737 | 2.105263 | . Sat 20.441379 | 2.993103 | 2.517241 | . Sun 21.410000 | 3.255132 | 2.842105 | . Thur 17.682742 | 2.771452 | 2.451613 | . We&#39;re only interested in the total_bill column, so we can select it (either before or after we do the aggregation): . df.groupby(&#39;day&#39;)[&#39;total_bill&#39;].aggregate(&#39;mean&#39;) . day Fri 17.151579 Sat 20.441379 Sun 21.410000 Thur 17.682742 Name: total_bill, dtype: float64 . Pandas has lots of shortcuts for the various ways to aggregate group values - we could use mean() here instead: . df.groupby(&#39;day&#39;)[&#39;total_bill&#39;].mean() . day Fri 17.151579 Sat 20.441379 Sun 21.410000 Thur 17.682742 Name: total_bill, dtype: float64 . Which meals were eaten on days where the average bill was greater than 20? . For this question, think again about the output we want - our goal here is to get a subset of the original rows, so this is a job for filter(). The argument to filter() must be a function or lambda that will take a group and return True or False to determine whether rows belonging to that group should be included in the output. Here&#39;s how we might do it with a lambda: . df.groupby(&#39;day&#39;).filter(lambda x : x[&#39;total_bill&#39;].mean() &gt; 20) . total_bill tip sex smoker day time size . 0 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | . 1 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | . 2 21.01 | 3.50 | Male | No | Sun | Dinner | 3 | . 3 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | . 4 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | . ... ... | ... | ... | ... | ... | ... | ... | . 238 35.83 | 4.67 | Female | No | Sat | Dinner | 3 | . 239 29.03 | 5.92 | Male | No | Sat | Dinner | 3 | . 240 27.18 | 2.00 | Female | Yes | Sat | Dinner | 2 | . 241 22.67 | 2.00 | Male | Yes | Sat | Dinner | 2 | . 242 17.82 | 1.75 | Male | No | Sat | Dinner | 2 | . 163 rows × 7 columns . Notice that our output dataframe has only 163 rows (compared to the 244 that we started with), and that the columns are exactly the same as the input. . Compared to our first example, it&#39;s a bit harder to see why this is useful - typically we&#39;ll do a filter like this and then follow it up with another operation. For example, we might want to compare the average party size on days where the average bill is high: . # surrounding parens let us split the different parts of the expression # over multiple lines ( df .groupby(&#39;day&#39;) .filter( lambda x : x[&#39;total_bill&#39;].mean() &gt; 20) [&#39;size&#39;] .mean() ) . 2.668711656441718 . with the average party size on days where the average bill is low: . ( df .groupby(&#39;day&#39;) .filter(lambda x : x[&#39;total_bill&#39;].mean() &lt;= 20) [&#39;size&#39;] .mean() ) . 2.3703703703703702 . Incidentally, a question that I&#39;m often asked is what the type of the argument to the lambda is - what actually is the variable x in our examples above? We can find out by passing a lambda that just prints the type of its input: . df.groupby(&#39;day&#39;).filter(lambda x: print(type(x))) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; . total_bill tip sex smoker day time size . And we see that each group is passed to our lambda function as a Pandas DataFrame, so we already know how to use it. . How did the cost of each meal compare to the average for the day? . This last example is the trickiest to understand, but remember our trick - start by thinking about the desired output. In this case we are trying to generate a new value for each input row - the total bill divided by the average total bill for each day. (If you have a scientific or maths background then you might think of this as a normalized or scaled total bill). To make a new value for each row, we use transform(). . To start with, let&#39;s see what happens when we pass in a lambda to transform() that just gives us the mean of its input: . df.groupby(&#39;day&#39;).transform(lambda x : x.mean()) . total_bill tip size . 0 21.410000 | 3.255132 | 2.842105 | . 1 21.410000 | 3.255132 | 2.842105 | . 2 21.410000 | 3.255132 | 2.842105 | . 3 21.410000 | 3.255132 | 2.842105 | . 4 21.410000 | 3.255132 | 2.842105 | . ... ... | ... | ... | . 239 20.441379 | 2.993103 | 2.517241 | . 240 20.441379 | 2.993103 | 2.517241 | . 241 20.441379 | 2.993103 | 2.517241 | . 242 20.441379 | 2.993103 | 2.517241 | . 243 17.682742 | 2.771452 | 2.451613 | . 244 rows × 3 columns . Notice that we get the same number of output rows as input rows - Pandas has calculated the mean for each group, then used the results as the new values for each row. We&#39;re only interested in the total bill, so let&#39;s get rid of the other columns: . df.groupby(&#39;day&#39;)[&#39;total_bill&#39;].transform(lambda x : x.mean()) . 0 21.410000 1 21.410000 2 21.410000 3 21.410000 4 21.410000 ... 239 20.441379 240 20.441379 241 20.441379 242 20.441379 243 17.682742 Name: total_bill, Length: 244, dtype: float64 . This gives us a series with the same number of rows as our input data. We could assign this to a new column in our dataframe: . df[&#39;day_average&#39;] = df.groupby(&#39;day&#39;)[&#39;total_bill&#39;].transform(lambda x : x.mean()) df . total_bill tip sex smoker day time size day_average . 0 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | 21.410000 | . 1 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | 21.410000 | . 2 21.01 | 3.50 | Male | No | Sun | Dinner | 3 | 21.410000 | . 3 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | 21.410000 | . 4 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | 21.410000 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 239 29.03 | 5.92 | Male | No | Sat | Dinner | 3 | 20.441379 | . 240 27.18 | 2.00 | Female | Yes | Sat | Dinner | 2 | 20.441379 | . 241 22.67 | 2.00 | Male | Yes | Sat | Dinner | 2 | 20.441379 | . 242 17.82 | 1.75 | Male | No | Sat | Dinner | 2 | 20.441379 | . 243 18.78 | 3.00 | Female | No | Thur | Dinner | 2 | 17.682742 | . 244 rows × 8 columns . Which would allow us to calculate the scaled total bills: . df[&#39;total_bill&#39;] / df[&#39;day_average&#39;] . 0 0.793554 1 0.482952 2 0.981317 3 1.106025 4 1.148529 ... 239 1.420159 240 1.329656 241 1.109025 242 0.871761 243 1.062052 Length: 244, dtype: float64 . But we could also calculate the scaled bill as part of the transform: . df[&#39;scaled bill&#39;] = df.groupby(&#39;day&#39;)[&#39;total_bill&#39;].transform(lambda x : x/x.mean()) df.head() . total_bill tip sex smoker day time size day_average scaled bill . 0 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | 21.41 | 0.793554 | . 1 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | 21.41 | 0.482952 | . 2 21.01 | 3.50 | Male | No | Sun | Dinner | 3 | 21.41 | 0.981317 | . 3 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | 21.41 | 1.106025 | . 4 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | 21.41 | 1.148529 | . In conclusion . All of our three examples used exactly the same groupby() call to begin with: . df.groupby(&#39;day&#39;)[&#39;total_bill&#39;].mean() df.groupby(&#39;day&#39;).filter(lambda x : x[&#39;total_bill&#39;].mean() &gt; 20) df.groupby(&#39;day&#39;)[&#39;total_bill&#39;].transform(lambda x : x/x.mean()) . but by doing different things with the resulting groups we get very different outputs. To reiterate: . if we want to get a single value for each group -&gt; use aggregate() | if we want to get a subset of the input rows -&gt; use filter() | if we want to get a new value for each input row -&gt; use transform() | .",
            "url": "https://drawingfromdata.com/pandas/grouping/2020/12/01/pandas-groupby-transform-aggregate-filter.html",
            "relUrl": "/pandas/grouping/2020/12/01/pandas-groupby-transform-aggregate-filter.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Making a pairwise distance matrix in pandas",
            "content": "This is a somewhat specialized problem that forms part of a lot of data science and clustering workflows. It starts with a relatively straightforward question: if we have a bunch of measurements for two different things, how do we come up with a single number that represents the difference between the two things? . . Tip: For a deeper dive into distances and clustering, checkout the chapter on matrix charts in the Drawing from Data book. . An example will make the question clearer. Let&#39;s load our olympic medal dataset: . import pandas as pd data = pd.read_csv(&quot;https://raw.githubusercontent.com/mojones/binders/master/olympics.csv&quot;, sep=&quot; t&quot;) data . City Year Sport ... Medal Country Int Olympic Committee code . 0 Athens | 1896 | Aquatics | ... | Gold | Hungary | HUN | . 1 Athens | 1896 | Aquatics | ... | Silver | Austria | AUT | . 2 Athens | 1896 | Aquatics | ... | Bronze | Greece | GRE | . 3 Athens | 1896 | Aquatics | ... | Gold | Greece | GRE | . 4 Athens | 1896 | Aquatics | ... | Silver | Greece | GRE | . ... ... | ... | ... | ... | ... | ... | ... | . 29211 Beijing | 2008 | Wrestling | ... | Silver | Germany | GER | . 29212 Beijing | 2008 | Wrestling | ... | Bronze | Lithuania | LTU | . 29213 Beijing | 2008 | Wrestling | ... | Bronze | Armenia | ARM | . 29214 Beijing | 2008 | Wrestling | ... | Gold | Cuba | CUB | . 29215 Beijing | 2008 | Wrestling | ... | Silver | Russia | RUS | . 29216 rows × 12 columns . and measure, for each different country, the number of medals they&#39;ve won in each different sport: . summary = data.groupby([&#39;Country&#39;, &#39;Sport&#39;]).size().unstack().fillna(0) summary . Sport Aquatics Archery Athletics ... Water Motorsports Weightlifting Wrestling . Country . Afghanistan 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | . Algeria 0.0 | 0.0 | 6.0 | ... | 0.0 | 0.0 | 0.0 | . Argentina 3.0 | 0.0 | 5.0 | ... | 0.0 | 2.0 | 0.0 | . Armenia 0.0 | 0.0 | 0.0 | ... | 0.0 | 4.0 | 4.0 | . Australasia 11.0 | 0.0 | 1.0 | ... | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | . Virgin Islands* 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | . West Germany 62.0 | 0.0 | 67.0 | ... | 0.0 | 7.0 | 9.0 | . Yugoslavia 91.0 | 0.0 | 2.0 | ... | 0.0 | 0.0 | 16.0 | . Zambia 0.0 | 0.0 | 1.0 | ... | 0.0 | 0.0 | 0.0 | . Zimbabwe 7.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | . 137 rows × 42 columns . Now we&#39;ll pick two countries: . summary.loc[[&#39;Germany&#39;, &#39;Italy&#39;]] . Sport Aquatics Archery Athletics ... Water Motorsports Weightlifting Wrestling . Country . Germany 175.0 | 6.0 | 99.0 | ... | 0.0 | 20.0 | 24.0 | . Italy 113.0 | 12.0 | 71.0 | ... | 0.0 | 14.0 | 20.0 | . 2 rows × 42 columns . Each country has 44 columns giving the total number of medals won in each sport. Our job is to come up with a single number that summarizes how different those two lists of numbers are. Mathematicians have figured out lots of different ways of doing that, many of which are implemented in the scipy.spatial.distance module. . If we just import pdist from the module, and pass in our dataframe of two countries, we&#39;ll get a measuremnt: . from scipy.spatial.distance import pdist pdist(summary.loc[[&#39;Germany&#39;, &#39;Italy&#39;]]) . array([342.3024978]) . That&#39;s the distance score using the default metric, which is called the euclidian distance. Think of it as the straight line distance between the two points in space defined by the two lists of 44 numbers. . Now, what happens if we pass in a dataframe with three countries? . pdist(summary.loc[[&#39;Germany&#39;, &#39;Italy&#39;, &#39;France&#39;]]) . array([342.3024978 , 317.98584874, 144.82403116]) . As we might expect, we have three measurements: . Germany and Italy | Germnay and France | Italy and France | . But it&#39;s not easy to figure out which belongs to which. Happily, scipy also has a helper function that will take this list of numbers and turn it back into a square matrix: . from scipy.spatial.distance import squareform squareform(pdist(summary.loc[[&#39;Germany&#39;, &#39;Italy&#39;, &#39;France&#39;]])) . array([[ 0. , 342.3024978 , 317.98584874], [342.3024978 , 0. , 144.82403116], [317.98584874, 144.82403116, 0. ]]) . In order to make sense of this, we need to re-attach the country names, which we can just do by turning it into a DataFrame: . pd.DataFrame( squareform(pdist(summary.loc[[&#39;Germany&#39;, &#39;Italy&#39;, &#39;France&#39;]])), columns = [&#39;Germany&#39;, &#39;Italy&#39;, &#39;France&#39;], index = [&#39;Germany&#39;, &#39;Italy&#39;, &#39;France&#39;] ) . Germany Italy France . Germany 0.000000 | 342.302498 | 317.985849 | . Italy 342.302498 | 0.000000 | 144.824031 | . France 317.985849 | 144.824031 | 0.000000 | . Hopefully this agrees with our intuition; the numbers on the diagonal are all zero, because each country is identical to itself, and the numbers above and below are mirror images, because the distance between Germany and France is the same as the distance between France and Germany (remember that we are talking about distance in terms of their medal totals, not geographical distance!) . Finally, to get pairwise measurements for the whole input dataframe, we just pass in the complete object and get the country names from the index: . pairwise = pd.DataFrame( squareform(pdist(summary)), columns = summary.index, index = summary.index ) pairwise . Country Afghanistan Algeria Argentina ... Yugoslavia Zambia Zimbabwe . Country . Afghanistan 0.000000 | 8.774964 | 96.643675 | ... | 171.947666 | 1.732051 | 17.492856 | . Algeria 8.774964 | 0.000000 | 95.199790 | ... | 171.688672 | 7.348469 | 19.519221 | . Argentina 96.643675 | 95.199790 | 0.000000 | ... | 148.128323 | 96.348326 | 89.810912 | . Armenia 5.830952 | 9.848858 | 96.477977 | ... | 171.604196 | 5.744563 | 18.384776 | . Australasia 18.708287 | 20.024984 | 97.744565 | ... | 166.991018 | 18.627936 | 22.360680 | . ... ... | ... | ... | ... | ... | ... | ... | . Virgin Islands* 1.414214 | 8.774964 | 96.457244 | ... | 171.947666 | 1.732051 | 17.492856 | . West Germany 153.052279 | 150.306354 | 142.537714 | ... | 184.945938 | 152.577849 | 144.045132 | . Yugoslavia 171.947666 | 171.688672 | 148.128323 | ... | 0.000000 | 171.874955 | 169.103519 | . Zambia 1.732051 | 7.348469 | 96.348326 | ... | 171.874955 | 0.000000 | 17.521415 | . Zimbabwe 17.492856 | 19.519221 | 89.810912 | ... | 169.103519 | 17.521415 | 0.000000 | . 137 rows × 137 columns . A nice way to visualize these is with a heatmap. 137 countries is a bit too much to show on a webpage, so let&#39;s restrict it to just the countries that have scored at least 500 medals total: . import seaborn as sns import matplotlib.pyplot as plt # make summary table for just top countries top_countries = ( data .groupby(&#39;Country&#39;) .filter(lambda x : len(x) &gt; 500) .groupby([&#39;Country&#39;, &#39;Sport&#39;]) .size() .unstack() .fillna(0) ) # make pairwise distance matrix pairwise_top = pd.DataFrame( squareform(pdist(top_countries)), columns = top_countries.index, index = top_countries.index ) # plot it with seaborn plt.figure(figsize=(10,10)) sns.heatmap( pairwise_top, cmap=&#39;OrRd&#39;, linewidth=1 ) . &lt;AxesSubplot:xlabel=&#39;Country&#39;, ylabel=&#39;Country&#39;&gt; . Now that we have a plot to look at, we can see a problem with the distance metric we&#39;re using. The US has won so many more medals than other countries that it distorts the measurement. And if we think about it, what we&#39;re really interested in is not the exact number of medals in each category, but the relative number. In other words, we want two contries to be considered similar if they both have about twice as many medals in boxing as athletics, for example, regardless of the exact numbers. . Luckily for us, there is a distance measure already implemented in scipy that has that property - it&#39;s called cosine distance. Think of it as a measurement that only looks at the relationships between the 44 numbers for each country, not their magnitude. We can switch to cosine distance by specifying the metric keyword argument in pdist: . pairwise_top = pd.DataFrame( squareform(pdist(top_countries, metric=&#39;cosine&#39;)), columns = top_countries.index, index = top_countries.index ) # plot it with seaborn plt.figure(figsize=(10,10)) sns.heatmap( pairwise_top, cmap=&#39;OrRd&#39;, linewidth=1 ) . &lt;AxesSubplot:xlabel=&#39;Country&#39;, ylabel=&#39;Country&#39;&gt; . And as you can see we spot some much more interstesting patterns. Notice, for example, that Russia and Soviet Union have a very low distance (i.e. their medal distributions are very similar). . When looking at data like this, remember that the shade of each cell is not telling us anything about how many medals a country has won - simply how different or similar each country is to each other. Compare the above heatmap with this one which displays the proportion of medals in each sport per country: . plt.figure(figsize=(10,10)) sns.heatmap( top_countries.apply(lambda x : x / x.sum(), axis=1), cmap=&#39;BuPu&#39;, square=True, cbar_kws = {&#39;fraction&#39; : 0.02} ) . &lt;AxesSubplot:xlabel=&#39;Sport&#39;, ylabel=&#39;Country&#39;&gt; . Finally, how might we find pairs of countries that have very similar medal distributions (i.e. very low numbers in the pairwise table)? By far the easiest way is to start of by reshaping the table into long form, so that each comparison is on a separate row: . pairwise = pd.DataFrame( squareform(pdist(summary, metric=&#39;cosine&#39;)), columns = summary.index, index = summary.index ) # move to long form long_form = pairwise.unstack() # rename columns and turn into a dataframe long_form.index.rename([&#39;Country A&#39;, &#39;Country B&#39;], inplace=True) long_form = long_form.to_frame(&#39;cosine distance&#39;).reset_index() . Now we can write our filter as normal, remembering to filter out the unintersting rows that tell us a country&#39;s distance from itself! . long_form[ (long_form[&#39;cosine distance&#39;] &lt; 0.05) &amp; (long_form[&#39;Country A&#39;] != long_form[&#39;Country B&#39;]) ] . Country A Country B cosine distance . 272 Algeria | Zambia | 0.026671 | . 1034 Azerbaijan | Mongolia | 0.045618 | . 1105 Bahamas | Barbados | 0.021450 | . 1111 Bahamas | British West Indies | 0.021450 | . 1113 Bahamas | Burundi | 0.021450 | . ... ... | ... | ... | . 17033 United Arab Emirates | Haiti | 0.010051 | . 17037 United Arab Emirates | Independent Olympic Participants | 0.000000 | . 17051 United Arab Emirates | Kuwait | 0.000000 | . 18164 Virgin Islands* | Netherlands Antilles* | 0.000000 | . 18496 Zambia | Algeria | 0.026671 | . 462 rows × 3 columns .",
            "url": "https://drawingfromdata.com/pandas/clustering/2020/12/01/making-a-pairwise-distance-matrix-in-pandas.html",
            "relUrl": "/pandas/clustering/2020/12/01/making-a-pairwise-distance-matrix-in-pandas.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Does reducing numerical precision affect real world datasets?",
            "content": "Executive summary . Switching from 64 bit to 32 bit precision for floating point data in pandas saves memory and has virtually no effect on the data, as long as the values aren&#39;t greater than around 10300 . Slightly longer summary . I wrote code to download ~1,000 real world datasets from kaggle and look at summary statistics and linear regressions of floating point columns in both 64 and 32 bit precision | The final analysis involved ~1,000 datasets, ~8,000 data files, ~500,000,000 individual floating point values and ~500,000 linear regressions. | Changing to 32 bit precision introduced a mean proportional error of around 10-8 in the summary statistics, with the worst affected columns having a proportional error of around 10-6 | For linear regressions, the mean proportional change in slope and intercept was on the order of 10-6 | A small number of outliers with near-zero slope (representing uninteresting relationships) had larger proportional errors | In ~400,000 valid linear regressions I was unable to find any relationships whose significance at the 5% level was changed by moving from 64 to 32 bit precision | For real world datasets, switching from 64 to 32 bit floating point precision is overwhelmingly unlikely to alter our conclusions about the data, and is an easy way to reduce memory usage. | . Introduction . As you&#39;ve probably noticed, this is a longish article, so grab a cup of coffee if you&#39;re planning to read it all. If you&#39;re already familiar with floating point precision and error in pandas you can probably skip part one. Most of the methods stuff is in part two, so if you just want to see the results, skip to part three. Also, just to head off the obvious criticism, I think that most of the conclusions of this article could have been arrived at by pure reasoning, without looking at any data at all. But hopefully some folks will, like me, be more satisfied by empirical data :-) . Obviously, I think that pandas is wonderful :-) But it has a number of quirks, among them a reputation for being memory hungry. There are no shortage of articles online discussing how to fit large dataframes in memory, and there are a number of well established techniques (there&#39;s a whole chapter on the subject in the Drawing from Data book, and also an extended discussion in the car accidents dataset video). . One technique that always gets mentioned is storing numbers in a reduced precision data type. This is usually proposed as one of the last things to try after other options have been exhausted. I think there are a couple of reasons for this. One is that messing about with data types is a bit harder to explain than simple things like ignoring unneeded columns. Another is that deliberately throwing away information - which is what reducing precision will do - feels very uncomfortable from a data analysis point of view! . One thing that I&#39;ve rarely seen in such discussions is a measurement of how much of an effect reduced precision will have in real life datasets. This is surprising, since pandas makes it very easy to measure. So let&#39;s go ahead and try it... . Part one: changing data types and measuring differences . Before we dive in, a quick reminder of the pandas tools we&#39;ll use. All of the methods etc. are described in detail in the Drawing from Data book, so we&#39;ll limit ourselves here to an overview. . As an example, let&#39;s load the planets dataset: . import pandas as pd planets = pd.read_csv(&#39;https://raw.githubusercontent.com/mwaskom/seaborn-data/master/planets.csv&#39;) planets . method number orbital_period mass distance year . 0 Radial Velocity | 1 | 269.300000 | 7.10 | 77.40 | 2006 | . 1 Radial Velocity | 1 | 874.774000 | 2.21 | 56.95 | 2008 | . 2 Radial Velocity | 1 | 763.000000 | 2.60 | 19.84 | 2011 | . 3 Radial Velocity | 1 | 326.030000 | 19.40 | 110.62 | 2007 | . 4 Radial Velocity | 1 | 516.220000 | 10.50 | 119.47 | 2009 | . ... ... | ... | ... | ... | ... | ... | . 1030 Transit | 1 | 3.941507 | NaN | 172.00 | 2006 | . 1031 Transit | 1 | 2.615864 | NaN | 148.00 | 2007 | . 1032 Transit | 1 | 3.191524 | NaN | 174.00 | 2007 | . 1033 Transit | 1 | 4.125083 | NaN | 293.00 | 2008 | . 1034 Transit | 1 | 4.187757 | NaN | 260.00 | 2008 | . 1035 rows × 6 columns . which stores information about planets that have been discovered around other stars. We are interested in the orbital period column, which we&#39;ll store in a variable for convenience: . op = planets[&#39;orbital_period&#39;] op . 0 269.300000 1 874.774000 2 763.000000 3 326.030000 4 516.220000 ... 1030 3.941507 1031 2.615864 1032 3.191524 1033 4.125083 1034 4.187757 Name: orbital_period, Length: 1035, dtype: float64 . which tells us how long each planet takes to revolve around its host star. Because it&#39;s comprised of floating point numbers, it get the float64 data type by default. To change the data type, we can just use the as_type method. Let&#39;s try float32: . op.astype(&#39;float32&#39;) . 0 269.299988 1 874.773987 2 763.000000 3 326.029999 4 516.219971 ... 1030 3.941507 1031 2.615864 1032 3.191524 1033 4.125083 1034 4.187757 Name: orbital_period, Length: 1035, dtype: float32 . All we need to know about these two data types is that, as the names suggest, a single float64 value uses 64 bits of memory, whereas a float32 uses, you guessed it, 32 bits. So it comes as no great surprise that the memory usage of the float32 version is very close to half that of the float64 version: . op.memory_usage(), op.astype(&#39;float32&#39;).memory_usage() . (8408, 4268) . As well as a difference in memory usage, we can see from the output above that there are slight differences in the values themselves. This is perhaps easiest to see if we put the two series next to each other: . pd.concat([op, op.astype(&#39;float32&#39;)], axis=1) . orbital_period orbital_period . 0 269.300000 | 269.299988 | . 1 874.774000 | 874.773987 | . 2 763.000000 | 763.000000 | . 3 326.030000 | 326.029999 | . 4 516.220000 | 516.219971 | . ... ... | ... | . 1030 3.941507 | 3.941507 | . 1031 2.615864 | 2.615864 | . 1032 3.191524 | 3.191524 | . 1033 4.125083 | 4.125083 | . 1034 4.187757 | 4.187757 | . 1035 rows × 2 columns . To figure out how different the two series are, pandas makes it easy to just take the difference: . op.astype(&#39;float32&#39;) - op . 0 -1.220703e-05 1 -1.318359e-05 2 0.000000e+00 3 -1.220703e-06 4 -2.929688e-05 ... 1030 1.264038e-08 1031 3.846741e-08 1032 -1.096405e-07 1033 1.696655e-07 1034 1.522827e-08 Name: orbital_period, Length: 1035, dtype: float64 . It&#39;s probably more useful to view these as absolute difference, i.e. to ignore the sign: . (op.astype(&#39;float32&#39;) - op).abs() . 0 1.220703e-05 1 1.318359e-05 2 0.000000e+00 3 1.220703e-06 4 2.929688e-05 ... 1030 1.264038e-08 1031 3.846741e-08 1032 1.096405e-07 1033 1.696655e-07 1034 1.522827e-08 Name: orbital_period, Length: 1035, dtype: float64 . Now we can look at a summary: . (op.astype(&#39;float32&#39;) - op).abs().describe() . count 9.920000e+02 mean 3.887688e-06 std 1.556957e-05 min 0.000000e+00 25% 1.572113e-08 50% 1.246643e-07 75% 1.220703e-06 max 1.953125e-04 Name: orbital_period, dtype: float64 . The general picture here is that the change from float64 to float32 has made very little difference to the values. On average, it introduces an absolute error of around 4x10-6, or 0.000004, and the single worst-case value has an absolute error of about 0.0002. These errors are very small relative to the range of the original values: . op.describe() . count 992.000000 mean 2002.917596 std 26014.728304 min 0.090706 25% 5.442540 50% 39.979500 75% 526.005000 max 730000.000000 Name: orbital_period, dtype: float64 . Proportional differences . If we wanted to compare the errors meaningfully across multiple different datasets, it might be useful to look at the error as a proportion of the original values: . ((op.astype(&#39;float32&#39;) - op).abs() / op).describe() . count 9.920000e+02 mean 1.733438e-08 std 1.494138e-08 min 0.000000e+00 25% 2.540837e-09 50% 1.536689e-08 75% 2.830340e-08 max 5.878977e-08 Name: orbital_period, dtype: float64 . This calculation is a bit more involved, but easier to interpret. It is telling us that the switch from float64 to float32 changed the values on average by about 10-8 - a truly miniscule chanage. . Another way to look at the difference is to compare the summaries of the two series. For any given aggregation we can figure out the proportional change by taking the absolute difference and dividing by the original: . import numpy as np np.abs(op.mean() - op.astype(&#39;float32&#39;).mean()) / op.mean() . 5.7728789459559594e-08 . So changing the series to float32 alters the mean by a factor of around 10-8. We can do the same calculation with any other summary statistic - for instance, the standard deviation: . np.abs(op.std() - op.astype(&#39;float32&#39;).std()) / op.std() . 8.132412130848449e-09 . Comparing correlations . Another way to measure the effect of reduced precision is to see if it changes the correlation between floating point columns. In our planets dataset we have two other floating point columns: the mass and the distance. Using pandas&#39; built in corr method we can calculate the pairwise pearson correlation of all the floating point columns: . planets.select_dtypes(&#39;float64&#39;).corr() . orbital_period mass distance . orbital_period 1.000000 | 0.173725 | -0.034365 | . mass 0.173725 | 1.000000 | 0.274082 | . distance -0.034365 | 0.274082 | 1.000000 | . Then do the same with a float32 version and take the difference: . ( planets.select_dtypes(&#39;float64&#39;).corr() - planets.select_dtypes(&#39;float64&#39;).astype(&#39;float32&#39;).corr() ) . orbital_period mass distance . orbital_period 0.000000e+00 | -6.383500e-10 | 7.736160e-11 | . mass -6.383500e-10 | 0.000000e+00 | 1.046595e-09 | . distance 7.736160e-11 | 1.046595e-09 | 0.000000e+00 | . As we can see, the differences are tiny, and would definitely not alter our conclusions about the relationships in the dataset. For a more sophisticated look at correlations, we could bring in a linear regression: . from scipy.stats import linregress linregress( planets.dropna()[&#39;orbital_period&#39;], planets.dropna()[&#39;mass&#39;] ) . LinregressResult(slope=0.00045766570228103964, intercept=2.1268128078962634, rvalue=0.1849061860050771, pvalue=3.298187969140348e-05, stderr=0.00010921992411493946) . and again take the difference between the 64 and 32 bit versions: . l_64 = linregress( planets.dropna()[&#39;orbital_period&#39;], planets.dropna()[&#39;mass&#39;] ) l_32 = linregress( planets.dropna()[&#39;orbital_period&#39;].astype(&#39;float32&#39;), planets.dropna()[&#39;mass&#39;].astype(&#39;float32&#39;) ) print(f&#39;slope difference : {l_64[0] - l_32[0]} &#39;) print(f&#39;intercept difference : {l_64[1] - l_32[1]} &#39;) print(f&#39;R value difference : {l_64[2] - l_32[2]} &#39;) . slope difference : -1.6647770943907514e-12 intercept difference : 2.81312630967534e-08 R value difference : -1.7317597356125702e-10 . Once again, the difference is incredibly minor. Given that the original values might be very small, a proprtional change might be easier to interpret: . print(f&#39;proportional slope difference : {(l_64[0] - l_32[0]) / l_64[0]} &#39;) print(f&#39;proportional intercept difference : {(l_64[1] - l_32[1]) / l_64[1]} &#39;) print(f&#39;proportional R value difference : {(l_64[2] - l_32[2]) / l_64[2]} &#39;) . proportional slope difference : -3.6375395536379053e-09 proportional intercept difference : 1.3226957723928434e-08 proportional R value difference : -9.365612763031196e-10 . Part two: let&#39;s try it on a larger scale . Attentive readers will have noticed that all of the calculations above were done by Python code :-) That means that it should be possible to automate this process and try it across a large number of different datasets. For a convenient source of real world dataset we turn to kaggle. . The collection of datasets hosted at Kaggle is perfect for our purposes. It contains real world data from a massive range of different fields and sources, mostly stored in files that will be easy to get in to pandas. It also has a convenient API with a Python client library, which will make it easy to download a large collection of different data files. . Downloading the data . To download our collection of datasets using the Kaggle API we have to go through a couple of steps. First we will connect to the API and authenticate: . from kaggle.api.kaggle_api_extended import KaggleApi api = KaggleApi() api.authenticate() . Next we can get a list of datasets by using the dataset_list method. We will search for CSV files, since they will be the easiest to load into pandas, and use pagination to get a large collection of dataset objects. Here I&#39;m using the tqdm package for a progress bar, since this code takes a long time to run. I&#39;m not sure if it&#39;s necessary, but I&#39;m also putting in a 1-second delay between requests in an attempt to be a good internet citizen: . from tqdm.notebook import trange, tqdm import time dataset = [] for page in trange(1,1000): datasets.append(api.dataset_list(file_type=&#39;csv&#39;, page=page)) time.sleep(1) . Now we have a list of dataset objects, we can get on to the actual downloading. To keep things organized I&#39;m using a separate folder for each dataset (which might end up containing multiple files). Once I&#39;ve created the folder it&#39;s just a case of calling dataset_download_files with each dataset reference from the previous step and the new folder name as the destination. I&#39;m also using unzip=True to make sure that we end up with extracted files. This will obviously take more disk space than leaving them compressed. . for dat in tqdm(all_datasets): folder_name = dat.ref.replace(&#39;/&#39;, &#39;-&#39;) if not os.path.exists(folder_name): os.mkdir(folder_name) print(dat.ref) api.dataset_download_files(str(dat.ref),path=folder_name, unzip=True) . Processing the downloaded files . Now that we&#39;ve done the download step, we can get on to reading our data files and measuring the effect of reducing precision. Let&#39;s take a look at the function to take a single file and just give us the columns we&#39;re interested in: . def get_float_columns(filepath): try: df = pd.read_csv(filepath, encoding=&#39;latin-1&#39;).select_dtypes([&#39;float64&#39;]) except: print(f&#39;cannot read {filepath}&#39;) return None # replace infinite values with missing data df.replace([np.inf, -np.inf], np.nan, inplace=True) # get rid of columns that are only floats because they contain missing data # i.e. they should actually be integers keep = [] for c in df.columns: if not all(df[c].dropna().astype(int) == df[c].dropna()): keep.append(c) if len(keep) == 0: return None return df[keep] . Some of the files will need special arguments to read_csv in order to open. Of course, since we have so many files, we can&#39;t go and manually figure out the correct arguments for every one. So to open the file we&#39;ll just call read_csv in a try block so that files requiring complex arguments won&#39;t crash our program. We are only interested in floating point columns, so we&#39;ll use select_dtypes to get them. . There&#39;s a slight complication in selecting the floating point columns: due to the way that pandas handles missing data, integer columns that contain missing data will end up as float64 by default. So there&#39;s a chunk of code to identify floating point columns that are really integers, and to drop them from the dataset. . We can test out the function by running it on our planets dataset: . get_float_columns(&#39;planets.csv&#39;) . orbital_period mass distance . 0 269.300000 | 7.10 | 77.40 | . 1 874.774000 | 2.21 | 56.95 | . 2 763.000000 | 2.60 | 19.84 | . 3 326.030000 | 19.40 | 110.62 | . 4 516.220000 | 10.50 | 119.47 | . ... ... | ... | ... | . 1030 3.941507 | NaN | 172.00 | . 1031 2.615864 | NaN | 148.00 | . 1032 3.191524 | NaN | 174.00 | . 1033 4.125083 | NaN | 293.00 | . 1034 4.187757 | NaN | 260.00 | . 1035 rows × 3 columns . As expected, we just get the floating point columns. . Comparing summary statistics . Now we can move on to the first question, calculating the summary statistics for each column in both 64 bit and 32 bit versions. Most of the work will be done by describe. We will add a _64 and a _32 suffix to the column names, then concatenate the two descriptions to give a wide summary table with one row per column in the original dataframe: . def summarize_columns(df): summary_64 = df.describe().T summary_64.columns = [c + &#39;_64&#39; for c in summary_64.columns] summary_64_abs = df.abs().describe().T summary_64_abs.columns = [c + &#39;_64_abs&#39; for c in summary_64_abs.columns] summary_32 = df.astype(&#39;float32&#39;).describe().T summary_32.columns = [c + &#39;_32&#39; for c in summary_32.columns] result = pd.concat([summary_32, summary_64, summary_64_abs], axis=1) return result planets = get_float_columns(&#39;planets.csv&#39;) summarize_columns(planets) . count_32 mean_32 std_32 min_32 25%_32 50%_32 75%_32 max_32 count_64 mean_64 ... 75%_64 max_64 count_64_abs mean_64_abs std_64_abs min_64_abs 25%_64_abs 50%_64_abs 75%_64_abs max_64_abs . orbital_period 992.0 | 2002.917480 | 26014.728516 | 0.090706 | 5.442540 | 39.9795 | 526.005005 | 730000.0 | 992.0 | 2002.917596 | ... | 526.005 | 730000.0 | 992.0 | 2002.917596 | 26014.728304 | 0.090706 | 5.44254 | 39.9795 | 526.005 | 730000.0 | . mass 513.0 | 2.638161 | 3.818617 | 0.003600 | 0.229000 | 1.2600 | 3.040000 | 25.0 | 513.0 | 2.638161 | ... | 3.040 | 25.0 | 513.0 | 2.638161 | 3.818617 | 0.003600 | 0.22900 | 1.2600 | 3.040 | 25.0 | . distance 808.0 | 264.069275 | 733.116455 | 1.350000 | 32.560001 | 55.2500 | 178.500000 | 8500.0 | 808.0 | 264.069282 | ... | 178.500 | 8500.0 | 808.0 | 264.069282 | 733.116493 | 1.350000 | 32.56000 | 55.2500 | 178.500 | 8500.0 | . 3 rows × 24 columns . There&#39;s one slight complication compared to our original planet code. Think about the case where we have a column of mixed negative and positive values. We might end up with a situation where the mean is very close to zero, even though the values themselves are not. This will give us an inflated estimate of the precision error when we divide by the mean. By working with a mean of the absolute values we can avoid this. The same goes for the other summary statistics - median, min, max, etc. . This dataframe is a bit awkward to read on a web page, since it&#39;s so wide. But the column structure makes it very easy to check, for example, the difference in the means, either in absolute terms: . summary = summarize_columns(planets) (summary[&#39;mean_64&#39;] - summary[&#39;mean_32&#39;]).abs() . orbital_period 1.156260e-04 mass 1.207711e-07 distance 7.275874e-06 dtype: float64 . Or as a proportion of the original: . summary = summarize_columns(planets) (summary[&#39;mean_64&#39;] - summary[&#39;mean_32&#39;]).abs() / summary[&#39;mean_64_abs&#39;] . orbital_period 5.772879e-08 mass 4.577852e-08 distance 2.755290e-08 dtype: float64 . Comparing linear regressions . Next we need a function that will take our dataframe of floating point columns and calculate pairwise linear regressions for both 32 and 64 bit versions. Here&#39;s what it looks like: . import scipy import itertools def summarize_regressions(df): df = df.dropna() # if fewer than two columns or fewer than ten rows, skip if len(df.columns) &lt; 2 or len(df) &lt; 10: return None df_32 = df.astype(&#39;float32&#39;) results = [] for x, y in itertools.combinations(df.columns, r=2): regression_64 = scipy.stats.linregress(df[x], df[y]) regression_32 = scipy.stats.linregress(df_32[x], df_32[y]) results.append( ( x, y, regression_64[0], # slope regression_32[0], regression_64[1], # intercept regression_32[1], regression_64[2], # R value regression_32[2], regression_64[3], # P value regression_32[3], ) ) return pd.DataFrame( results, columns = [&#39;x&#39;, &#39;y&#39;, &#39;slope_64&#39;, &#39;slope_32&#39;, &#39;intercept_64&#39;, &#39;intercept_32&#39;, &#39;rvalue_64&#39;, &#39;rvalue_32&#39;, &#39;pvalue_64&#39;, &#39;pvalue_32&#39;] ) summarize_regressions(planets) . x y slope_64 slope_32 intercept_64 intercept_32 rvalue_64 rvalue_32 pvalue_64 pvalue_32 . 0 orbital_period | mass | 0.000458 | 0.000458 | 2.126813 | 2.126813 | 0.184906 | 0.184906 | 3.298188e-05 | 3.298188e-05 | . 1 orbital_period | distance | -0.001112 | -0.001112 | 52.997824 | 52.997825 | -0.035069 | -0.035069 | 4.348817e-01 | 4.348817e-01 | . 2 mass | distance | 3.512154 | 3.512154 | 43.255093 | 43.255095 | 0.274082 | 0.274082 | 4.954410e-10 | 4.954411e-10 | . The output is another summary table, this time showing us the slope, intercept, rvalue and pvalue for each pair of columns in both 64 bit and 32 bit form. As in the summary statistics, this makes it easy to find the differences in the various regression properties. Here&#39;s the difference in slope as a proportion of the original slope: . summary = summarize_regressions(planets) (summary[&#39;slope_64&#39;] - summary[&#39;slope_32&#39;]).abs() / summary[&#39;slope_64&#39;] . 0 3.637540e-09 1 -3.431153e-08 2 8.654275e-09 dtype: float64 . Part three: looking at the results . Having written the functions above, actually analysing all of the downloaded data files is pretty straightforward. We will use glob to iterate over the folders, and just call the functions in order. . import glob from tqdm.notebook import tqdm dfs = [] for filepath in tqdm(glob.glob(&#39;*-*/*.csv&#39;)): df = get_float_columns(filepath) if df is not None: summary = summarize_columns(df) summary[&#39;filepath&#39;] = filepath dfs.append(summary) summary_stats = pd.concat(dfs) . At the end of the process we have a big dataframe that contains the summary for all the data files: . summary_stats.head() . count_32 mean_32 std_32 min_32 25%_32 50%_32 75%_32 max_32 count_64 mean_64 ... max_64 count_64_abs mean_64_abs std_64_abs min_64_abs 25%_64_abs 50%_64_abs 75%_64_abs max_64_abs filepath . BMI 768.0 | 31.992579 | 7.884161 | 0.000 | 27.299999 | 32.000000 | 36.599998 | 67.099998 | 768.0 | 31.992578 | ... | 67.10 | 768.0 | 31.992578 | 7.884160 | 0.000 | 27.300000 | 32.000000 | 36.600000 | 67.10 | uciml-pima-indians-diabetes-database/diabetes.csv | . DiabetesPedigreeFunction 768.0 | 0.471876 | 0.331329 | 0.078 | 0.243750 | 0.372500 | 0.626250 | 2.420000 | 768.0 | 0.471876 | ... | 2.42 | 768.0 | 0.471876 | 0.331329 | 0.078 | 0.243750 | 0.372500 | 0.626250 | 2.42 | uciml-pima-indians-diabetes-database/diabetes.csv | . MILES* 1156.0 | 21.115400 | 359.299011 | 0.500 | 2.900000 | 6.000000 | 10.400000 | 12204.700195 | 1156.0 | 21.115398 | ... | 12204.70 | 1156.0 | 21.115398 | 359.299007 | 0.500 | 2.900000 | 6.000000 | 10.400000 | 12204.70 | zusmani-uberdrives/My Uber Drives - 2016.csv | . avg_rating 13608.0 | 3.923293 | 1.031304 | 0.000 | 3.800000 | 4.194440 | 4.450000 | 5.000000 | 13608.0 | 3.923293 | ... | 5.00 | 13608.0 | 3.923293 | 1.031304 | 0.000 | 3.800000 | 4.194440 | 4.450000 | 5.00 | jilkothari-finance-accounting-courses-udemy-13... | . avg_rating_recent 13608.0 | 3.912241 | 1.039237 | 0.000 | 3.787315 | 4.181735 | 4.452105 | 5.000000 | 13608.0 | 3.912242 | ... | 5.00 | 13608.0 | 3.912242 | 1.039237 | 0.000 | 3.787315 | 4.181735 | 4.452105 | 5.00 | jilkothari-finance-accounting-courses-udemy-13... | . 5 rows × 25 columns . Let&#39;s first take a look at the volume of data. We have processed 919 data files: . summary_stats[&#39;filepath&#39;].nunique() . 919 . containing just over 8000 floating point columns: . len(summary_stats.reset_index().groupby([&#39;index&#39;, &#39;filepath&#39;])) . 8040 . and just over half a billion individual floating point values: . summary_stats[&#39;count_64&#39;].sum() . 520432586.0 . One slight complication is that some of our data files contain very large values that will be outside the range of 32 bit floating point numbers. It&#39;s easy to spot those in the file as the maximum of the 32 bit version will end up infinite: . summary_stats[np.isinf(summary_stats[&#39;max_32&#39;])] . count_32 mean_32 std_32 min_32 25%_32 50%_32 75%_32 max_32 count_64 mean_64 ... max_64 count_64_abs mean_64_abs std_64_abs min_64_abs 25%_64_abs 50%_64_abs 75%_64_abs max_64_abs filepath . PublicScoreLeaderboardDisplay 8357340.0 | NaN | NaN | -inf | 0.4333 | 0.69630 | 0.93937 | inf | 8357340.0 | 8.374128e+295 | ... | 6.567444e+302 | 8357340.0 | 8.374128e+295 | inf | 0.0 | 0.45359 | 0.71052 | 0.94674 | 6.567444e+302 | kaggle-meta-kaggle/Submissions.csv | . PrivateScoreLeaderboardDisplay 8357340.0 | NaN | NaN | -inf | 0.4352 | 0.70579 | 0.93160 | inf | 8357340.0 | 1.094999e+296 | ... | 6.870993e+302 | 8357340.0 | 1.094999e+296 | inf | 0.0 | 0.45700 | 0.72148 | 0.94210 | 6.870993e+302 | kaggle-meta-kaggle/Submissions.csv | . 2 rows × 25 columns . It&#39;s also pretty easy to remove them from the results: . results = summary_stats[ ~ np.isinf(summary_stats[&#39;max_32&#39;])] . Looking at changes in summary statistics . Now we can start to address the questions of how much effect changing to 32 bit precision had on the data. Since the results dataframe is in the same format as our earlier planets example, the code will be very similar. We can calculate the absolute error: . (results[&#39;mean_64&#39;] - results[&#39;mean_32&#39;]).abs() . BMI 3.814697e-07 DiabetesPedigreeFunction 8.662542e-09 MILES* 2.390456e-06 avg_rating 3.664424e-08 avg_rating_recent 1.100372e-07 ... 5a_credit_market_reg 5.570170e-07 5b_labor_market_reg 2.104992e-07 5c_business_reg 3.096381e-07 5_regulation 3.479504e-07 oldpeak 1.180290e-08 Length: 8038, dtype: float64 . but since we have so many values, we need to summarize them. We can do this with summary statistics: . (results[&#39;mean_64&#39;] - results[&#39;mean_32&#39;]).abs().describe() . count 8.038000e+03 mean 3.698345e+06 std 3.315559e+08 min 0.000000e+00 25% 2.756953e-08 50% 4.126828e-07 75% 3.578393e-06 max 2.972561e+10 dtype: float64 . Notice that we have a pretty wild distribution here. The mean absolute error is about three million, but the median (50% in the above table) is 0.0000004! This is why we need to normalize these by viewing them as a proportion of the original mean of absolute values: . prop_errors = (results[&#39;mean_64&#39;] - results[&#39;mean_32&#39;]).abs() / results[&#39;mean_64_abs&#39;] prop_errors.describe() . count 8.038000e+03 mean 4.340236e-08 std 5.796892e-08 min 0.000000e+00 25% 1.366587e-08 50% 3.117527e-08 75% 5.799712e-08 max 1.963675e-06 dtype: float64 . This is easier to look at. The mean and median proportional error are both on the order of 10-8, and the worst affected data column has its mean altered by around 10-6. In practical terms, these are incredibly minor effects. We can easily view them as a distribution to see the long tail: . import seaborn as sns sns.displot( prop_errors, aspect=2 ) . &lt;seaborn.axisgrid.FacetGrid at 0x7f609ba57fa0&gt; . We can do the same thing for the other aggregations of the original data - let&#39;s add the median (50% in our table) and the max. Calculating the proportional error in the minimum is tricky, since for many data columns the minimum value will be zero, leading to an infinite proportional error, so we will skip it. . Adding these measurements as additional columns: . results[&#39;mean prop error&#39;] = (results[&#39;mean_64&#39;] - results[&#39;mean_32&#39;]).abs() / results[&#39;mean_64_abs&#39;] results[&#39;median prop error&#39;] = (results[&#39;50%_64&#39;] - results[&#39;50%_32&#39;]).abs() / results[&#39;50%_64_abs&#39;] results[&#39;max prop error&#39;] = (results[&#39;max_64&#39;] - results[&#39;max_32&#39;]).abs() / results[&#39;max_64_abs&#39;] . will allow us to make a summary table: . results[[&#39;mean prop error&#39;, &#39;median prop error&#39;, &#39;max prop error&#39;]].describe() . mean prop error median prop error max prop error . count 8.038000e+03 | 7.835000e+03 | 8.038000e+03 | . mean 4.340236e-08 | 1.546485e-08 | 1.370840e-08 | . std 5.796892e-08 | 1.516289e-08 | 1.422228e-08 | . min 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 25% 1.366587e-08 | 1.382856e-09 | 0.000000e+00 | . 50% 3.117527e-08 | 1.254835e-08 | 1.014960e-08 | . 75% 5.799712e-08 | 2.447450e-08 | 2.442311e-08 | . max 1.963675e-06 | 1.067892e-07 | 5.848003e-08 | . A little bit of rearrangement using melt will allow us to look at the distribution of proportional errors for all three summary statistics: . g = sns.displot( data = results[[&#39;mean prop error&#39;, &#39;median prop error&#39;, &#39;max prop error&#39;]].melt(), x = &#39;value&#39;, col = &#39;variable&#39;, ) import matplotlib.pyplot as plt plt.xlim((0, 2e-7)) . (0.0, 2e-07) . Looking at linear regressions . Now we can build a similar dataframe for the linear regression results. The logic is very similar to the summary statistics; we are just using the summarize_regressions function instead: . import glob from tqdm.notebook import tqdm dfs = [] for filepath in tqdm(glob.glob(&#39;*-*/*.csv&#39;)): df = get_float_columns(filepath) if df is not None: summary = summarize_regressions(df) if summary is not None: summary[&#39;filepath&#39;] = filepath dfs.append(summary) regressions = pd.concat(dfs) . The resulting dataframe has the same wide shape as before, but this time each row represents a regression result between two columns from the same data file. We have just over half a million regression results in total: . regressions . x y slope_64 slope_32 intercept_64 intercept_32 rvalue_64 rvalue_32 pvalue_64 pvalue_32 filepath . 0 BMI | DiabetesPedigreeFunction | 5.910630e-03 | 5.910631e-03 | 0.282780 | 0.282780 | 0.140647 | 0.140647 | 9.197970e-05 | 9.197967e-05 | uciml-pima-indians-diabetes-database/diabetes.csv | . 0 avg_rating | avg_rating_recent | 9.968315e-01 | 9.968315e-01 | 0.001379 | 0.001379 | 0.989222 | 0.989222 | 0.000000e+00 | 0.000000e+00 | jilkothari-finance-accounting-courses-udemy-13... | . 1 avg_rating | rating | 9.968315e-01 | 9.968315e-01 | 0.001379 | 0.001379 | 0.989222 | 0.989222 | 0.000000e+00 | 0.000000e+00 | jilkothari-finance-accounting-courses-udemy-13... | . 2 avg_rating_recent | rating | 1.000000e+00 | 1.000000e+00 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 0.000000e+00 | 0.000000e+00 | jilkothari-finance-accounting-courses-udemy-13... | . 0 ts | co | 1.613924e-10 | 1.613926e-10 | -0.252759 | -0.252759 | 0.025757 | 0.025758 | 1.964944e-60 | 1.964217e-60 | garystafford-environmental-sensor-data-132k/io... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 401 5a_credit_market_reg | 5c_business_reg | 3.975257e-01 | 3.975257e-01 | 2.992313 | 2.992313 | 0.444242 | 0.444242 | 3.433446e-65 | 3.433454e-65 | gsutters-economic-freedom/efw_cc.csv | . 402 5a_credit_market_reg | 5_regulation | 5.486941e-01 | 5.486941e-01 | 2.423950 | 2.423951 | 0.747747 | 0.747747 | 1.517789e-237 | 1.517798e-237 | gsutters-economic-freedom/efw_cc.csv | . 403 5b_labor_market_reg | 5c_business_reg | 3.465142e-01 | 3.465142e-01 | 4.137618 | 4.137618 | 0.394059 | 0.394059 | 1.834371e-50 | 1.834375e-50 | gsutters-economic-freedom/efw_cc.csv | . 404 5b_labor_market_reg | 5_regulation | 5.288462e-01 | 5.288462e-01 | 3.682333 | 3.682333 | 0.733397 | 0.733397 | 5.207614e-224 | 5.207626e-224 | gsutters-economic-freedom/efw_cc.csv | . 405 5c_business_reg | 5_regulation | 6.481920e-01 | 6.481920e-01 | 2.940607 | 2.940607 | 0.790448 | 0.790448 | 6.484037e-284 | 6.484023e-284 | gsutters-economic-freedom/efw_cc.csv | . 550136 rows × 11 columns . Because the number of pairwise comparisons increases with the square of the number of columns, the results are dominated by a small number of data files with many floating point columns: . regressions[&#39;filepath&#39;].value_counts() . rajanand-key-indicators-of-annual-health-survey/Key_indicator_districtwise.csv 182710 uciml-human-activity-recognition-with-smartphones/train.csv 157080 uciml-human-activity-recognition-with-smartphones/test.csv 157080 johnjdavisiv-us-counties-covid19-weather-sociohealth-data/US_counties_COVID19_health_weather_data.csv 11175 johnjdavisiv-us-counties-covid19-weather-sociohealth-data/us_county_sociohealth_data.csv 7260 ... chrisfilo-urbansound8k/UrbanSound8K.csv 1 johnjdavisiv-us-counties-covid19-weather-sociohealth-data/us_county_geometry.csv 1 tunguz-us-elections-dataset/2016-precinct-house.csv 1 sudalairajkumar-novel-corona-virus-2019-dataset/time_series_covid_19_confirmed_US.csv 1 prakrutchauhan-indian-candidates-for-general-election-2019/LS_2.0.csv 1 Name: filepath, Length: 289, dtype: int64 . Out of our 919 data files, only 289 had at least two floating point columns. The data file with the most floating point columns contributes nearly two hundred thousand regression results! . To get an overview of the difference for each property of the regression result, we can add new columns as we did before and summarize them: . regressions[&#39;slope error&#39;] = ((regressions[&#39;slope_64&#39;] - regressions[&#39;slope_32&#39;]) / regressions[&#39;slope_64&#39;]).abs() regressions[&#39;intercept error&#39;] = ((regressions[&#39;intercept_64&#39;] - regressions[&#39;intercept_32&#39;]) / regressions[&#39;intercept_64&#39;]).abs() regressions[&#39;rvalue error&#39;] = ((regressions[&#39;rvalue_64&#39;] - regressions[&#39;rvalue_32&#39;]) / regressions[&#39;rvalue_64&#39;]).abs() regressions[&#39;pvalue error&#39;] = ((regressions[&#39;pvalue_64&#39;] - regressions[&#39;pvalue_32&#39;]) / regressions[&#39;pvalue_64&#39;]).abs() regressions[[&#39;slope error&#39;, &#39;intercept error&#39;, &#39;rvalue error&#39;, &#39;pvalue error&#39;]].dropna().describe() . slope error intercept error rvalue error pvalue error . count 4.181990e+05 | 4.181990e+05 | 4.181990e+05 | 4.181990e+05 | . mean 4.237303e-04 | 7.675898e-05 | 4.237294e-04 | 1.471530e-06 | . std 2.486475e-02 | 1.618511e-02 | 2.486475e-02 | 8.407760e-06 | . min 0.000000e+00 | 2.753007e-13 | 0.000000e+00 | 0.000000e+00 | . 25% 4.304286e-09 | 1.899837e-08 | 3.850181e-09 | 1.095305e-07 | . 50% 1.669817e-08 | 3.986912e-08 | 1.547252e-08 | 3.511528e-07 | . 75% 6.553311e-08 | 7.240586e-08 | 6.388698e-08 | 1.105768e-06 | . max 4.985437e+00 | 4.448138e+00 | 4.985437e+00 | 2.476720e-03 | . Notice that in the count row we can see how many of the comparisons gave meaningful (i.e. non-missing) output for the linear regressions on both the 64 and 32 bit versions. . Remember that these are proportional errors i.e. the proportion by which the values change. Although the mean errors are a couple of orders of magnitude greater than for the summary statistics, they are still very small, and very unlikely to change any conclusions about the data. . We have a very long tailed distribution for all four properties: the mean is several orders of magnitude higher than the median (50%) and the maximum values are several orders of magnitude higher again, suggesting a small number of extreme outliers. One way to show this is with a boxen plot: . sns.catplot( data = regressions[[&#39;slope error&#39;, &#39;intercept error&#39;, &#39;rvalue error&#39;, &#39;pvalue error&#39;]].dropna().melt(), x = &#39;variable&#39;, y = &#39;value&#39;, kind=&#39;boxen&#39;, aspect = 2 ) . &lt;seaborn.axisgrid.FacetGrid at 0x7f60e525e1f0&gt; . The presence of the outliers make it impossible to see the distribution of the remaining values. If we plot the slope against the slope error, we can see that these outliers all represent regressions where the original slope was very near zero, suggesting no interesting relationship between the variables: . sns.relplot( data = regressions, x = &#39;slope_64&#39;, y = &#39;slope error&#39; ) plt.xlim(-1e-8,1e-8) . (-1e-08, 1e-08) . The same is true for the intercept: . sns.relplot( data = regressions, x = &#39;intercept_64&#39;, y = &#39;intercept error&#39; ) plt.xlim(-1e-5,1e-5) . (-1e-05, 1e-05) . An alternative way of looking at the pvalue error is to ask whether the change in pvalue would result in a difference in whether the relationship was considered significant. Setting aside the problems with interpreting pvalues in this way, we can use pandas to find any regressions where one version of the regression was significant at the 5% level and the other wasn&#39;t: . ((regressions.dropna()[&#39;pvalue_64&#39;] &lt; 0.05) != (regressions.dropna()[&#39;pvalue_32&#39;] &lt; 0.05)).value_counts() . False 418199 dtype: int64 . In the ~400,000 out of our ~500,000 comparisions that had no missing data, there were no such cases. . Conclusion . It looks like in real world datasets, switching from 64 bit to 32 bit precision for floating point numbers has a vanishingly small chance of affecting our conclusions, and we should probably do it without fear when trying to minimize memory usage. . If you&#39;ve made it this far, you should definitely subscribe to the Drawing from Data newsletter, follow me on Twitter, or buy the Drawing from Data book! .",
            "url": "https://drawingfromdata.com/pandas/numpy/kaggle/2020/12/01/Article.html",
            "relUrl": "/pandas/numpy/kaggle/2020/12/01/Article.html",
            "date": " • Dec 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About Me . I’m a freelance trainer specialising in teaching Python with a strong focus on data analysis and exploration. I trained as a biologist and completed my PhD in large-scale phylogenetics in 2007, then held a number of academic positions at the University of Edinburgh ending in a two year stint as Lecturer in Bioinformatics. I’ve been teaching and writing freelance full-time since 2015. . Contact . Want to chat? Drop me an email: martin@drawingfromdata.com .",
          "url": "https://drawingfromdata.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": ". Time to get to grips with your data . With Python, pandas and seaborn in your toolbox, you too can develop data exploration superpowers. . We’re currently in a golden age of data. It’s never been easier to assemble large datasets to probe questions in almost any field. . But these large datasets come with their own problems: . How to clean and validate data? . | How to combine datasets from multiple sources? . | And how to look for patterns in large, complex datasets and display your findings? . | . These are the questions that we all ask when we start working with these rich data sources. . The solution to these problems comes in the form of Python’s scientific software stack. The combination of a friendly, expressive language and high quality packages makes a fantastic set of tools for data exploration. . But the packages themselves can be hard to get to grips with. It’s difficult to know where to get started, or which sets of tools will be most useful. . You may have already encountered this. If you look at the matplotlib website, it’s clear that it’s a powerful charting tool - but looking at the tutorial can be daunting. The same goes for pandas: it can carry out almost any type of data manipulation, but that same power makes it hard to get to grips with. . . . . Happily, learning to use Python effectively for data exploration is a superpower that you can learn. With a basic knowledge of Python, pandas (for data manipulation) and seaborn (for data visualization) you’ll be able to understand complex datasets quickly and mine them for insight. . You’ll be able to make beautiful, informative charts for posters, papers and presentations, and rapidly update them to reflect new data or test new hypotheses. . And you’ll be able to quickly make sense of datasets from other projects - millions of rows of data will no longer be a scary prospect! . In this book, I have drawn on years of teaching experience to give you the tools you need to answer your data exploration questions. Starting with the basics, you’ll learn how to use Python, pandas, seaborn and matplotlib effectively using real world examples throughout. . . . Just the best bits of the best data exploration packages . . Rather than overwhelm you with information, the book concentrates on the most useful parts of the relevant libraries. Full color illustrations show hundreds of examples covering dozens of different chart types, with complete code samples that you can tweak and use for your own work. . The book is designed to help you get over the most common obstacles that we encounter with real world data. . You’ll learn what to do if: . your input files have errors, or missing data. . | your input files are too big to fit in memory . | you need to combine data from multiple sources . | you have to visualize datasets with thousands of rows or millions of columns . | you need to make complex filters for your data . | your data are in the wrong format for the analysis you want to do . | . Once you understand the basics of pandas, seaborn and matplotlib, the chapters on visualization will show you how to make sophisticated charts with minimal code and how to best use color to make clear charts. . . . All the chart types you’ll need . . The book covers the most basic charts that we use every day - histograms, scatter plots and boxplots - and more exotic charts like clustermaps and violin plots - all with full working code and real world examples. . . . . For getting started with data exploration in Python, the step-by-step approach offered by the book beats everything else. But the real power of Python’s data processing stack becomes apparent when you combine the individual tools in a single project. . That’s where the videos come in. . In each of the four detailed videos, we’ll take a single real world dataset and walk through all the steps involved in analyzing it, starting with publicly available data files and ending with our final figures. The videos give us a chance to explore different types of data, demonstrate tips and techniques that don’t fit in the book, and use visualization to answer interesting questions on wide-ranging topics. . . . . . . Video 1: 5000 Movie dataset . In this video we look at a dataset of 5000 movies with genres, dates and budgets . In it you’ll learn: . how to clean crowd-sourced data | how to spot artefacts caused by human error | how to deal with structured data inside columns | how to quickly spot correlations between columns | how to view trends over time | . . . . . . Video 2: Car accident reports . In this video we explore reports of 3.5 million US car accidents . In it you’ll learn: . how to reduce memory usage of large files | what to look out for when dealing with automated data collection | how to use heatmaps for visualizing temporal data | how to use GeoPandas to deal with geographic data | how to visualize geographical data at multiple levels of detail | . . . . . . Video 3: Lego set components . In this video we explore Lego sets - bricks, colors and metadata . In it you’ll learn: . how to combine information that’s spread across multiple files | how to deal with complex many-to-many relationships | how to look for categorical patterns across time | how to use color thematically | how to visualize heirarchical categories | . . . . . . Video 4: Software developer survey . In this video we take a look at the results of a career survey of 100,000 developers . In it you’ll learn: . how to deal effectively with survey data | how to rapidly understand the structure of a new dataset | how to visualize multiple overlapping categories | how to visualize and interpret rank data | how to optimize small multiple charts for readability | . . Just like in the book, all the video files come with working code in Jupyter notebooks that you can use to run the same analysis live, or tweak for your own work. . . . . If you work with data of any size, getting to grips with data exploration in Python is one of the biggest boosts you can make to your work and your career. . Free your data from Excel! Transform your messy spreadsheets into clean, tidy tables! Understand your data better! . Ready to get started? Check out the packages below (and if you want to see a complete chapter list, scroll down to the bottom of the page). . All of the packages come with a no-questions-asked refund policy so you can buy with confidence. My goal with these books and videos is to make them directly useful to you - if they don’t work for you, you can get your money back right away. . You can pay securely with a credit card or with Paypal. . . . . The complete package: bookshelf plus videos . Everything for complete beginners: the Drawing from Data book, the Python for complete beginners book, and all the videos . . This is the complete package, intended for those who’ve never used Python before, or who want a refresher before diving in to the data exploration material. . It includes my first book Python for complete beginners, which will take you from the very basics of programming and Python to give you all the background you’ll need to follow everything in the Drawing from Data book. . These two books come as searchable, DRM-free PDF files that you can keep forever and read on any device, along with exercise and example files to practice on. . You’ll also get the dataset walkthrough videos as high definition files that you can download and watch on any device. The videos come with interactive Jupyter notebook files that you can run, edit, and reuse for your own datasets. . Your files will be delivered by email, so you can start reading (and watching) right away. And you’ll get free access to any updates to the books (for example, when code changes to reflect changes in the libraries) and videos (for example, when new videos are added). . . Buy the complete package now for $169 . . . . The bookshelf package . The Drawing from Data book plus the Python for complete beginners book, and all the videos . . Don’t like videos? This package includes Python for complete beginners along with the Drawing from Data book. . These two books come as searchable, DRM-free PDF files that you can keep forever and read on any device, along with exercise and example files to practice on. . Your files will be delivered by email, so you can start reading right away. And you’ll get free access to any updates to the books (for example, when code changes to reflect changes in the libraries). . . Buy the bookshelf package now for $79 . . . . Just the book . . Don’t like videos and already know the basic of Python? This package includes just the Drawing from Data book. . These two books come as searchable, DRM-free PDF files that you can keep forever and read on any device, along with exercise and example files to practice on. . Your files will be delivered by email, so you can start reading right away. And you’ll get free access to any updates to the book (for example, when code changes to reflect changes in the libraries). . Buy the book now for $39 . . . Frequently asked questions: . What’s covered in the book? . Rather than trying to give an exhaustive tour pandas, seaborn, matplotlib and numpy (each of which could fill several books), the book concentrates on the most useful parts of each library. Here’s a complete list of chapters and topics: . getting data into pandas: series and dataframes, read_csv, dtypes/info/describe/head/len, missing data, giving column names | working with series: getting a single column, descriptive statistics, value_counts, broadcasting operations, numpy vectorized ufuncs, string methods, selecting multiple columns, setting an index, dropna, | filtering and selecting: boolean masks, basic filtering, isin, selecting with strings, multiple conditions, filter/select/aggregate pattern | pandas examples: turning series into list, iterating over unique values | intro to seaborn and plotting distributions: imports, distplot, setting title, distplot in a loop, relplot, hue and size for numerical values, adding a new column and plotting | special types of scatter plots: alpha/size/sample/hexbin/contour for large numbers, lmplot for regressions, pairwise plots | conditioning charts on categories: hue with category, size for category, style for category, small multiples with row/column, multiples with relplot | categorical axes wtih seaborn: about different types of categories, strip plots, swarm plots, multiples, box plots, problems with order, boxplots hide distribution details, violin plots, boxen plots, bar plots, point plots, line charts, count plots | styling figures with seaborn: three levels (seaborn high. seaborn low, matplotlib), aspect/height, labels, styles and contexts, passing keywords, | working with colours in seaborn: setting single colours, setting palettes, sequential data, don’t use rainbows, custom sequential, categorical palettes, redundant information, diverging palettes, when to use colour, matching between charts, colour plus style, displaying redundant catfor book layoutegories, adding metadata, highlighting categories, using colours consistently, don’t reuse | working with groups in pandas: types of categories, uniqueness, groupby, grouping on multiple columns, aggregating, turning into dataframes, filtering, transforming, iterating over groups, sorting to get group names | binning data: simple binary with filter and replace, increased options for visualisation, using pd.cut, range, uneven bins, exponential bins, using size for ordered categories, turning categories into ordered ones. | Long vs wide form and indices: why long data is best for visualization, using unstack to make a summary, using melt to get data into tidy form, setting an index, index performance, multi indexing, index slicing and loc | Matrix charts: displaying summary tables, setting scales, annotation, missing data, lots of categories, diverging palettes, binning, custom annotation, clustering walkthrough, row/col cluster, normalizing, color annotation, bubble grids, plotting context | Tricky data files in pandas: awkward input files, skiprows, setting names, using columns, footers, comments, thousands/decimals, boolean values, method chaining, assign and pipe, concat, adding a column by assigning a series, merging, inner/outer, dealing with large datasets, skipping columns, categories, precision | Using facet grids directly: making a facetgrid, using map for simple things, using map_dataframe with custom functions, reusing custom functions, using hue for single charts, small multiple heat maps | Unexpected behaviours: missing groups in groupby, fixing it with categories, breaking it with multiple grouping, workarounds, unsorted categories after unstacking, min_count in sum, odd scales in seaborn | High performance pandas: looping vs. vectorization, looping vs. apply, apply on dataframes, caching, replace and memoization, sampling large datasets, categories, indices, unique indices | Further reading: datetimes, alternative syntax, plotting directly from pandas, jupyter widgets, bokeh, machine learning, statistics | . Do I need to know Python in order to follow the book? . Yes - you don’t need to be a Python expert, but you do need to know the basics: the book isn’t intended for complete beginners. If you’ve never used Python before, you’ll need to learn the core of the language first. You can do this by buying one of the bookshelf packages (which includes Python for complete beginners, which is intended, as the name suggests, for complete beginners), or following various introductory Python tutorials online. . Who are you? . Hi, I’m Martin - formerly a bioinformatics researcher and lecturer at the University of Edinburgh; now a freelance Python writer and trainer. I’ve been using and teaching Python, with an emphasis on data exploration, for about ten years now. . Can I buy a group license for my team/office/research group/startup? . Sure, drop me an email martin@drawingfromdata.com and we’ll work something out. .",
          "url": "https://drawingfromdata.com/book/",
          "relUrl": "/book/",
          "date": ""
      }
      
  

  

  
      ,"page4": {
          "title": "Newsletter",
          "content": "Get the Drawing from Data newsletter . Sign up below to get an email with links to useful Python data science articles here and elsewhere. No spam ever, and an email every few weeks. . Get Python data analysis links and articles in your inbox . . . Email address Sign up",
          "url": "https://drawingfromdata.com/newsletter/",
          "relUrl": "/newsletter/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://drawingfromdata.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}