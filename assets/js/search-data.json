{
  
    
        "post0": {
            "title": "Working with string methods in pandas",
            "content": "One of the most useful features of pandas is its ability to take a bit of code that looks like it operates on a single bit of data, and have it operate on a whole bunch of different values. . . Tip: For a much more detailed explanation vectorization and lots more string processing examples, checkout the chapter on working with groups in the Drawing from Data book. . To see an example, let&#39;s load a dataset that shows fuel efficiency for different car models: . import pandas as pd pd.options.display.max_rows = 10 df = pd.read_csv( &quot;https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mpg.csv&quot; ) df . mpg cylinders displacement horsepower weight acceleration model_year origin name . 0 18.0 | 8 | 307.0 | 130.0 | 3504 | 12.0 | 70 | usa | chevrolet chevelle malibu | . 1 15.0 | 8 | 350.0 | 165.0 | 3693 | 11.5 | 70 | usa | buick skylark 320 | . 2 18.0 | 8 | 318.0 | 150.0 | 3436 | 11.0 | 70 | usa | plymouth satellite | . 3 16.0 | 8 | 304.0 | 150.0 | 3433 | 12.0 | 70 | usa | amc rebel sst | . 4 17.0 | 8 | 302.0 | 140.0 | 3449 | 10.5 | 70 | usa | ford torino | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 393 27.0 | 4 | 140.0 | 86.0 | 2790 | 15.6 | 82 | usa | ford mustang gl | . 394 44.0 | 4 | 97.0 | 52.0 | 2130 | 24.6 | 82 | europe | vw pickup | . 395 32.0 | 4 | 135.0 | 84.0 | 2295 | 11.6 | 82 | usa | dodge rampage | . 396 28.0 | 4 | 120.0 | 79.0 | 2625 | 18.6 | 82 | usa | ford ranger | . 397 31.0 | 4 | 119.0 | 82.0 | 2720 | 19.4 | 82 | usa | chevy s-10 | . 398 rows × 9 columns . Each row is a car, and for each car we have miles per gallon (mpg), plus a bunch of information about the engine and car, and finally the name. Say we want to convert the horsepower column into Kilowatts (kW). A quick trip to Google shows that one horsepower equals 0.7457 kW, so we might just iterate over the values and convert them like this: . kilowatts = [] for horsepower in df[&#39;horsepower&#39;]: kilowatts.append(horsepower * 0.7457) kilowatts[:10] . [96.941, 123.04050000000001, 111.855, 111.855, 104.39800000000001, 147.64860000000002, 164.054, 160.3255, 167.7825, 141.683] . However, the magic of pandas allows us to do this instead: . df[&#39;horsepower&#39;] * 0.7457 . 0 96.9410 1 123.0405 2 111.8550 3 111.8550 4 104.3980 ... 393 64.1302 394 38.7764 395 62.6388 396 58.9103 397 61.1474 Name: horsepower, Length: 398, dtype: float64 . We just write the expression as if we were trying to convert a single number, and pandas takes care of applying it to the whole series. This is called vectorization, and it&#39;s generally faster and more convenient than writing for loops. . However, we often run into problems when we try to use the same technique with text data. Let&#39;s get a list of the manufacturers for each car, which we can do just by getting the word before the first space in the name field. . First we&#39;ll use a loop: . manufacturers = [] for model in df[&#39;name&#39;]: manufacturers.append(model.split(&#39; &#39;)[0]) manufacturers[:10] . [&#39;chevrolet&#39;, &#39;buick&#39;, &#39;plymouth&#39;, &#39;amc&#39;, &#39;ford&#39;, &#39;ford&#39;, &#39;chevrolet&#39;, &#39;plymouth&#39;, &#39;pontiac&#39;, &#39;amc&#39;] . So far so good. Now, can we use the same trick as we did earlier and write the expression as if we&#39;re calculating it for a single value? . df[&#39;name&#39;].split(&#39; &#39;)[0] . AttributeError Traceback (most recent call last) &lt;ipython-input-5-453444d0ed9d&gt; in &lt;module&gt; -&gt; 1 df[&#39;name&#39;].split(&#39; &#39;)[0] ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/generic.py in __getattr__(self, name) 5128 if self._info_axis._can_hold_identifiers_and_holds_name(name): 5129 return self[name] -&gt; 5130 return object.__getattribute__(self, name) 5131 5132 def __setattr__(self, name: str, value) -&gt; None: AttributeError: &#39;Series&#39; object has no attribute &#39;split&#39; . The magic doesn&#39;t work. To put it in simple terms, a pandas Series object &quot;knows&quot; about the * operator, but it doesn&#39;t know anything about split(). If we think about it, we can see why this must be the case. A Series object has to be able to hold either numbers or text, so it doesn&#39;t make sense for it to have methods like split() that only work on text. . One way round this is to switch to apply(), which lets us run arbitrary code on values in a series. We can write a function that takes a single name and returns the manufacturer: . def get_manufacturer(name): return name.split(&#39; &#39;)[0] . Then apply it to the name column: . df[&#39;name&#39;].apply(get_manufacturer) . 0 chevrolet 1 buick 2 plymouth 3 amc 4 ford ... 393 ford 394 vw 395 dodge 396 ford 397 chevy Name: name, Length: 398, dtype: object . But a better way is to use the series str attribute. This attribute has nearly all of the string methods that we might be used to, plus a bunch more functionality specific to pandas. Let&#39;s start with a simpler example - say we want to change all the names to upper case. Here it is with apply(): . df[&#39;name&#39;].apply(str.upper) . 0 CHEVROLET CHEVELLE MALIBU 1 BUICK SKYLARK 320 2 PLYMOUTH SATELLITE 3 AMC REBEL SST 4 FORD TORINO ... 393 FORD MUSTANG GL 394 VW PICKUP 395 DODGE RAMPAGE 396 FORD RANGER 397 CHEVY S-10 Name: name, Length: 398, dtype: object . And here is is using the str attribute: . df[&#39;name&#39;].str.upper() . 0 CHEVROLET CHEVELLE MALIBU 1 BUICK SKYLARK 320 2 PLYMOUTH SATELLITE 3 AMC REBEL SST 4 FORD TORINO ... 393 FORD MUSTANG GL 394 VW PICKUP 395 DODGE RAMPAGE 396 FORD RANGER 397 CHEVY S-10 Name: name, Length: 398, dtype: object . Going back to our original problem, we can split the name into a list of strings like this: . df[&#39;name&#39;].str.split(&#39; &#39;) . 0 [chevrolet, chevelle, malibu] 1 [buick, skylark, 320] 2 [plymouth, satellite] 3 [amc, rebel, sst] 4 [ford, torino] ... 393 [ford, mustang, gl] 394 [vw, pickup] 395 [dodge, rampage] 396 [ford, ranger] 397 [chevy, s-10] Name: name, Length: 398, dtype: object . Now there&#39;s one final complication before we can get just the first word - the result of the expression above is itself a series. So we have to access its str attribute again before we can use square brackets to get just the first word: . df[&#39;name&#39;].str.split(&#39; &#39;).str[0] . 0 chevrolet 1 buick 2 plymouth 3 amc 4 ford ... 393 ford 394 vw 395 dodge 396 ford 397 chevy Name: name, Length: 398, dtype: object . Now we have a series which contains the data we want. At this point we could store it in a new column: . df[&#39;manufacturer&#39;] = df[&#39;name&#39;].str.split(&#39; &#39;).str[0] df . mpg cylinders displacement horsepower weight acceleration model_year origin name manufacturer . 0 18.0 | 8 | 307.0 | 130.0 | 3504 | 12.0 | 70 | usa | chevrolet chevelle malibu | chevrolet | . 1 15.0 | 8 | 350.0 | 165.0 | 3693 | 11.5 | 70 | usa | buick skylark 320 | buick | . 2 18.0 | 8 | 318.0 | 150.0 | 3436 | 11.0 | 70 | usa | plymouth satellite | plymouth | . 3 16.0 | 8 | 304.0 | 150.0 | 3433 | 12.0 | 70 | usa | amc rebel sst | amc | . 4 17.0 | 8 | 302.0 | 140.0 | 3449 | 10.5 | 70 | usa | ford torino | ford | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 393 27.0 | 4 | 140.0 | 86.0 | 2790 | 15.6 | 82 | usa | ford mustang gl | ford | . 394 44.0 | 4 | 97.0 | 52.0 | 2130 | 24.6 | 82 | europe | vw pickup | vw | . 395 32.0 | 4 | 135.0 | 84.0 | 2295 | 11.6 | 82 | usa | dodge rampage | dodge | . 396 28.0 | 4 | 120.0 | 79.0 | 2625 | 18.6 | 82 | usa | ford ranger | ford | . 397 31.0 | 4 | 119.0 | 82.0 | 2720 | 19.4 | 82 | usa | chevy s-10 | chevy | . 398 rows × 10 columns . Or do any other type of series processing: . df[&#39;name&#39;].str.split(&#39; &#39;).str[0].value_counts() . ford 51 chevrolet 43 plymouth 31 amc 28 dodge 28 .. hi 1 chevroelt 1 nissan 1 vokswagen 1 toyouta 1 Name: name, Length: 37, dtype: int64 . This technique is also very useful for filtering. Say we want to find just the cars made by Ford. We can use the startswith method of the str attribute to get a series of boolean values: . df[&#39;name&#39;].str.startswith(&#39;ford&#39;) . 0 False 1 False 2 False 3 False 4 True ... 393 True 394 False 395 False 396 True 397 False Name: name, Length: 398, dtype: bool . Which we can then use as a filter mask: . df[df[&#39;name&#39;].str.startswith(&#39;ford&#39;)] . mpg cylinders displacement horsepower weight acceleration model_year origin name manufacturer . 4 17.0 | 8 | 302.0 | 140.0 | 3449 | 10.5 | 70 | usa | ford torino | ford | . 5 15.0 | 8 | 429.0 | 198.0 | 4341 | 10.0 | 70 | usa | ford galaxie 500 | ford | . 17 21.0 | 6 | 200.0 | 85.0 | 2587 | 16.0 | 70 | usa | ford maverick | ford | . 25 10.0 | 8 | 360.0 | 215.0 | 4615 | 14.0 | 70 | usa | ford f250 | ford | . 32 25.0 | 4 | 98.0 | NaN | 2046 | 19.0 | 71 | usa | ford pinto | ford | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 365 20.2 | 6 | 200.0 | 88.0 | 3060 | 17.1 | 81 | usa | ford granada gl | ford | . 373 24.0 | 4 | 140.0 | 92.0 | 2865 | 16.4 | 82 | usa | ford fairmont futura | ford | . 389 22.0 | 6 | 232.0 | 112.0 | 2835 | 14.7 | 82 | usa | ford granada l | ford | . 393 27.0 | 4 | 140.0 | 86.0 | 2790 | 15.6 | 82 | usa | ford mustang gl | ford | . 396 28.0 | 4 | 120.0 | 79.0 | 2625 | 18.6 | 82 | usa | ford ranger | ford | . 51 rows × 10 columns . There are many useful methods hiding in the str attribute. Let&#39;s find all the cars with names longer than 30 characters, which we can do with the len() method: . long_name_cars = df[df[&#39;name&#39;].str.len() &gt; 30] long_name_cars . mpg cylinders displacement horsepower weight acceleration model_year origin name manufacturer . 73 13.0 | 8 | 307.0 | 130.0 | 4098 | 14.0 | 72 | usa | chevrolet chevelle concours (sw) | chevrolet | . 133 16.0 | 6 | 250.0 | 100.0 | 3781 | 17.0 | 74 | usa | chevrolet chevelle malibu classic | chevrolet | . 187 17.5 | 8 | 305.0 | 140.0 | 4215 | 13.0 | 76 | usa | chevrolet chevelle malibu classic | chevrolet | . 244 43.1 | 4 | 90.0 | 48.0 | 1985 | 21.5 | 78 | europe | volkswagen rabbit custom diesel | volkswagen | . 249 19.9 | 8 | 260.0 | 110.0 | 3365 | 15.5 | 78 | usa | oldsmobile cutlass salon brougham | oldsmobile | . 263 17.7 | 6 | 231.0 | 165.0 | 3445 | 13.4 | 78 | usa | buick regal sport coupe (turbo) | buick | . 292 18.5 | 8 | 360.0 | 150.0 | 3940 | 13.0 | 79 | usa | chrysler lebaron town @ country (sw) | chrysler | . 300 23.9 | 8 | 260.0 | 90.0 | 3420 | 22.2 | 79 | usa | oldsmobile cutlass salon brougham | oldsmobile | . 387 38.0 | 6 | 262.0 | 85.0 | 3015 | 17.0 | 82 | usa | oldsmobile cutlass ciera (diesel) | oldsmobile | . Here&#39;s a common annoyance when making charts with long axis labels: . import matplotlib.pyplot as plt plt.figure(figsize=(8,8)) long_name_cars.set_index(&#39;name&#39;)[&#39;horsepower&#39;].plot(kind=&#39;barh&#39;) None . The long labels take up a bunch of space on the left hand side of the chart. We can fix this by using the .wrap() method to split the names over multiple lines: . plt.figure(figsize=(8,8)) long_name_cars[&#39;display_name&#39;] = long_name_cars[&#39;name&#39;].str.wrap(15) long_name_cars.set_index(&#39;display_name&#39;)[&#39;horsepower&#39;].plot(kind=&#39;barh&#39;) None . One last example: a bunch of the car names end with the string &quot;(sw)&quot;. Let&#39;s find them: . df[df[&#39;name&#39;].str.endswith(&#39;(sw)&#39;)] . mpg cylinders displacement horsepower weight acceleration model_year origin name manufacturer . 13 14.0 | 8 | 455.0 | 225.0 | 3086 | 10.0 | 70 | usa | buick estate wagon (sw) | buick | . 42 12.0 | 8 | 383.0 | 180.0 | 4955 | 11.5 | 71 | usa | dodge monaco (sw) | dodge | . 43 13.0 | 8 | 400.0 | 170.0 | 4746 | 12.0 | 71 | usa | ford country squire (sw) | ford | . 44 13.0 | 8 | 400.0 | 175.0 | 5140 | 12.0 | 71 | usa | pontiac safari (sw) | pontiac | . 45 18.0 | 6 | 258.0 | 110.0 | 2962 | 13.5 | 71 | usa | amc hornet sportabout (sw) | amc | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 289 16.9 | 8 | 350.0 | 155.0 | 4360 | 14.9 | 79 | usa | buick estate wagon (sw) | buick | . 290 15.5 | 8 | 351.0 | 142.0 | 4054 | 14.3 | 79 | usa | ford country squire (sw) | ford | . 291 19.2 | 8 | 267.0 | 125.0 | 3605 | 15.0 | 79 | usa | chevrolet malibu classic (sw) | chevrolet | . 292 18.5 | 8 | 360.0 | 150.0 | 3940 | 13.0 | 79 | usa | chrysler lebaron town @ country (sw) | chrysler | . 340 25.8 | 4 | 156.0 | 92.0 | 2620 | 14.4 | 81 | usa | dodge aries wagon (sw) | dodge | . 28 rows × 10 columns . The &quot;sw&quot; stands for &quot;Station Wagon&quot;. Let&#39;s replace the abbreviation with the full name: . df[&#39;name&#39;] = df[&#39;name&#39;].str.replace(&quot; (sw )&quot;, &quot;(Station Wagon)&quot;) df[40:50] . mpg cylinders displacement horsepower weight acceleration model_year origin name manufacturer . 40 14.0 | 8 | 351.0 | 153.0 | 4154 | 13.5 | 71 | usa | ford galaxie 500 | ford | . 41 14.0 | 8 | 318.0 | 150.0 | 4096 | 13.0 | 71 | usa | plymouth fury iii | plymouth | . 42 12.0 | 8 | 383.0 | 180.0 | 4955 | 11.5 | 71 | usa | dodge monaco (Station Wagon) | dodge | . 43 13.0 | 8 | 400.0 | 170.0 | 4746 | 12.0 | 71 | usa | ford country squire (Station Wagon) | ford | . 44 13.0 | 8 | 400.0 | 175.0 | 5140 | 12.0 | 71 | usa | pontiac safari (Station Wagon) | pontiac | . 45 18.0 | 6 | 258.0 | 110.0 | 2962 | 13.5 | 71 | usa | amc hornet sportabout (Station Wagon) | amc | . 46 22.0 | 4 | 140.0 | 72.0 | 2408 | 19.0 | 71 | usa | chevrolet vega (Station Wagon) | chevrolet | . 47 19.0 | 6 | 250.0 | 100.0 | 3282 | 15.0 | 71 | usa | pontiac firebird | pontiac | . 48 18.0 | 6 | 250.0 | 88.0 | 3139 | 14.5 | 71 | usa | ford mustang | ford | . 49 23.0 | 4 | 122.0 | 86.0 | 2220 | 14.0 | 71 | usa | mercury capri 2000 | mercury | . Notice that we have to escape the parentheses in the &quot;(sw)&quot; string, as the replace() method works on regular expressions. . There are many other useful string processing methods in the pandas str attribute. The documentation page has a bunch of examples, and a method summary at the end. So before you set out to process strings either by writing a loop, or by using apply(), check to see if there&#39;s a method there that will do what you want! .",
            "url": "https://drawingfromdata.com/pandas/strings/working-with-pandas-string-methods.html",
            "relUrl": "/pandas/strings/working-with-pandas-string-methods.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Setting the size of a figure in matplotlib and seaborn",
            "content": "TL;DR . if you&#39;re using plot() on a pandas Series or Dataframe, use the figsize keyword | if you&#39;re using matplotlib directly, use matplotlib.pyplot.figure with the figsize keyword | if you&#39;re using a seaborn function that draws a single plot, use matplotlib.pyplot.figure with the figsize keyword | if you&#39;re using a seaborn function that draws multiple plots, use the height and aspect keyword arguments | . Introduction . Setting figure sizes is one of those things that feels like it should be very straightforward. However, it still manages to show up on the first page of stackoverflow questions for both matplotlib and seaborn. Part of the confusion arises because there are so many ways to do the same thing - this highly upvoted question has six suggested solutions: . manually create an Axes object with the desired size | pass some configuration paramteters to seaborn so that the size you want is the default | call a method on the figure once it&#39;s been created | pass hight and aspect keywords to the seaborn plotting function | use the matplotlib.pyplot interface and call the figure() function | use the matplotlib.pyplot interface to get the current figure then set its size using a method | . each of which will work in some circumstances but not others! . . Tip: For a full overview of the difference between figure and axis level charts, checkout the chapters on styles and small multiples in the Drawing from Data book. . Drawing a figure using pandas . Let&#39;s jump in. As an example we&#39;ll use the olympic medal dataset, which we can load directly from a URL:: . import pandas as pd data = pd.read_csv(&quot;https://raw.githubusercontent.com/mojones/binders/master/olympics.csv&quot;, sep=&quot; t&quot;) data . City Year Sport ... Medal Country Int Olympic Committee code . 0 Athens | 1896 | Aquatics | ... | Gold | Hungary | HUN | . 1 Athens | 1896 | Aquatics | ... | Silver | Austria | AUT | . 2 Athens | 1896 | Aquatics | ... | Bronze | Greece | GRE | . 3 Athens | 1896 | Aquatics | ... | Gold | Greece | GRE | . 4 Athens | 1896 | Aquatics | ... | Silver | Greece | GRE | . ... ... | ... | ... | ... | ... | ... | ... | . 29211 Beijing | 2008 | Wrestling | ... | Silver | Germany | GER | . 29212 Beijing | 2008 | Wrestling | ... | Bronze | Lithuania | LTU | . 29213 Beijing | 2008 | Wrestling | ... | Bronze | Armenia | ARM | . 29214 Beijing | 2008 | Wrestling | ... | Gold | Cuba | CUB | . 29215 Beijing | 2008 | Wrestling | ... | Silver | Russia | RUS | . 29216 rows × 12 columns . For our first figure, we&#39;ll count how many medals have been won in total by each country, then take the top thirty: . data[&#39;Country&#39;].value_counts().head(30) . United States 4335 Soviet Union 2049 United Kingdom 1594 France 1314 Italy 1228 ... Spain 377 Switzerland 376 Brazil 372 Bulgaria 331 Czechoslovakia 329 Name: Country, Length: 30, dtype: int64 . And turn it into a bar chart: . data[&#39;Country&#39;].value_counts().head(30).plot(kind=&#39;barh&#39;) . &lt;AxesSubplot:&gt; . Ignoring other asthetic aspects of the plot, it&#39;s obvious that we need to change the size - or rather the shape. Part of the confusion over sizes in plotting is that sometimes we need to just make the chart bigger or smaller, and sometimes we need to make it thinner or fatter. If we just scaled up this plot so that it was big enough to read the names on the vertical axis, then it would also be very wide. We can set the size by adding a figsize keyword argument to our pandas plot() function. The value has to be a tuple of sizes - it&#39;s actually the horizontal and vertical size in inches, but for most purposes we can think of them as arbirary units. . Here&#39;s what happens if we make the plot bigger, but keep the original shape: . data[&#39;Country&#39;].value_counts().head(30).plot(kind=&#39;barh&#39;, figsize=(20,10)) . &lt;AxesSubplot:&gt; . And here&#39;s a version that keeps the large vertical size but shrinks the chart horizontally so it doesn&#39;t take up so much space: . data[&#39;Country&#39;].value_counts().head(30).plot(kind=&#39;barh&#39;, figsize=(6,10)) . &lt;AxesSubplot:&gt; . Drawing a figure using matplotlib . OK, but what if we aren&#39;t using pandas&#39; convenient plot() method but drawing the chart using matplotlib directly? Let&#39;s look at the number of medals awarded in each year: . plt.plot(data[&#39;Year&#39;].value_counts().sort_index()) . [&lt;matplotlib.lines.Line2D at 0x7fb1bd16de20&gt;] . This time, we&#39;ll say that we want to make the plot longer in the horizontal direction, to better see the pattern over time. If we search the documentation for the matplotlib plot() funtion, we won&#39;t find any mention of size or shape. This actually makes sense in the design of matplotlib - plots don&#39;t really have a size, figures do. So to change it we have to call the figure() function: . import matplotlib.pyplot as plt plt.figure(figsize=(15,4)) plt.plot(data[&#39;Year&#39;].value_counts().sort_index()) . [&lt;matplotlib.lines.Line2D at 0x7fb1bd5f3dc0&gt;] . Notice that with the figure() function we have to call it before we make the call to plot(), otherwise it won&#39;t take effect: . plt.plot(data[&#39;Year&#39;].value_counts().sort_index()) # no effect, the plot has already been drawn plt.figure(figsize=(15,4)) . &lt;Figure size 1080x288 with 0 Axes&gt; . &lt;Figure size 1080x288 with 0 Axes&gt; . Drawing a figure with seaborn . OK, now what if we&#39;re using seaborn rather than matplotlib? Well, happily the same technique will work. We know from our first plot which countries have won the most medals overall, but now let&#39;s look at how this varies by year. We&#39;ll create a summary table to show the number of medals per year for all countries that have won at least 500 medals total. . (ignore this panda stuff if it seems confusing, and just look at the final table) . summary = ( data .groupby(&#39;Country&#39;) .filter(lambda x : len(x) &gt; 500) .groupby([&#39;Country&#39;, &#39;Year&#39;]) .size() .to_frame(&#39;medal count&#39;) .reset_index() ) # wrap long country names summary[&#39;Country&#39;] = summary[&#39;Country&#39;].str.replace(&#39; &#39;, &#39; n&#39;) summary . Country Year medal count . 0 Australia | 1896 | 2 | . 1 Australia | 1900 | 5 | . 2 Australia | 1920 | 6 | . 3 Australia | 1924 | 10 | . 4 Australia | 1928 | 4 | . ... ... | ... | ... | . 309 United nStates | 1992 | 224 | . 310 United nStates | 1996 | 260 | . 311 United nStates | 2000 | 248 | . 312 United nStates | 2004 | 264 | . 313 United nStates | 2008 | 315 | . 314 rows × 3 columns . Now we can do a box plot to show the distribution of yearly medal totals for each country: . import seaborn as sns sns.boxplot( data=summary, x=&#39;Country&#39;, y=&#39;medal count&#39;, color=&#39;red&#39;) . &lt;AxesSubplot:xlabel=&#39;Country&#39;, ylabel=&#39;medal count&#39;&gt; . This is hard to read because of all the names, so let&#39;s space them out a bit: . plt.figure(figsize=(20,5)) sns.boxplot( data=summary, x=&#39;Country&#39;, y=&#39;medal count&#39;, color=&#39;red&#39;) . &lt;AxesSubplot:xlabel=&#39;Country&#39;, ylabel=&#39;medal count&#39;&gt; . Now we come to the final complication; let&#39;s say we want to look at the distributions of the different medal types separately. We&#39;ll make a new summary table - again, ignore the pandas stuff if it&#39;s confusing, and just look at the final table: . summary_by_medal = ( data .groupby(&#39;Country&#39;) .filter(lambda x : len(x) &gt; 500) .groupby([&#39;Country&#39;, &#39;Year&#39;, &#39;Medal&#39;]) .size() .to_frame(&#39;medal count&#39;) .reset_index() ) summary_by_medal[&#39;Country&#39;] = summary_by_medal[&#39;Country&#39;].str.replace(&#39; &#39;, &#39; n&#39;) summary_by_medal . Country Year Medal medal count . 0 Australia | 1896 | Gold | 2 | . 1 Australia | 1900 | Bronze | 3 | . 2 Australia | 1900 | Gold | 2 | . 3 Australia | 1920 | Bronze | 1 | . 4 Australia | 1920 | Silver | 5 | . ... ... | ... | ... | ... | . 881 United nStates | 2004 | Gold | 116 | . 882 United nStates | 2004 | Silver | 75 | . 883 United nStates | 2008 | Bronze | 81 | . 884 United nStates | 2008 | Gold | 125 | . 885 United nStates | 2008 | Silver | 109 | . 886 rows × 4 columns . Now we will switch from boxplot() to the higher level catplot(), as this makes it easy to switch between different plot types. But notice that now our call to plt.figure() gets ignored: . plt.figure(figsize=(20,5)) sns.catplot( data=summary_by_medal, x=&#39;Country&#39;, y=&#39;medal count&#39;, hue=&#39;Medal&#39;, kind=&#39;box&#39; ) . &lt;seaborn.axisgrid.FacetGrid at 0x7fb1bcc23ac0&gt; . &lt;Figure size 1440x360 with 0 Axes&gt; . The reason for this is that the higher level plotting functions in seaborn (what the documentation calls Figure-level interfaces) have a different way of managing size, largely due to the fact that the often produce multiple subplots. To set the size when using catplot() or relplot() (also pairplot(), lmplot() and jointplot()), use the height keyword to control the size and the aspect keyword to control the shape: . sns.catplot( data=summary_by_medal, x=&#39;Country&#39;, y=&#39;medal count&#39;, hue=&#39;Medal&#39;, kind=&#39;box&#39;, height=5, # make the plot 5 units high aspect=3) # height should be three times width . &lt;seaborn.axisgrid.FacetGrid at 0x7fb1bc40d850&gt; . Because we often end up drawing small multiples with catplot() and relplot(), being able to control the shape separately from the size is very convenient. The height and aspect keywords apply to each subplot separately, not to the figure as a whole. So if we put each medal on a separate row rather than using hue, we&#39;ll end up with three subplots, so we&#39;ll want to set the height to be smaller, but the aspect ratio to be bigger: . sns.catplot( data=summary_by_medal, x=&#39;Country&#39;, y=&#39;medal count&#39;, row=&#39;Medal&#39;, kind=&#39;box&#39;, height=3, aspect=4, color=&#39;blue&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7fb1bc04edf0&gt; . Printing a figure . Finally, a word about printing. If the reason that you need to change the size of a plot, rather than the shape, is because you need to print it, then don&#39;t worry about the size - get the shape that you want, then use savefig() to make the plot in SVG format: . plt.savefig(&#39;medals.svg&#39;) . &lt;Figure size 432x288 with 0 Axes&gt; . This will give you a plot in Scalable Vector Graphics format, which stores the actual lines and shapes of the chart so that you can print it at any size - even a giant poster - and it will look sharp. As a nice bonus, you can also edit individual bits of the chart using a graphical SVG editor (Inkscape is free and powerful, though takes a bit of effort to learn). .",
            "url": "https://drawingfromdata.com/pandas/seaborn/matplotlib/visualization/setting-figure-size-matplotlib-seaborn.html",
            "relUrl": "/pandas/seaborn/matplotlib/visualization/setting-figure-size-matplotlib-seaborn.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Rotating axis labels in matplotlib and seaborn",
            "content": "There&#39;s a common pattern which often occurs when working with charting libraries: drawing charts with all the defaults seems very straightforward, but when we want to change some aspect of the chart things get complicated. This pattern is even more noticable when working with a high-level library like seaborn - the library does all sorts of clever things to make our life easier, and lets us draw sophisticated, beautiful charts, so it&#39;s frustrating when we want to change something that feels like it should be simple. . . Tip: For a full overview of styling charts, check out the chapters on styles and color in the Drawing from Data book. . In this article, we&#39;ll take a look at the classic example of this phenomenon - rotating axis tick labels. This seems like such a common thing that it should be easy, but it&#39;s one of the most commonly asked questions on StackOverflow for both seaborn and matplotlib. As an example dataset, we&#39;ll look at a table of Olympic medal winners. We can load it into pandas directly from a URL: . import pandas as pd data = pd.read_csv(&quot;https://raw.githubusercontent.com/mojones/binders/master/olympics.csv&quot;, sep=&quot; t&quot;) data . City Year Sport ... Medal Country Int Olympic Committee code . 0 Athens | 1896 | Aquatics | ... | Gold | Hungary | HUN | . 1 Athens | 1896 | Aquatics | ... | Silver | Austria | AUT | . 2 Athens | 1896 | Aquatics | ... | Bronze | Greece | GRE | . 3 Athens | 1896 | Aquatics | ... | Gold | Greece | GRE | . 4 Athens | 1896 | Aquatics | ... | Silver | Greece | GRE | . ... ... | ... | ... | ... | ... | ... | ... | . 29211 Beijing | 2008 | Wrestling | ... | Silver | Germany | GER | . 29212 Beijing | 2008 | Wrestling | ... | Bronze | Lithuania | LTU | . 29213 Beijing | 2008 | Wrestling | ... | Bronze | Armenia | ARM | . 29214 Beijing | 2008 | Wrestling | ... | Gold | Cuba | CUB | . 29215 Beijing | 2008 | Wrestling | ... | Silver | Russia | RUS | . 29216 rows × 12 columns . Each row is a single medal, and we have a bunch of different information like where and when the event took place, the classification of the event, and the name of the athlete that won. . We&#39;ll start with something simple; let&#39;s grab all the events for the 1980 games and see how many fall into each type of sport: . import seaborn as sns import matplotlib.pyplot as plt # set the figure size plt.figure(figsize=(10,5)) # draw the chart chart = sns.countplot( data=data[data[&#39;Year&#39;] == 1980], x=&#39;Sport&#39;, palette=&#39;Set1&#39; ) . Here we have the classic problem with categorical data: we need to display all the labels and because some of them are quite long, they overlap. How are we going to rotate them? The key is to look at what type of object we&#39;ve created. What is the type of the return value from the countplot() function, which we have stored in chart? . type(chart) . matplotlib.axes._subplots.AxesSubplot . Looks like chart is a matplotlib AxesSubplot object. This actually doesn&#39;t help us very much - if we go searching for the documentation for AxesSubplot we won&#39;t find anything useful. Instead, we have to know that an AxesSubplot is a type of Axes object, and now we can go look up the documentation for Axes in which we find the set_xticklabels() method. . Looking at the documentation for set_xticklabels() we don&#39;t actually see any obvious reference to rotation. The clue we&#39;re looking for is in the &quot;Other parameters&quot; section at the end, where it tells us that we can supply a list of keyword arguments that are properties of Text objects. . Finally, in the documentation for Text objects we can see a list of the properties, including rotation. This was a long journey! but hopefully it will pay off - there are lots of other useful properties here as well. Now we can finally set the rotation: . plt.figure(figsize=(10,5)) chart = sns.countplot( data=data[data[&#39;Year&#39;] == 1980], x=&#39;Sport&#39;, palette=&#39;Set1&#39; ) chart.set_xticklabels(rotation=45) . TypeError Traceback (most recent call last) &lt;ipython-input-27-059eaf6ffa77&gt; in &lt;module&gt; 5 palette=&#39;Set1&#39; 6 ) -&gt; 7 chart.set_xticklabels(rotation=45) ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/matplotlib/axes/_base.py in wrapper(self, *args, **kwargs) 61 62 def wrapper(self, *args, **kwargs): &gt; 63 return get_method(self)(*args, **kwargs) 64 65 wrapper.__module__ = owner.__module__ ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs) 449 &#34;parameter will become keyword-only %(removal)s.&#34;, 450 name=name, obj_type=f&#34;parameter of {func.__name__}()&#34;) --&gt; 451 return func(*args, **kwargs) 452 453 return wrapper TypeError: _set_ticklabels() missing 1 required positional argument: &#39;labels&#39; . Disaster! We need to pass set_xticklabels() a list of the actual labels we want to use. Since we don&#39;t want to change the labels themselves, we can just call get_xticklabels(): . plt.figure(figsize=(10,5)) chart = sns.countplot( data=data[data[&#39;Year&#39;] == 1980], x=&#39;Sport&#39;, palette=&#39;Set1&#39; ) chart.set_xticklabels(chart.get_xticklabels(), rotation=45) None #don&#39;t show the label objects . This looks better, but notice how the &quot;Modern Pentathlon&quot; label is running into the &quot;Sailing&quot; label? That&#39;s because the labels have been rotated about their center - which also makes it hard to see which label belongs to which bar. We should also set the horizontal alignment to &quot;right&quot;: . plt.figure(figsize=(10,5)) chart = sns.countplot( data=data[data[&#39;Year&#39;] == 1980], x=&#39;Sport&#39;, palette=&#39;Set1&#39; ) chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) None #don&#39;t show the label objects . And just to show a few more things that we can do with set_xticklabels() we&#39;ll also set the font weight to be a bit lighter, and the font size to be a bit bigger: . plt.figure(figsize=(10,5)) chart = sns.countplot( data=data[data[&#39;Year&#39;] == 1980], x=&#39;Sport&#39;, palette=&#39;Set1&#39; ) chart.set_xticklabels( chart.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;, fontweight=&#39;light&#39;, fontsize=&#39;x-large&#39; ) None #don&#39;t show the label objects . In all of these examples, we&#39;ve been using the object-oriented interface to matplotlib - notice that we&#39;re calling set_xticklabels() directly on the chart object. . Another object is to use the pyplot interface. There&#39;s a method simply called xticks() which we could use like this: . import matplotlib.pyplot as plt plt.figure(figsize=(10,5)) chart = sns.countplot( data=data[data[&#39;Year&#39;] == 1980], x=&#39;Sport&#39;, palette=&#39;Set1&#39; ) plt.xticks( rotation=45, horizontalalignment=&#39;right&#39;, fontweight=&#39;light&#39;, fontsize=&#39;x-large&#39; ) None #don&#39;t show the label objects . Notice that when we do it this way the list of labels is optional, so we don&#39;t need to call get_xticklabels(). . Althought the pyplot interface is easier to use in this case, in general I find it clearer to use the object-oriented interface, as it tends to be more explicit. . Everything that we&#39;ve seen above applies if we&#39;re using matplotlib directly instead of seaborn: once we have an Axes object, we can call set_xticklabels() on it. Let&#39;s do the same thing using pandas&#39;s built in plotting function: . chart = data[data[&#39;Year&#39;] == 1980][&#39;Sport&#39;].value_counts().plot(kind=&#39;bar&#39;) chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) None . Dealing with multiple plots . Let&#39;s try another plot. One of the great features of seaborn is that it makes it very easy to draw multiple plots. Let&#39;s see how the distribution of medals in each sport changed between 1980 and 2008: . chart = sns.catplot( data=data[data[&#39;Year&#39;].isin([1980, 2008])], x=&#39;Sport&#39;, kind=&#39;count&#39;, palette=&#39;Set1&#39;, row=&#39;Year&#39;, aspect=3, height=3 ) . As before, the labels need to be rotated. Let&#39;s try the approach that we used before: . chart = sns.catplot( data=data[data[&#39;Year&#39;].isin([1980, 2008])], x=&#39;Sport&#39;, kind=&#39;count&#39;, palette=&#39;Set1&#39;, row=&#39;Year&#39;, aspect=3, height=3 ) chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) . AttributeError Traceback (most recent call last) &lt;ipython-input-8-69ed9d536d8c&gt; in &lt;module&gt; 8 height=3 9 ) &gt; 10 chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) AttributeError: &#39;FacetGrid&#39; object has no attribute &#39;get_xticklabels&#39; . We run into an error. Note that the missing attribute is not set_xticklabels() but get_xticklabels(). The reason why this approach worked for countplot() and not for factorplot() is that the output from countplot() is a single Axes object, as we saw above, but the output from factorplot() is a seaborn FacetGrid object: . type(chart) . seaborn.axisgrid.FacetGrid . whose job is to store a collection of multiple axes - two in this case. So how to rotate the labels? It turns out that FacetGrid has its own version of set_xticklabels that will take care of things: . chart = sns.catplot( data=data[data[&#39;Year&#39;].isin([1980, 2008])], x=&#39;Sport&#39;, kind=&#39;count&#39;, palette=&#39;Set1&#39;, row=&#39;Year&#39;, aspect=3, height=3 ) chart.set_xticklabels(rotation=65, horizontalalignment=&#39;right&#39;) None . The pyplot interface that we saw earlier also works fine: . chart = sns.catplot( data=data[data[&#39;Year&#39;].isin([1980, 2008])], x=&#39;Sport&#39;, kind=&#39;count&#39;, palette=&#39;Set1&#39;, row=&#39;Year&#39;, aspect=3, height=3 ) plt.xticks(rotation=65, horizontalalignment=&#39;right&#39;) None . And, of course, everything that we&#39;ve done here will work for y-axis labels as well - we typically don&#39;t need to change their rotation, but we might want to set their other properties. As an example, let&#39;s count how many medals were won at each Olypmic games for each country in each year. To keep the dataset managable, we&#39;ll just look at countries that have won more than 500 metals in total: . by_sport = (data .groupby(&#39;Country&#39;) .filter(lambda x : len(x) &gt; 500) .groupby([&#39;Country&#39;, &#39;Year&#39;]) .size() .unstack() ) by_sport . Year 1896 1900 1904 ... 2000 2004 2008 . Country . Australia 2.0 | 5.0 | NaN | ... | 183.0 | 157.0 | 149.0 | . Canada NaN | 2.0 | 35.0 | ... | 31.0 | 17.0 | 34.0 | . China NaN | NaN | NaN | ... | 79.0 | 94.0 | 184.0 | . East Germany NaN | NaN | NaN | ... | NaN | NaN | NaN | . France 11.0 | 185.0 | NaN | ... | 66.0 | 53.0 | 76.0 | . ... ... | ... | ... | ... | ... | ... | ... | . Russia NaN | NaN | NaN | ... | 188.0 | 192.0 | 143.0 | . Soviet Union NaN | NaN | NaN | ... | NaN | NaN | NaN | . Sweden NaN | 1.0 | NaN | ... | 32.0 | 12.0 | 7.0 | . United Kingdom 7.0 | 78.0 | 2.0 | ... | 55.0 | 57.0 | 77.0 | . United States 20.0 | 55.0 | 394.0 | ... | 248.0 | 264.0 | 315.0 | . 17 rows × 26 columns . If the use of two groupby() method calls is confusing, take a look at this article on grouping. The first one just gives us the rows belonging to countries that have won more than 500 medals; the second one does the aggregation and fills in missing data. The natural way to display a table like this is as a heatmap: . plt.figure(figsize=(10,10)) g = sns.heatmap( by_sport, square=True, # make cells square cbar_kws={&#39;fraction&#39; : 0.01}, # shrink colour bar cmap=&#39;OrRd&#39;, # use orange/red colour map linewidth=1 # space between cells ) . This example is perfectly readable, but by way of an example we&#39;ll rotate both the x and y axis labels: . plt.figure(figsize=(10,10)) g = sns.heatmap( by_sport, square=True, cbar_kws={&#39;fraction&#39; : 0.01}, cmap=&#39;OrRd&#39;, linewidth=1 ) g.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) g.set_yticklabels(g.get_yticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) None # prevent the list of label objects showing up annoyingly in the output . OK, I think that covers it. That was an agonizingly long article to read just about rotating labels, but hopefully it&#39;s given you an insight into what&#39;s going on. It all comes down to understanding what type of object you&#39;re working with - an Axes, a FacetGrid, or a PairGrid. . If you encounter a situation where none of these work, drop me an email at martin@drawingwithdata.com and I&#39;ll update this article! .",
            "url": "https://drawingfromdata.com/seaborn/matplotlib/visualization/rotate-axis-labels-matplotlib-seaborn.html",
            "relUrl": "/seaborn/matplotlib/visualization/rotate-axis-labels-matplotlib-seaborn.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "When to use aggreagate/filter/transform with pandas",
            "content": "I&#39;ve been teaching quite a lot of Pandas recently, and a lot of the recurring questions are about grouping. That&#39;s no surprise, as it&#39;s one of the most flexible features of Pandas. However, that flexibility also makes it sometimes confusing. . Tip: For a much more detailed explanation of grouping operations, checkout the chapter on working with groups in the Drawing from Data book. . I think that most of the confusion arises because the same grouping logic is used for (at least) three distinct operations in Pandas. In the order that we normally learn them, these are: . calculating some aggregate measurement for each group (size, mean, etc.) | filtering the rows on a property of the group they belong to | calculating a new value for each row based on a property of the group. | . This leads commonly to situations where we know that we need to use groupby() - and may even be able to easily figure out what the arguments to groupby() should be - but are unsure about what to do next. . Here&#39;s a trick that I&#39;ve found useful when teaching these ideas: think about the result you want, and work back from there. If you want to get a single value for each group, use aggregate() (or one of its shortcuts). If you want to get a subset of the original rows, use filter(). And if you want to get a new value for each original row, use transpose(). . Here&#39;s a minimal example of the three different situations, all of which require exactly the same call to groupby() but which do different things with the result. We&#39;ll use the well known tips dataset which we can load directly from the web: . import pandas as pd df = pd.read_csv(&quot;https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv&quot;) pd.options.display.max_rows = 10 df . total_bill tip sex smoker day time size . 0 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | . 1 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | . 2 21.01 | 3.50 | Male | No | Sun | Dinner | 3 | . 3 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | . 4 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | . ... ... | ... | ... | ... | ... | ... | ... | . 239 29.03 | 5.92 | Male | No | Sat | Dinner | 3 | . 240 27.18 | 2.00 | Female | Yes | Sat | Dinner | 2 | . 241 22.67 | 2.00 | Male | Yes | Sat | Dinner | 2 | . 242 17.82 | 1.75 | Male | No | Sat | Dinner | 2 | . 243 18.78 | 3.00 | Female | No | Thur | Dinner | 2 | . 244 rows × 7 columns . If you&#39;re not familiar with this dataset, all you need to know is that each row represents a meal at a restaurant, and the columns store the value of the total bill and the tip, plus some metadata about the customer - their sex, whether or not they were a smoker, what day and time they ate at, and the size of their party. Also, notice that we have 244 rows - this will be important later on. . What was the average total bill on each day? . To answer this, let&#39;s imagine that we have already figured out that we need to group by day: . df.groupby(&#39;day&#39;) . now what&#39;s the next step? Use the trick that I just described and start by imagining what we want the output to look like. We want a single value for each group, so we need to use aggregate(): . df.groupby(&#39;day&#39;).aggregate(&#39;mean&#39;) . total_bill tip size . day . Fri 17.151579 | 2.734737 | 2.105263 | . Sat 20.441379 | 2.993103 | 2.517241 | . Sun 21.410000 | 3.255132 | 2.842105 | . Thur 17.682742 | 2.771452 | 2.451613 | . We&#39;re only interested in the total_bill column, so we can select it (either before or after we do the aggregation): . df.groupby(&#39;day&#39;)[&#39;total_bill&#39;].aggregate(&#39;mean&#39;) . day Fri 17.151579 Sat 20.441379 Sun 21.410000 Thur 17.682742 Name: total_bill, dtype: float64 . Pandas has lots of shortcuts for the various ways to aggregate group values - we could use mean() here instead: . df.groupby(&#39;day&#39;)[&#39;total_bill&#39;].mean() . day Fri 17.151579 Sat 20.441379 Sun 21.410000 Thur 17.682742 Name: total_bill, dtype: float64 . Which meals were eaten on days where the average bill was greater than 20? . For this question, think again about the output we want - our goal here is to get a subset of the original rows, so this is a job for filter(). The argument to filter() must be a function or lambda that will take a group and return True or False to determine whether rows belonging to that group should be included in the output. Here&#39;s how we might do it with a lambda: . df.groupby(&#39;day&#39;).filter(lambda x : x[&#39;total_bill&#39;].mean() &gt; 20) . total_bill tip sex smoker day time size . 0 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | . 1 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | . 2 21.01 | 3.50 | Male | No | Sun | Dinner | 3 | . 3 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | . 4 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | . ... ... | ... | ... | ... | ... | ... | ... | . 238 35.83 | 4.67 | Female | No | Sat | Dinner | 3 | . 239 29.03 | 5.92 | Male | No | Sat | Dinner | 3 | . 240 27.18 | 2.00 | Female | Yes | Sat | Dinner | 2 | . 241 22.67 | 2.00 | Male | Yes | Sat | Dinner | 2 | . 242 17.82 | 1.75 | Male | No | Sat | Dinner | 2 | . 163 rows × 7 columns . Notice that our output dataframe has only 163 rows (compared to the 244 that we started with), and that the columns are exactly the same as the input. . Compared to our first example, it&#39;s a bit harder to see why this is useful - typically we&#39;ll do a filter like this and then follow it up with another operation. For example, we might want to compare the average party size on days where the average bill is high: . # surrounding parens let us split the different parts of the expression # over multiple lines ( df .groupby(&#39;day&#39;) .filter( lambda x : x[&#39;total_bill&#39;].mean() &gt; 20) [&#39;size&#39;] .mean() ) . 2.668711656441718 . with the average party size on days where the average bill is low: . ( df .groupby(&#39;day&#39;) .filter(lambda x : x[&#39;total_bill&#39;].mean() &lt;= 20) [&#39;size&#39;] .mean() ) . 2.3703703703703702 . Incidentally, a question that I&#39;m often asked is what the type of the argument to the lambda is - what actually is the variable x in our examples above? We can find out by passing a lambda that just prints the type of its input: . df.groupby(&#39;day&#39;).filter(lambda x: print(type(x))) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; . total_bill tip sex smoker day time size . And we see that each group is passed to our lambda function as a Pandas DataFrame, so we already know how to use it. . How did the cost of each meal compare to the average for the day? . This last example is the trickiest to understand, but remember our trick - start by thinking about the desired output. In this case we are trying to generate a new value for each input row - the total bill divided by the average total bill for each day. (If you have a scientific or maths background then you might think of this as a normalized or scaled total bill). To make a new value for each row, we use transform(). . To start with, let&#39;s see what happens when we pass in a lambda to transform() that just gives us the mean of its input: . df.groupby(&#39;day&#39;).transform(lambda x : x.mean()) . total_bill tip size . 0 21.410000 | 3.255132 | 2.842105 | . 1 21.410000 | 3.255132 | 2.842105 | . 2 21.410000 | 3.255132 | 2.842105 | . 3 21.410000 | 3.255132 | 2.842105 | . 4 21.410000 | 3.255132 | 2.842105 | . ... ... | ... | ... | . 239 20.441379 | 2.993103 | 2.517241 | . 240 20.441379 | 2.993103 | 2.517241 | . 241 20.441379 | 2.993103 | 2.517241 | . 242 20.441379 | 2.993103 | 2.517241 | . 243 17.682742 | 2.771452 | 2.451613 | . 244 rows × 3 columns . Notice that we get the same number of output rows as input rows - Pandas has calculated the mean for each group, then used the results as the new values for each row. We&#39;re only interested in the total bill, so let&#39;s get rid of the other columns: . df.groupby(&#39;day&#39;)[&#39;total_bill&#39;].transform(lambda x : x.mean()) . 0 21.410000 1 21.410000 2 21.410000 3 21.410000 4 21.410000 ... 239 20.441379 240 20.441379 241 20.441379 242 20.441379 243 17.682742 Name: total_bill, Length: 244, dtype: float64 . This gives us a series with the same number of rows as our input data. We could assign this to a new column in our dataframe: . df[&#39;day_average&#39;] = df.groupby(&#39;day&#39;)[&#39;total_bill&#39;].transform(lambda x : x.mean()) df . total_bill tip sex smoker day time size day_average . 0 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | 21.410000 | . 1 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | 21.410000 | . 2 21.01 | 3.50 | Male | No | Sun | Dinner | 3 | 21.410000 | . 3 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | 21.410000 | . 4 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | 21.410000 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 239 29.03 | 5.92 | Male | No | Sat | Dinner | 3 | 20.441379 | . 240 27.18 | 2.00 | Female | Yes | Sat | Dinner | 2 | 20.441379 | . 241 22.67 | 2.00 | Male | Yes | Sat | Dinner | 2 | 20.441379 | . 242 17.82 | 1.75 | Male | No | Sat | Dinner | 2 | 20.441379 | . 243 18.78 | 3.00 | Female | No | Thur | Dinner | 2 | 17.682742 | . 244 rows × 8 columns . Which would allow us to calculate the scaled total bills: . df[&#39;total_bill&#39;] / df[&#39;day_average&#39;] . 0 0.793554 1 0.482952 2 0.981317 3 1.106025 4 1.148529 ... 239 1.420159 240 1.329656 241 1.109025 242 0.871761 243 1.062052 Length: 244, dtype: float64 . But we could also calculate the scaled bill as part of the transform: . df[&#39;scaled bill&#39;] = df.groupby(&#39;day&#39;)[&#39;total_bill&#39;].transform(lambda x : x/x.mean()) df.head() . total_bill tip sex smoker day time size day_average scaled bill . 0 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | 21.41 | 0.793554 | . 1 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | 21.41 | 0.482952 | . 2 21.01 | 3.50 | Male | No | Sun | Dinner | 3 | 21.41 | 0.981317 | . 3 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | 21.41 | 1.106025 | . 4 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | 21.41 | 1.148529 | . In conclusion . All of our three examples used exactly the same groupby() call to begin with: . df.groupby(&#39;day&#39;)[&#39;total_bill&#39;].mean() df.groupby(&#39;day&#39;).filter(lambda x : x[&#39;total_bill&#39;].mean() &gt; 20) df.groupby(&#39;day&#39;)[&#39;total_bill&#39;].transform(lambda x : x/x.mean()) . but by doing different things with the resulting groups we get very different outputs. To reiterate: . if we want to get a single value for each group -&gt; use aggregate() | if we want to get a subset of the input rows -&gt; use filter() | if we want to get a new value for each input row -&gt; use transform() | .",
            "url": "https://drawingfromdata.com/pandas/grouping/pandas-groupby-transform-aggregate-filter.html",
            "relUrl": "/pandas/grouping/pandas-groupby-transform-aggregate-filter.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Does reducing numerical precision affect real world datasets?",
            "content": "Executive summary . Switching from 64 bit to 32 bit precision for floating point data in pandas saves memory and has virtually no effect on the data, as long as the values aren&#39;t greater than around 10300 . Slightly longer summary . I wrote code to download ~1,000 real world datasets from kaggle and look at summary statistics and linear regressions of floating point columns in both 64 and 32 bit precision | The final analysis involved ~1,000 datasets, ~8,000 data files, ~500,000,000 individual floating point values and ~500,000 linear regressions. | Changing to 32 bit precision introduced a mean proportional error of around 10-8 in the summary statistics, with the worst affected columns having a proportional error of around 10-6 | For linear regressions, the mean proportional change in slope and intercept was on the order of 10-6 | A small number of outliers with near-zero slope (representing uninteresting relationships) had larger proportional errors | In ~400,000 valid linear regressions I was unable to find any relationships whose significance at the 5% level was changed by moving from 64 to 32 bit precision | For real world datasets, switching from 64 to 32 bit floating point precision is overwhelmingly unlikely to alter our conclusions about the data, and is an easy way to reduce memory usage. | . Introduction . As you&#39;ve probably noticed, this is a longish article, so grab a cup of coffee if you&#39;re planning to read it all. If you&#39;re already familiar with floating point precision and error in pandas you can probably skip part one. Most of the methods stuff is in part two, so if you just want to see the results, skip to part three. Also, just to head off the obvious criticism, I think that most of the conclusions of this article could have been arrived at by pure reasoning, without looking at any data at all. But hopefully some folks will, like me, be more satisfied by empirical data :-) . Obviously, I think that pandas is wonderful :-) But it has a number of quirks, among them a reputation for being memory hungry. There are no shortage of articles online discussing how to fit large dataframes in memory, and there are a number of well established techniques (there&#39;s a whole chapter on the subject in the Drawing from Data book, and also an extended discussion in the car accidents dataset video). . One technique that always gets mentioned is storing numbers in a reduced precision data type. This is usually proposed as one of the last things to try after other options have been exhausted. I think there are a couple of reasons for this. One is that messing about with data types is a bit harder to explain than simple things like ignoring unneeded columns. Another is that deliberately throwing away information - which is what reducing precision will do - feels very uncomfortable from a data analysis point of view! . One thing that I&#39;ve rarely seen in such discussions is a measurement of how much of an effect reduced precision will have in real life datasets. This is surprising, since pandas makes it very easy to measure. So let&#39;s go ahead and try it... . Part one: changing data types and measuring differences . Before we dive in, a quick reminder of the pandas tools we&#39;ll use. All of the methods etc. are described in detail in the Drawing from Data book, so we&#39;ll limit ourselves here to an overview. . As an example, let&#39;s load the planets dataset: . import pandas as pd pd.options.display.max_columns=4 planets = pd.read_csv(&#39;https://raw.githubusercontent.com/mwaskom/seaborn-data/master/planets.csv&#39;) planets . method number ... distance year . 0 Radial Velocity | 1 | ... | 77.40 | 2006 | . 1 Radial Velocity | 1 | ... | 56.95 | 2008 | . 2 Radial Velocity | 1 | ... | 19.84 | 2011 | . 3 Radial Velocity | 1 | ... | 110.62 | 2007 | . 4 Radial Velocity | 1 | ... | 119.47 | 2009 | . ... ... | ... | ... | ... | ... | . 1030 Transit | 1 | ... | 172.00 | 2006 | . 1031 Transit | 1 | ... | 148.00 | 2007 | . 1032 Transit | 1 | ... | 174.00 | 2007 | . 1033 Transit | 1 | ... | 293.00 | 2008 | . 1034 Transit | 1 | ... | 260.00 | 2008 | . 1035 rows × 6 columns . which stores information about planets that have been discovered around other stars. We are interested in the orbital period column, which we&#39;ll store in a variable for convenience: . op = planets[&#39;orbital_period&#39;] op . 0 269.300000 1 874.774000 2 763.000000 3 326.030000 4 516.220000 ... 1030 3.941507 1031 2.615864 1032 3.191524 1033 4.125083 1034 4.187757 Name: orbital_period, Length: 1035, dtype: float64 . which tells us how long each planet takes to revolve around its host star. Because it&#39;s comprised of floating point numbers, it get the float64 data type by default. To change the data type, we can just use the as_type method. Let&#39;s try float32: . op.astype(&#39;float32&#39;) . 0 269.299988 1 874.773987 2 763.000000 3 326.029999 4 516.219971 ... 1030 3.941507 1031 2.615864 1032 3.191524 1033 4.125083 1034 4.187757 Name: orbital_period, Length: 1035, dtype: float32 . All we need to know about these two data types is that, as the names suggest, a single float64 value uses 64 bits of memory, whereas a float32 uses, you guessed it, 32 bits. So it comes as no great surprise that the memory usage of the float32 version is very close to half that of the float64 version: . op.memory_usage(), op.astype(&#39;float32&#39;).memory_usage() . (8408, 4268) . As well as a difference in memory usage, we can see from the output above that there are slight differences in the values themselves. This is perhaps easiest to see if we put the two series next to each other: . pd.concat([op, op.astype(&#39;float32&#39;)], axis=1) . orbital_period orbital_period . 0 269.300000 | 269.299988 | . 1 874.774000 | 874.773987 | . 2 763.000000 | 763.000000 | . 3 326.030000 | 326.029999 | . 4 516.220000 | 516.219971 | . ... ... | ... | . 1030 3.941507 | 3.941507 | . 1031 2.615864 | 2.615864 | . 1032 3.191524 | 3.191524 | . 1033 4.125083 | 4.125083 | . 1034 4.187757 | 4.187757 | . 1035 rows × 2 columns . To figure out how different the two series are, pandas makes it easy to just take the difference: . op.astype(&#39;float32&#39;) - op . 0 -1.220703e-05 1 -1.318359e-05 2 0.000000e+00 3 -1.220703e-06 4 -2.929688e-05 ... 1030 1.264038e-08 1031 3.846741e-08 1032 -1.096405e-07 1033 1.696655e-07 1034 1.522827e-08 Name: orbital_period, Length: 1035, dtype: float64 . It&#39;s probably more useful to view these as absolute difference, i.e. to ignore the sign: . (op.astype(&#39;float32&#39;) - op).abs() . 0 1.220703e-05 1 1.318359e-05 2 0.000000e+00 3 1.220703e-06 4 2.929688e-05 ... 1030 1.264038e-08 1031 3.846741e-08 1032 1.096405e-07 1033 1.696655e-07 1034 1.522827e-08 Name: orbital_period, Length: 1035, dtype: float64 . Now we can look at a summary: . (op.astype(&#39;float32&#39;) - op).abs().describe() . count 9.920000e+02 mean 3.887688e-06 std 1.556957e-05 min 0.000000e+00 25% 1.572113e-08 50% 1.246643e-07 75% 1.220703e-06 max 1.953125e-04 Name: orbital_period, dtype: float64 . The general picture here is that the change from float64 to float32 has made very little difference to the values. On average, it introduces an absolute error of around 4x10-6, or 0.000004, and the single worst-case value has an absolute error of about 0.0002. These errors are very small relative to the range of the original values: . op.describe() . count 992.000000 mean 2002.917596 std 26014.728304 min 0.090706 25% 5.442540 50% 39.979500 75% 526.005000 max 730000.000000 Name: orbital_period, dtype: float64 . Proportional differences . If we wanted to compare the errors meaningfully across multiple different datasets, it might be useful to look at the error as a proportion of the original values: . ((op.astype(&#39;float32&#39;) - op).abs() / op).describe() . count 9.920000e+02 mean 1.733438e-08 std 1.494138e-08 min 0.000000e+00 25% 2.540837e-09 50% 1.536689e-08 75% 2.830340e-08 max 5.878977e-08 Name: orbital_period, dtype: float64 . This calculation is a bit more involved, but easier to interpret. It is telling us that the switch from float64 to float32 changed the values on average by about 10-8 - a truly miniscule chanage. . Another way to look at the difference is to compare the summaries of the two series. For any given aggregation we can figure out the proportional change by taking the absolute difference and dividing by the original: . import numpy as np np.abs(op.mean() - op.astype(&#39;float32&#39;).mean()) / op.mean() . 5.7728789459559594e-08 . So changing the series to float32 alters the mean by a factor of around 10-8. We can do the same calculation with any other summary statistic - for instance, the standard deviation: . np.abs(op.std() - op.astype(&#39;float32&#39;).std()) / op.std() . 8.132412130848449e-09 . Comparing correlations . Another way to measure the effect of reduced precision is to see if it changes the correlation between floating point columns. In our planets dataset we have two other floating point columns: the mass and the distance. Using pandas&#39; built in corr method we can calculate the pairwise pearson correlation of all the floating point columns: . planets.select_dtypes(&#39;float64&#39;).corr() . orbital_period mass distance . orbital_period 1.000000 | 0.173725 | -0.034365 | . mass 0.173725 | 1.000000 | 0.274082 | . distance -0.034365 | 0.274082 | 1.000000 | . Then do the same with a float32 version and take the difference: . ( planets.select_dtypes(&#39;float64&#39;).corr() - planets.select_dtypes(&#39;float64&#39;).astype(&#39;float32&#39;).corr() ) . orbital_period mass distance . orbital_period 0.000000e+00 | -6.383500e-10 | 7.736160e-11 | . mass -6.383500e-10 | 0.000000e+00 | 1.046595e-09 | . distance 7.736160e-11 | 1.046595e-09 | 0.000000e+00 | . As we can see, the differences are tiny, and would definitely not alter our conclusions about the relationships in the dataset. For a more sophisticated look at correlations, we could bring in a linear regression: . from scipy.stats import linregress linregress( planets.dropna()[&#39;orbital_period&#39;], planets.dropna()[&#39;mass&#39;] ) . LinregressResult(slope=0.00045766570228103964, intercept=2.1268128078962634, rvalue=0.1849061860050771, pvalue=3.298187969140348e-05, stderr=0.00010921992411493946) . and again take the difference between the 64 and 32 bit versions: . l_64 = linregress( planets.dropna()[&#39;orbital_period&#39;], planets.dropna()[&#39;mass&#39;] ) l_32 = linregress( planets.dropna()[&#39;orbital_period&#39;].astype(&#39;float32&#39;), planets.dropna()[&#39;mass&#39;].astype(&#39;float32&#39;) ) print(f&#39;slope difference : {l_64[0] - l_32[0]} &#39;) print(f&#39;intercept difference : {l_64[1] - l_32[1]} &#39;) print(f&#39;R value difference : {l_64[2] - l_32[2]} &#39;) . slope difference : -1.6647770943907514e-12 intercept difference : 2.81312630967534e-08 R value difference : -1.7317597356125702e-10 . Once again, the difference is incredibly minor. Given that the original values might be very small, a proprtional change might be easier to interpret: . print(f&#39;proportional slope difference : {(l_64[0] - l_32[0]) / l_64[0]} &#39;) print(f&#39;proportional intercept difference : {(l_64[1] - l_32[1]) / l_64[1]} &#39;) print(f&#39;proportional R value difference : {(l_64[2] - l_32[2]) / l_64[2]} &#39;) . proportional slope difference : -3.6375395536379053e-09 proportional intercept difference : 1.3226957723928434e-08 proportional R value difference : -9.365612763031196e-10 . Part two: let&#39;s try it on a larger scale . Attentive readers will have noticed that all of the calculations above were done by Python code :-) That means that it should be possible to automate this process and try it across a large number of different datasets. For a convenient source of real world dataset we turn to kaggle. . The collection of datasets hosted at Kaggle is perfect for our purposes. It contains real world data from a massive range of different fields and sources, mostly stored in files that will be easy to get in to pandas. It also has a convenient API with a Python client library, which will make it easy to download a large collection of different data files. . Downloading the data . To download our collection of datasets using the Kaggle API we have to go through a couple of steps. First we will connect to the API and authenticate: . from kaggle.api.kaggle_api_extended import KaggleApi api = KaggleApi() api.authenticate() . Next we can get a list of datasets by using the dataset_list method. We will search for CSV files, since they will be the easiest to load into pandas, and use pagination to get a large collection of dataset objects. Here I&#39;m using the tqdm package for a progress bar, since this code takes a long time to run. I&#39;m not sure if it&#39;s necessary, but I&#39;m also putting in a 1-second delay between requests in an attempt to be a good internet citizen: . from tqdm.notebook import trange, tqdm import time dataset = [] for page in trange(1,1000): datasets.append(api.dataset_list(file_type=&#39;csv&#39;, page=page)) time.sleep(1) . Now we have a list of dataset objects, we can get on to the actual downloading. To keep things organized I&#39;m using a separate folder for each dataset (which might end up containing multiple files). Once I&#39;ve created the folder it&#39;s just a case of calling dataset_download_files with each dataset reference from the previous step and the new folder name as the destination. I&#39;m also using unzip=True to make sure that we end up with extracted files. This will obviously take more disk space than leaving them compressed. . for dat in tqdm(all_datasets): folder_name = dat.ref.replace(&#39;/&#39;, &#39;-&#39;) if not os.path.exists(folder_name): os.mkdir(folder_name) print(dat.ref) api.dataset_download_files(str(dat.ref),path=folder_name, unzip=True) . Processing the downloaded files . Now that we&#39;ve done the download step, we can get on to reading our data files and measuring the effect of reducing precision. Let&#39;s take a look at the function to take a single file and just give us the columns we&#39;re interested in: . def get_float_columns(filepath): try: df = pd.read_csv(filepath, encoding=&#39;latin-1&#39;).select_dtypes([&#39;float64&#39;]) except: print(f&#39;cannot read {filepath}&#39;) return None # replace infinite values with missing data df.replace([np.inf, -np.inf], np.nan, inplace=True) # get rid of columns that are only floats because they contain missing data # i.e. they should actually be integers keep = [] for c in df.columns: if not all(df[c].dropna().astype(int) == df[c].dropna()): keep.append(c) if len(keep) == 0: return None return df[keep] . Some of the files will need special arguments to read_csv in order to open. Of course, since we have so many files, we can&#39;t go and manually figure out the correct arguments for every one. So to open the file we&#39;ll just call read_csv in a try block so that files requiring complex arguments won&#39;t crash our program. We are only interested in floating point columns, so we&#39;ll use select_dtypes to get them. . There&#39;s a slight complication in selecting the floating point columns: due to the way that pandas handles missing data, integer columns that contain missing data will end up as float64 by default. So there&#39;s a chunk of code to identify floating point columns that are really integers, and to drop them from the dataset. . We can test out the function by running it on our planets dataset: . get_float_columns(&#39;planets.csv&#39;) . orbital_period mass distance . 0 269.300000 | 7.10 | 77.40 | . 1 874.774000 | 2.21 | 56.95 | . 2 763.000000 | 2.60 | 19.84 | . 3 326.030000 | 19.40 | 110.62 | . 4 516.220000 | 10.50 | 119.47 | . ... ... | ... | ... | . 1030 3.941507 | NaN | 172.00 | . 1031 2.615864 | NaN | 148.00 | . 1032 3.191524 | NaN | 174.00 | . 1033 4.125083 | NaN | 293.00 | . 1034 4.187757 | NaN | 260.00 | . 1035 rows × 3 columns . As expected, we just get the floating point columns. . Comparing summary statistics . Now we can move on to the first question, calculating the summary statistics for each column in both 64 bit and 32 bit versions. Most of the work will be done by describe. We will add a _64 and a _32 suffix to the column names, then concatenate the two descriptions to give a wide summary table with one row per column in the original dataframe: . import numpy as np def summarize_columns(df): summary_64 = df.describe().T summary_64.columns = [c + &#39;_64&#39; for c in summary_64.columns] summary_64_abs = df.abs().describe().T summary_64_abs.columns = [c + &#39;_64_abs&#39; for c in summary_64_abs.columns] summary_32 = df.astype(&#39;float32&#39;).describe().T summary_32.columns = [c + &#39;_32&#39; for c in summary_32.columns] result = pd.concat([summary_32, summary_64, summary_64_abs], axis=1) return result planets = get_float_columns(&#39;planets.csv&#39;) summarize_columns(planets) . count_32 mean_32 ... 75%_64_abs max_64_abs . orbital_period 992.0 | 2002.917480 | ... | 526.005 | 730000.0 | . mass 513.0 | 2.638161 | ... | 3.040 | 25.0 | . distance 808.0 | 264.069275 | ... | 178.500 | 8500.0 | . 3 rows × 24 columns . There&#39;s one slight complication compared to our original planet code. Think about the case where we have a column of mixed negative and positive values. We might end up with a situation where the mean is very close to zero, even though the values themselves are not. This will give us an inflated estimate of the precision error when we divide by the mean. By working with a mean of the absolute values we can avoid this. The same goes for the other summary statistics - median, min, max, etc. . This dataframe is a bit awkward to read on a web page, since it&#39;s so wide. But the column structure makes it very easy to check, for example, the difference in the means, either in absolute terms: . summary = summarize_columns(planets) (summary[&#39;mean_64&#39;] - summary[&#39;mean_32&#39;]).abs() . orbital_period 1.156260e-04 mass 1.207711e-07 distance 7.275874e-06 dtype: float64 . Or as a proportion of the original: . summary = summarize_columns(planets) (summary[&#39;mean_64&#39;] - summary[&#39;mean_32&#39;]).abs() / summary[&#39;mean_64_abs&#39;] . orbital_period 5.772879e-08 mass 4.577852e-08 distance 2.755290e-08 dtype: float64 . Comparing linear regressions . Next we need a function that will take our dataframe of floating point columns and calculate pairwise linear regressions for both 32 and 64 bit versions. Here&#39;s what it looks like: . import scipy.stats import itertools def summarize_regressions(df): df = df.dropna() # if fewer than two columns or fewer than ten rows, skip if len(df.columns) &lt; 2 or len(df) &lt; 10: return None df_32 = df.astype(&#39;float32&#39;) results = [] for x, y in itertools.combinations(df.columns, r=2): regression_64 = scipy.stats.linregress(df[x], df[y]) regression_32 = scipy.stats.linregress(df_32[x], df_32[y]) results.append( ( x, y, regression_64[0], # slope regression_32[0], regression_64[1], # intercept regression_32[1], regression_64[2], # R value regression_32[2], regression_64[3], # P value regression_32[3], ) ) return pd.DataFrame( results, columns = [&#39;x&#39;, &#39;y&#39;, &#39;slope_64&#39;, &#39;slope_32&#39;, &#39;intercept_64&#39;, &#39;intercept_32&#39;, &#39;rvalue_64&#39;, &#39;rvalue_32&#39;, &#39;pvalue_64&#39;, &#39;pvalue_32&#39;] ) summarize_regressions(planets) . x y ... pvalue_64 pvalue_32 . 0 orbital_period | mass | ... | 3.298188e-05 | 3.298188e-05 | . 1 orbital_period | distance | ... | 4.348817e-01 | 4.348817e-01 | . 2 mass | distance | ... | 4.954410e-10 | 4.954411e-10 | . 3 rows × 10 columns . The output is another summary table, this time showing us the slope, intercept, rvalue and pvalue for each pair of columns in both 64 bit and 32 bit form. As in the summary statistics, this makes it easy to find the differences in the various regression properties. Here&#39;s the difference in slope as a proportion of the original slope: . summary = summarize_regressions(planets) (summary[&#39;slope_64&#39;] - summary[&#39;slope_32&#39;]).abs() / summary[&#39;slope_64&#39;] . 0 3.637540e-09 1 -3.431153e-08 2 8.654275e-09 dtype: float64 . Part three: looking at the results . Having written the functions above, actually analysing all of the downloaded data files is pretty straightforward. We will use glob to iterate over the folders, and just call the functions in order. . import glob from tqdm.notebook import tqdm dfs = [] for filepath in tqdm(glob.glob(&#39;/home/martin/Downloads/kaggle_test/*-*/*.csv&#39;)): df = get_float_columns(filepath) if df is not None: summary = summarize_columns(df) summary[&#39;filepath&#39;] = filepath dfs.append(summary) summary_stats = pd.concat(dfs) . At the end of the process we have a big dataframe that contains the summary for all the data files: . summary_stats.head() . count_32 mean_32 ... max_64_abs filepath . BMI 768.0 | 31.992579 | ... | 67.10 | /home/martin/Downloads/kaggle_test/uciml-pima-... | . DiabetesPedigreeFunction 768.0 | 0.471876 | ... | 2.42 | /home/martin/Downloads/kaggle_test/uciml-pima-... | . MILES* 1156.0 | 21.115400 | ... | 12204.70 | /home/martin/Downloads/kaggle_test/zusmani-ube... | . avg_rating 13608.0 | 3.923293 | ... | 5.00 | /home/martin/Downloads/kaggle_test/jilkothari-... | . avg_rating_recent 13608.0 | 3.912241 | ... | 5.00 | /home/martin/Downloads/kaggle_test/jilkothari-... | . 5 rows × 25 columns . If you want to play with the data, you can download a compressed copy here. . Let&#39;s first take a look at the volume of data. We have processed 919 data files: . summary_stats[&#39;filepath&#39;].nunique() . 919 . containing just over 8000 floating point columns: . len(summary_stats.reset_index().groupby([&#39;index&#39;, &#39;filepath&#39;])) . 8040 . and just over half a billion individual floating point values: . summary_stats[&#39;count_64&#39;].sum() . 520432586.0 . One slight complication is that some of our data files contain very large values that will be outside the range of 32 bit floating point numbers. It&#39;s easy to spot those in the file as the maximum of the 32 bit version will end up infinite: . summary_stats[np.isinf(summary_stats[&#39;max_32&#39;])] . count_32 mean_32 ... max_64_abs filepath . PublicScoreLeaderboardDisplay 8357340.0 | NaN | ... | 6.567444e+302 | /home/martin/Downloads/kaggle_test/kaggle-meta... | . PrivateScoreLeaderboardDisplay 8357340.0 | NaN | ... | 6.870993e+302 | /home/martin/Downloads/kaggle_test/kaggle-meta... | . 2 rows × 25 columns . It&#39;s also pretty easy to remove them from the results: . results = summary_stats[ ~ np.isinf(summary_stats[&#39;max_32&#39;])] . Looking at changes in summary statistics . Now we can start to address the questions of how much effect changing to 32 bit precision had on the data. Since the results dataframe is in the same format as our earlier planets example, the code will be very similar. We can calculate the absolute error: . (results[&#39;mean_64&#39;] - results[&#39;mean_32&#39;]).abs() . BMI 3.814697e-07 DiabetesPedigreeFunction 8.662542e-09 MILES* 2.390456e-06 avg_rating 3.664424e-08 avg_rating_recent 1.100372e-07 ... 5a_credit_market_reg 5.570170e-07 5b_labor_market_reg 2.104992e-07 5c_business_reg 3.096381e-07 5_regulation 3.479504e-07 oldpeak 1.180290e-08 Length: 8038, dtype: float64 . but since we have so many values, we need to summarize them. We can do this with summary statistics: . (results[&#39;mean_64&#39;] - results[&#39;mean_32&#39;]).abs().describe() . count 8.038000e+03 mean 3.698345e+06 std 3.315559e+08 min 0.000000e+00 25% 2.756953e-08 50% 4.126828e-07 75% 3.578393e-06 max 2.972561e+10 dtype: float64 . Notice that we have a pretty wild distribution here. The mean absolute error is about three million, but the median (50% in the above table) is 0.0000004! This is why we need to normalize these by viewing them as a proportion of the original mean of absolute values: . prop_errors = (results[&#39;mean_64&#39;] - results[&#39;mean_32&#39;]).abs() / results[&#39;mean_64_abs&#39;] prop_errors.describe() . count 8.038000e+03 mean 4.340236e-08 std 5.796892e-08 min 0.000000e+00 25% 1.366587e-08 50% 3.117527e-08 75% 5.799712e-08 max 1.963675e-06 dtype: float64 . This is easier to look at. The mean and median proportional error are both on the order of 10-8, and the worst affected data column has its mean altered by around 10-6. In practical terms, these are incredibly minor effects. We can easily view them as a distribution to see the long tail: . import seaborn as sns sns.displot( prop_errors, aspect=2 ) . &lt;seaborn.axisgrid.FacetGrid at 0x7f609ba57fa0&gt; . We can do the same thing for the other aggregations of the original data - let&#39;s add the median (50% in our table) and the max. Calculating the proportional error in the minimum is tricky, since for many data columns the minimum value will be zero, leading to an infinite proportional error, so we will skip it. . Adding these measurements as additional columns: . results[&#39;mean prop error&#39;] = (results[&#39;mean_64&#39;] - results[&#39;mean_32&#39;]).abs() / results[&#39;mean_64_abs&#39;] results[&#39;median prop error&#39;] = (results[&#39;50%_64&#39;] - results[&#39;50%_32&#39;]).abs() / results[&#39;50%_64_abs&#39;] results[&#39;max prop error&#39;] = (results[&#39;max_64&#39;] - results[&#39;max_32&#39;]).abs() / results[&#39;max_64_abs&#39;] . will allow us to make a summary table: . results[[&#39;mean prop error&#39;, &#39;median prop error&#39;, &#39;max prop error&#39;]].describe() . mean prop error median prop error max prop error . count 8.038000e+03 | 7.835000e+03 | 8.038000e+03 | . mean 4.340236e-08 | 1.546485e-08 | 1.370840e-08 | . std 5.796892e-08 | 1.516289e-08 | 1.422228e-08 | . min 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 25% 1.366587e-08 | 1.382856e-09 | 0.000000e+00 | . 50% 3.117527e-08 | 1.254835e-08 | 1.014960e-08 | . 75% 5.799712e-08 | 2.447450e-08 | 2.442311e-08 | . max 1.963675e-06 | 1.067892e-07 | 5.848003e-08 | . A little bit of rearrangement using melt will allow us to look at the distribution of proportional errors for all three summary statistics: . g = sns.displot( data = results[[&#39;mean prop error&#39;, &#39;median prop error&#39;, &#39;max prop error&#39;]].melt(), x = &#39;value&#39;, col = &#39;variable&#39;, ) import matplotlib.pyplot as plt plt.xlim((0, 2e-7)) . (0.0, 2e-07) . Looking at linear regressions . Now we can build a similar dataframe for the linear regression results. The logic is very similar to the summary statistics; we are just using the summarize_regressions function instead: . import glob from tqdm.notebook import tqdm dfs = [] for filepath in tqdm(glob.glob(&#39;/home/martin/Downloads/kaggle_test/*-*/*.csv&#39;)): df = get_float_columns(filepath) if df is not None: summary = summarize_regressions(df) if summary is not None: summary[&#39;filepath&#39;] = filepath dfs.append(summary) regressions = pd.concat(dfs) . If you want to play with the data, you can download a compressed version here. . The resulting dataframe has the same wide shape as before, but this time each row represents a regression result between two columns from the same data file. We have just over half a million regression results in total: . regressions . x y ... pvalue_32 filepath . 0 BMI | DiabetesPedigreeFunction | ... | 9.197967e-05 | /home/martin/Downloads/kaggle_test/uciml-pima-... | . 0 avg_rating | avg_rating_recent | ... | 0.000000e+00 | /home/martin/Downloads/kaggle_test/jilkothari-... | . 1 avg_rating | rating | ... | 0.000000e+00 | /home/martin/Downloads/kaggle_test/jilkothari-... | . 2 avg_rating_recent | rating | ... | 0.000000e+00 | /home/martin/Downloads/kaggle_test/jilkothari-... | . 0 ts | co | ... | 1.964217e-60 | /home/martin/Downloads/kaggle_test/garystaffor... | . ... ... | ... | ... | ... | ... | . 401 5a_credit_market_reg | 5c_business_reg | ... | 3.433454e-65 | /home/martin/Downloads/kaggle_test/gsutters-ec... | . 402 5a_credit_market_reg | 5_regulation | ... | 1.517798e-237 | /home/martin/Downloads/kaggle_test/gsutters-ec... | . 403 5b_labor_market_reg | 5c_business_reg | ... | 1.834375e-50 | /home/martin/Downloads/kaggle_test/gsutters-ec... | . 404 5b_labor_market_reg | 5_regulation | ... | 5.207626e-224 | /home/martin/Downloads/kaggle_test/gsutters-ec... | . 405 5c_business_reg | 5_regulation | ... | 6.484023e-284 | /home/martin/Downloads/kaggle_test/gsutters-ec... | . 550136 rows × 11 columns . Because the number of pairwise comparisons increases with the square of the number of columns, the results are dominated by a small number of data files with many floating point columns: . regressions[&#39;filepath&#39;].value_counts() . /home/martin/Downloads/kaggle_test/rajanand-key-indicators-of-annual-health-survey/Key_indicator_districtwise.csv 182710 /home/martin/Downloads/kaggle_test/uciml-human-activity-recognition-with-smartphones/test.csv 157080 /home/martin/Downloads/kaggle_test/uciml-human-activity-recognition-with-smartphones/train.csv 157080 /home/martin/Downloads/kaggle_test/johnjdavisiv-us-counties-covid19-weather-sociohealth-data/US_counties_COVID19_health_weather_data.csv 11175 /home/martin/Downloads/kaggle_test/johnjdavisiv-us-counties-covid19-weather-sociohealth-data/us_county_sociohealth_data.csv 7260 ... /home/martin/Downloads/kaggle_test/marcodena-mobile-phone-activity/mi-to-provinces-2013-11-07.csv 1 /home/martin/Downloads/kaggle_test/russellyates88-suicide-rates-overview-1985-to-2016/master.csv 1 /home/martin/Downloads/kaggle_test/crowdflower-twitter-user-gender-classification/gender-classifier-DFE-791531.csv 1 /home/martin/Downloads/kaggle_test/PromptCloudHQ-imdb-data/IMDB-Movie-Data.csv 1 /home/martin/Downloads/kaggle_test/claytonmiller-building-data-genome-project-v1/meta_open.csv 1 Name: filepath, Length: 289, dtype: int64 . Out of our 919 data files, only 289 had at least two floating point columns. The data file with the most floating point columns contributes nearly two hundred thousand regression results! . To get an overview of the difference for each property of the regression result, we can add new columns as we did before and summarize them: . regressions[&#39;slope error&#39;] = ((regressions[&#39;slope_64&#39;] - regressions[&#39;slope_32&#39;]) / regressions[&#39;slope_64&#39;]).abs() regressions[&#39;intercept error&#39;] = ((regressions[&#39;intercept_64&#39;] - regressions[&#39;intercept_32&#39;]) / regressions[&#39;intercept_64&#39;]).abs() regressions[&#39;rvalue error&#39;] = ((regressions[&#39;rvalue_64&#39;] - regressions[&#39;rvalue_32&#39;]) / regressions[&#39;rvalue_64&#39;]).abs() regressions[&#39;pvalue error&#39;] = ((regressions[&#39;pvalue_64&#39;] - regressions[&#39;pvalue_32&#39;]) / regressions[&#39;pvalue_64&#39;]).abs() regressions[[&#39;slope error&#39;, &#39;intercept error&#39;, &#39;rvalue error&#39;, &#39;pvalue error&#39;]].dropna().describe() . slope error intercept error rvalue error pvalue error . count 4.181990e+05 | 4.181990e+05 | 4.181990e+05 | 4.181990e+05 | . mean 4.237303e-04 | 7.675898e-05 | 4.237294e-04 | 1.471530e-06 | . std 2.486475e-02 | 1.618511e-02 | 2.486475e-02 | 8.407760e-06 | . min 0.000000e+00 | 2.753007e-13 | 0.000000e+00 | 0.000000e+00 | . 25% 4.304286e-09 | 1.899837e-08 | 3.850181e-09 | 1.095305e-07 | . 50% 1.669817e-08 | 3.986912e-08 | 1.547252e-08 | 3.511528e-07 | . 75% 6.553311e-08 | 7.240586e-08 | 6.388698e-08 | 1.105768e-06 | . max 4.985437e+00 | 4.448138e+00 | 4.985437e+00 | 2.476720e-03 | . Notice that in the count row we can see how many of the comparisons gave meaningful (i.e. non-missing) output for the linear regressions on both the 64 and 32 bit versions. . Remember that these are proportional errors i.e. the proportion by which the values change. Although the mean errors are a couple of orders of magnitude greater than for the summary statistics, they are still very small, and very unlikely to change any conclusions about the data. . We have a very long tailed distribution for all four properties: the mean is several orders of magnitude higher than the median (50%) and the maximum values are several orders of magnitude higher again, suggesting a small number of extreme outliers. One way to show this is with a boxen plot: . sns.catplot( data = regressions[[&#39;slope error&#39;, &#39;intercept error&#39;, &#39;rvalue error&#39;, &#39;pvalue error&#39;]].dropna().melt(), x = &#39;variable&#39;, y = &#39;value&#39;, kind=&#39;boxen&#39;, aspect = 2 ) . &lt;seaborn.axisgrid.FacetGrid at 0x7f60e525e1f0&gt; . The presence of the outliers make it impossible to see the distribution of the remaining values. If we plot the slope against the slope error, we can see that these outliers all represent regressions where the original slope was very near zero, suggesting no interesting relationship between the variables: . sns.relplot( data = regressions, x = &#39;slope_64&#39;, y = &#39;slope error&#39; ) plt.xlim(-1e-8,1e-8) . (-1e-08, 1e-08) . The same is true for the intercept: . sns.relplot( data = regressions, x = &#39;intercept_64&#39;, y = &#39;intercept error&#39; ) plt.xlim(-1e-5,1e-5) . (-1e-05, 1e-05) . An alternative way of looking at the pvalue error is to ask whether the change in pvalue would result in a difference in whether the relationship was considered significant. Setting aside the problems with interpreting pvalues in this way, we can use pandas to find any regressions where one version of the regression was significant at the 5% level and the other wasn&#39;t: . ((regressions.dropna()[&#39;pvalue_64&#39;] &lt; 0.05) != (regressions.dropna()[&#39;pvalue_32&#39;] &lt; 0.05)).value_counts() . False 418199 dtype: int64 . In the ~400,000 out of our ~500,000 comparisions that had no missing data, there were no such cases. . Conclusion . It looks like in real world datasets, switching from 64 bit to 32 bit precision for floating point numbers has a vanishingly small chance of affecting our conclusions, and we should probably do it without fear when trying to minimize memory usage. . If you&#39;ve made it this far, you should definitely subscribe to the Drawing from Data newsletter, follow me on Twitter, or buy the Drawing from Data book! .",
            "url": "https://drawingfromdata.com/pandas/numpy/kaggle/numerical-precision.html",
            "relUrl": "/pandas/numpy/kaggle/numerical-precision.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Making a pairwise distance matrix in pandas",
            "content": "This is a somewhat specialized problem that forms part of a lot of data science and clustering workflows. It starts with a relatively straightforward question: if we have a bunch of measurements for two different things, how do we come up with a single number that represents the difference between the two things? . . Tip: For a deeper dive into distances and clustering, checkout the chapter on matrix charts in the Drawing from Data book. . An example will make the question clearer. Let&#39;s load our olympic medal dataset: . import pandas as pd data = pd.read_csv(&quot;https://raw.githubusercontent.com/mojones/binders/master/olympics.csv&quot;, sep=&quot; t&quot;) data . City Year Sport ... Medal Country Int Olympic Committee code . 0 Athens | 1896 | Aquatics | ... | Gold | Hungary | HUN | . 1 Athens | 1896 | Aquatics | ... | Silver | Austria | AUT | . 2 Athens | 1896 | Aquatics | ... | Bronze | Greece | GRE | . 3 Athens | 1896 | Aquatics | ... | Gold | Greece | GRE | . 4 Athens | 1896 | Aquatics | ... | Silver | Greece | GRE | . ... ... | ... | ... | ... | ... | ... | ... | . 29211 Beijing | 2008 | Wrestling | ... | Silver | Germany | GER | . 29212 Beijing | 2008 | Wrestling | ... | Bronze | Lithuania | LTU | . 29213 Beijing | 2008 | Wrestling | ... | Bronze | Armenia | ARM | . 29214 Beijing | 2008 | Wrestling | ... | Gold | Cuba | CUB | . 29215 Beijing | 2008 | Wrestling | ... | Silver | Russia | RUS | . 29216 rows × 12 columns . and measure, for each different country, the number of medals they&#39;ve won in each different sport: . summary = data.groupby([&#39;Country&#39;, &#39;Sport&#39;]).size().unstack().fillna(0) summary . Sport Aquatics Archery Athletics ... Water Motorsports Weightlifting Wrestling . Country . Afghanistan 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | . Algeria 0.0 | 0.0 | 6.0 | ... | 0.0 | 0.0 | 0.0 | . Argentina 3.0 | 0.0 | 5.0 | ... | 0.0 | 2.0 | 0.0 | . Armenia 0.0 | 0.0 | 0.0 | ... | 0.0 | 4.0 | 4.0 | . Australasia 11.0 | 0.0 | 1.0 | ... | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | . Virgin Islands* 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | . West Germany 62.0 | 0.0 | 67.0 | ... | 0.0 | 7.0 | 9.0 | . Yugoslavia 91.0 | 0.0 | 2.0 | ... | 0.0 | 0.0 | 16.0 | . Zambia 0.0 | 0.0 | 1.0 | ... | 0.0 | 0.0 | 0.0 | . Zimbabwe 7.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | . 137 rows × 42 columns . Now we&#39;ll pick two countries: . summary.loc[[&#39;Germany&#39;, &#39;Italy&#39;]] . Sport Aquatics Archery Athletics ... Water Motorsports Weightlifting Wrestling . Country . Germany 175.0 | 6.0 | 99.0 | ... | 0.0 | 20.0 | 24.0 | . Italy 113.0 | 12.0 | 71.0 | ... | 0.0 | 14.0 | 20.0 | . 2 rows × 42 columns . Each country has 44 columns giving the total number of medals won in each sport. Our job is to come up with a single number that summarizes how different those two lists of numbers are. Mathematicians have figured out lots of different ways of doing that, many of which are implemented in the scipy.spatial.distance module. . If we just import pdist from the module, and pass in our dataframe of two countries, we&#39;ll get a measuremnt: . from scipy.spatial.distance import pdist pdist(summary.loc[[&#39;Germany&#39;, &#39;Italy&#39;]]) . array([342.3024978]) . That&#39;s the distance score using the default metric, which is called the euclidian distance. Think of it as the straight line distance between the two points in space defined by the two lists of 44 numbers. . Now, what happens if we pass in a dataframe with three countries? . pdist(summary.loc[[&#39;Germany&#39;, &#39;Italy&#39;, &#39;France&#39;]]) . array([342.3024978 , 317.98584874, 144.82403116]) . As we might expect, we have three measurements: . Germany and Italy | Germnay and France | Italy and France | . But it&#39;s not easy to figure out which belongs to which. Happily, scipy also has a helper function that will take this list of numbers and turn it back into a square matrix: . from scipy.spatial.distance import squareform squareform(pdist(summary.loc[[&#39;Germany&#39;, &#39;Italy&#39;, &#39;France&#39;]])) . array([[ 0. , 342.3024978 , 317.98584874], [342.3024978 , 0. , 144.82403116], [317.98584874, 144.82403116, 0. ]]) . In order to make sense of this, we need to re-attach the country names, which we can just do by turning it into a DataFrame: . pd.DataFrame( squareform(pdist(summary.loc[[&#39;Germany&#39;, &#39;Italy&#39;, &#39;France&#39;]])), columns = [&#39;Germany&#39;, &#39;Italy&#39;, &#39;France&#39;], index = [&#39;Germany&#39;, &#39;Italy&#39;, &#39;France&#39;] ) . Germany Italy France . Germany 0.000000 | 342.302498 | 317.985849 | . Italy 342.302498 | 0.000000 | 144.824031 | . France 317.985849 | 144.824031 | 0.000000 | . Hopefully this agrees with our intuition; the numbers on the diagonal are all zero, because each country is identical to itself, and the numbers above and below are mirror images, because the distance between Germany and France is the same as the distance between France and Germany (remember that we are talking about distance in terms of their medal totals, not geographical distance!) . Finally, to get pairwise measurements for the whole input dataframe, we just pass in the complete object and get the country names from the index: . pairwise = pd.DataFrame( squareform(pdist(summary)), columns = summary.index, index = summary.index ) pairwise . Country Afghanistan Algeria Argentina ... Yugoslavia Zambia Zimbabwe . Country . Afghanistan 0.000000 | 8.774964 | 96.643675 | ... | 171.947666 | 1.732051 | 17.492856 | . Algeria 8.774964 | 0.000000 | 95.199790 | ... | 171.688672 | 7.348469 | 19.519221 | . Argentina 96.643675 | 95.199790 | 0.000000 | ... | 148.128323 | 96.348326 | 89.810912 | . Armenia 5.830952 | 9.848858 | 96.477977 | ... | 171.604196 | 5.744563 | 18.384776 | . Australasia 18.708287 | 20.024984 | 97.744565 | ... | 166.991018 | 18.627936 | 22.360680 | . ... ... | ... | ... | ... | ... | ... | ... | . Virgin Islands* 1.414214 | 8.774964 | 96.457244 | ... | 171.947666 | 1.732051 | 17.492856 | . West Germany 153.052279 | 150.306354 | 142.537714 | ... | 184.945938 | 152.577849 | 144.045132 | . Yugoslavia 171.947666 | 171.688672 | 148.128323 | ... | 0.000000 | 171.874955 | 169.103519 | . Zambia 1.732051 | 7.348469 | 96.348326 | ... | 171.874955 | 0.000000 | 17.521415 | . Zimbabwe 17.492856 | 19.519221 | 89.810912 | ... | 169.103519 | 17.521415 | 0.000000 | . 137 rows × 137 columns . A nice way to visualize these is with a heatmap. 137 countries is a bit too much to show on a webpage, so let&#39;s restrict it to just the countries that have scored at least 500 medals total: . import seaborn as sns import matplotlib.pyplot as plt # make summary table for just top countries top_countries = ( data .groupby(&#39;Country&#39;) .filter(lambda x : len(x) &gt; 500) .groupby([&#39;Country&#39;, &#39;Sport&#39;]) .size() .unstack() .fillna(0) ) # make pairwise distance matrix pairwise_top = pd.DataFrame( squareform(pdist(top_countries)), columns = top_countries.index, index = top_countries.index ) # plot it with seaborn plt.figure(figsize=(10,10)) sns.heatmap( pairwise_top, cmap=&#39;OrRd&#39;, linewidth=1 ) . &lt;AxesSubplot:xlabel=&#39;Country&#39;, ylabel=&#39;Country&#39;&gt; . Now that we have a plot to look at, we can see a problem with the distance metric we&#39;re using. The US has won so many more medals than other countries that it distorts the measurement. And if we think about it, what we&#39;re really interested in is not the exact number of medals in each category, but the relative number. In other words, we want two contries to be considered similar if they both have about twice as many medals in boxing as athletics, for example, regardless of the exact numbers. . Luckily for us, there is a distance measure already implemented in scipy that has that property - it&#39;s called cosine distance. Think of it as a measurement that only looks at the relationships between the 44 numbers for each country, not their magnitude. We can switch to cosine distance by specifying the metric keyword argument in pdist: . pairwise_top = pd.DataFrame( squareform(pdist(top_countries, metric=&#39;cosine&#39;)), columns = top_countries.index, index = top_countries.index ) # plot it with seaborn plt.figure(figsize=(10,10)) sns.heatmap( pairwise_top, cmap=&#39;OrRd&#39;, linewidth=1 ) . &lt;AxesSubplot:xlabel=&#39;Country&#39;, ylabel=&#39;Country&#39;&gt; . And as you can see we spot some much more interstesting patterns. Notice, for example, that Russia and Soviet Union have a very low distance (i.e. their medal distributions are very similar). . When looking at data like this, remember that the shade of each cell is not telling us anything about how many medals a country has won - simply how different or similar each country is to each other. Compare the above heatmap with this one which displays the proportion of medals in each sport per country: . plt.figure(figsize=(10,10)) sns.heatmap( top_countries.apply(lambda x : x / x.sum(), axis=1), cmap=&#39;BuPu&#39;, square=True, cbar_kws = {&#39;fraction&#39; : 0.02} ) . &lt;AxesSubplot:xlabel=&#39;Sport&#39;, ylabel=&#39;Country&#39;&gt; . Finally, how might we find pairs of countries that have very similar medal distributions (i.e. very low numbers in the pairwise table)? By far the easiest way is to start of by reshaping the table into long form, so that each comparison is on a separate row: . pairwise = pd.DataFrame( squareform(pdist(summary, metric=&#39;cosine&#39;)), columns = summary.index, index = summary.index ) # move to long form long_form = pairwise.unstack() # rename columns and turn into a dataframe long_form.index.rename([&#39;Country A&#39;, &#39;Country B&#39;], inplace=True) long_form = long_form.to_frame(&#39;cosine distance&#39;).reset_index() . Now we can write our filter as normal, remembering to filter out the unintersting rows that tell us a country&#39;s distance from itself! . long_form[ (long_form[&#39;cosine distance&#39;] &lt; 0.05) &amp; (long_form[&#39;Country A&#39;] != long_form[&#39;Country B&#39;]) ] . Country A Country B cosine distance . 272 Algeria | Zambia | 0.026671 | . 1034 Azerbaijan | Mongolia | 0.045618 | . 1105 Bahamas | Barbados | 0.021450 | . 1111 Bahamas | British West Indies | 0.021450 | . 1113 Bahamas | Burundi | 0.021450 | . ... ... | ... | ... | . 17033 United Arab Emirates | Haiti | 0.010051 | . 17037 United Arab Emirates | Independent Olympic Participants | 0.000000 | . 17051 United Arab Emirates | Kuwait | 0.000000 | . 18164 Virgin Islands* | Netherlands Antilles* | 0.000000 | . 18496 Zambia | Algeria | 0.026671 | . 462 rows × 3 columns .",
            "url": "https://drawingfromdata.com/pandas/clustering/making-a-pairwise-distance-matrix-in-pandas.html",
            "relUrl": "/pandas/clustering/making-a-pairwise-distance-matrix-in-pandas.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Finding invalid values in numerical columns",
            "content": "Introduction . In an ideal world, every time we load a data file into pandas we will end up with a nice neat dataframe with well defined columns. In the real world, however, we often end up with columns that contain values in a mixture of different formats. This makes it hard to carry out our analyses. In this article we&#39;ll look at a simple way to quickly identify invalid values so that we can fix them. . As always, to keep this article to a reasonable size we won&#39;t be going in to too much detail on the methods involved. All of the methods we&#39;ll use here are covered in detail in the Drawing from Data book. In particular, check out chapter 2 for a discussion of data types and chapter 16 for a discussion of the memory implications. . If you&#39;re interested in more articles and tips on data exploration with Python, you should subscribe to the Drawing from Data newsletter or follow me on Twitter. . The MMA dataset . For our example, we&#39;ll look at a real life data file taken from the Metropolitan Museum of Art in New York. The museum has published a dataset describing around half a million different artworks. The data are available as a CSV file in this GitHub repo. . This is a good dataset to explore when looking for messy formatting, for a number of reasons: . it&#39;s large, so even rare errors are likely to be well represented. | it&#39;s curated by humans, rather than being automatically generated, so it&#39;s likely to contain typos and other human errors | furthermore, it&#39;s large enough to have multiple human contributors, so the data will likely reflect inconsistencies between different ideas about data entry | it&#39;s very heterogeneous - the individual artworks are of very different types. We have data for paintings, sculptures, photography, ceramics, fabrics, etc. Any time we have multiple different types of objects being represented by a single data structure, we have the oppourtunity for messy data. | . Because the data are hosted on GitHub, we should be able to read them directly and take a look at the dataframe: . import pandas as pd pd.options.display.max_columns=4 met = pd.read_csv(&#39;https://github.com/metmuseum/openaccess/raw/master/MetObjects.csv&#39;) . /home/martin/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (5,7,10,11,12,13,14,15,22,34,35,36,37,38,39,40,41,42,43,44,46) have mixed types.Specify dtype option on import or set low_memory=False. has_raised = await self.run_ast_nodes(code_ast.body, cell_name, . Notice that we immediately get a hint that something is weird in the data file when pandas complains about columns with mixed types. Let&#39;s ignore this for now and bravely press on with a look at the dataframe: . met . Object Number Is Highlight ... Tags AAT URL Tags Wikidata URL . 0 1979.486.1 | False | ... | NaN | NaN | . 1 1980.264.5 | False | ... | NaN | NaN | . 2 67.265.9 | False | ... | NaN | NaN | . 3 67.265.10 | False | ... | NaN | NaN | . 4 67.265.11 | False | ... | NaN | NaN | . ... ... | ... | ... | ... | ... | . 474521 07.225.14b | False | ... | http://vocab.getty.edu/page/aat/300054534|http... | https://www.wikidata.org/wiki/Q333|https://www... | . 474522 69.292.12 | False | ... | http://vocab.getty.edu/page/aat/300250049|http... | https://www.wikidata.org/wiki/Q2092297|https:/... | . 474523 See attached list | False | ... | NaN | NaN | . 474524 See attached list - chairs | False | ... | NaN | NaN | . 474525 61.100.3c | False | ... | http://vocab.getty.edu/page/aat/300266506 | https://www.wikidata.org/wiki/Q5113 | . 474526 rows × 54 columns . All looks OK here; we have a dataframe with just under half a million rows. There are too many columns to fit comfortably on the screen, so let&#39;s just check their names: . met.columns . Index([&#39;Object Number&#39;, &#39;Is Highlight&#39;, &#39;Is Timeline Work&#39;, &#39;Is Public Domain&#39;, &#39;Object ID&#39;, &#39;Gallery Number&#39;, &#39;Department&#39;, &#39;AccessionYear&#39;, &#39;Object Name&#39;, &#39;Title&#39;, &#39;Culture&#39;, &#39;Period&#39;, &#39;Dynasty&#39;, &#39;Reign&#39;, &#39;Portfolio&#39;, &#39;Constiuent ID&#39;, &#39;Artist Role&#39;, &#39;Artist Prefix&#39;, &#39;Artist Display Name&#39;, &#39;Artist Display Bio&#39;, &#39;Artist Suffix&#39;, &#39;Artist Alpha Sort&#39;, &#39;Artist Nationality&#39;, &#39;Artist Begin Date&#39;, &#39;Artist End Date&#39;, &#39;Artist Gender&#39;, &#39;Artist ULAN URL&#39;, &#39;Artist Wikidata URL&#39;, &#39;Object Date&#39;, &#39;Object Begin Date&#39;, &#39;Object End Date&#39;, &#39;Medium&#39;, &#39;Dimensions&#39;, &#39;Credit Line&#39;, &#39;Geography Type&#39;, &#39;City&#39;, &#39;State&#39;, &#39;County&#39;, &#39;Country&#39;, &#39;Region&#39;, &#39;Subregion&#39;, &#39;Locale&#39;, &#39;Locus&#39;, &#39;Excavation&#39;, &#39;River&#39;, &#39;Classification&#39;, &#39;Rights and Reproduction&#39;, &#39;Link Resource&#39;, &#39;Object Wikidata URL&#39;, &#39;Metadata Date&#39;, &#39;Repository&#39;, &#39;Tags&#39;, &#39;Tags AAT URL&#39;, &#39;Tags Wikidata URL&#39;], dtype=&#39;object&#39;) . Most of these seem pretty self evident. We can see here a common pattern when representing multiple different types of objects in a single data structure: a large number of columns, many of which are mostly missing data. For example, all but 2000 values are missing from the River column: . met[&#39;River&#39;].describe() . count 2097 unique 229 top Upper Sepik River freq 362 Name: River, dtype: object . Presumably there are a small number of objects that are associated with a particular river, and because we want to store that information we need a river column even though most of the objects don&#39;t have it. . One of the most well-populated columns is accession year, with more than 99% of the values present: . met[&#39;AccessionYear&#39;].count() / met[&#39;AccessionYear&#39;].size . 0.9916421860972845 . This makes sense. We would expect an art museum to have pretty thorough records of when each object was acquired. . Let&#39;s remove any rows without an accession year: . met = met.dropna(subset=[&#39;AccessionYear&#39;]) . then take a look at the series: . met[&#39;AccessionYear&#39;] . 0 1979.0 1 1980.0 2 1967.0 3 1967.0 4 1967.0 ... 474519 1964.0 474520 1978.0 474521 1907.0 474522 1969.0 474525 1961.0 Name: AccessionYear, Length: 470560, dtype: object . This looks pretty straightforward, let&#39;s try to find all the objects that were acquired before 1950: . met[met[&#39;AccessionYear&#39;] &lt; 1950] . TypeError Traceback (most recent call last) &lt;ipython-input-8-ac1cd7060217&gt; in &lt;module&gt; -&gt; 1 met[met[&#39;AccessionYear&#39;] &lt; 1950] ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/ops/common.py in new_method(self, other) 63 other = item_from_zerodim(other) 64 &gt; 65 return method(self, other) 66 67 return new_method ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/arraylike.py in __lt__(self, other) 35 @unpack_zerodim_and_defer(&#34;__lt__&#34;) 36 def __lt__(self, other): &gt; 37 return self._cmp_method(other, operator.lt) 38 39 @unpack_zerodim_and_defer(&#34;__le__&#34;) ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/series.py in _cmp_method(self, other, op) 4937 rvalues = extract_array(other, extract_numpy=True) 4938 -&gt; 4939 res_values = ops.comparison_op(lvalues, rvalues, op) 4940 4941 return self._construct_result(res_values, name=res_name) ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/ops/array_ops.py in comparison_op(left, right, op) 241 242 elif is_object_dtype(lvalues.dtype): --&gt; 243 res_values = comp_method_OBJECT_ARRAY(op, lvalues, rvalues) 244 245 else: ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/ops/array_ops.py in comp_method_OBJECT_ARRAY(op, x, y) 53 result = libops.vec_compare(x.ravel(), y.ravel(), op) 54 else: &gt; 55 result = libops.scalar_compare(x.ravel(), y, op) 56 return result.reshape(x.shape) 57 pandas/_libs/ops.pyx in pandas._libs.ops.scalar_compare() TypeError: &#39;&lt;&#39; not supported between instances of &#39;str&#39; and &#39;int&#39; . Now we have run into a problem. We were assuming that the accession year column was numeric based on the values that we saw, but it is actually of type object, as we can easily check: . met[&#39;AccessionYear&#39;].dtype . dtype(&#39;O&#39;) . We might think to fix this by explicity changing the data type: . met[&#39;AccessionYear&#39;].astype(&#39;int&#39;) . ValueError Traceback (most recent call last) &lt;ipython-input-10-d07a2f803482&gt; in &lt;module&gt; -&gt; 1 met[&#39;AccessionYear&#39;].astype(&#39;int&#39;) ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/generic.py in astype(self, dtype, copy, errors) 5870 else: 5871 # else, only a single dtype is given -&gt; 5872 new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors) 5873 return self._constructor(new_data).__finalize__(self, method=&#34;astype&#34;) 5874 ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/managers.py in astype(self, dtype, copy, errors) 629 self, dtype, copy: bool = False, errors: str = &#34;raise&#34; 630 ) -&gt; &#34;BlockManager&#34;: --&gt; 631 return self.apply(&#34;astype&#34;, dtype=dtype, copy=copy, errors=errors) 632 633 def convert( ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, ignore_failures, **kwargs) 425 applied = b.apply(f, **kwargs) 426 else: --&gt; 427 applied = getattr(b, f)(**kwargs) 428 except (TypeError, NotImplementedError): 429 if not ignore_failures: ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/blocks.py in astype(self, dtype, copy, errors) 671 vals1d = values.ravel() 672 try: --&gt; 673 values = astype_nansafe(vals1d, dtype, copy=True) 674 except (ValueError, TypeError): 675 # e.g. astype_nansafe can fail on object-dtype of strings ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna) 1072 # work around NumPy brokenness, #1987 1073 if np.issubdtype(dtype.type, np.integer): -&gt; 1074 return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape) 1075 1076 # if we have a datetime/timedelta array of objects pandas/_libs/lib.pyx in pandas._libs.lib.astype_intsafe() ValueError: invalid literal for int() with base 10: &#39;2005-02-15&#39; . But now we encounter the reason that this column ended up with an object data type rather than int or float. At least one of the values in the colum is written as a complete date: 2005-02-15 rather than just a year, so we get a ValueError when trying to convert it to an integer. . Which values can&#39;t be converted? . So here&#39;s where it get annoying. Because Python crashes at the first value that it can&#39;t turn into an integer, we have no idea how many non-integer values are in the column. If it&#39;s just one out of half a million, then we can probably just delete that row and try again. But if we have lots of non-integer values, then that would be throwing away a lot of our data. . Furthermore, if there are lots of non-integer values, are they all written in the same format? If it turns out that we have many values written in year-month-day format, just like the one we already found, then fixing them might be a simple string processing job. But if we have lots of different formats, then the task will be much trickier. . Because we have nearly half a million values to check, obviously it&#39;s impossible to look at them all manually. So we will have to write a function to help us. The trick is to use try/except to prevent pandas from simply crashing out at the first invalid value. So here&#39;s the code: . import numpy as np def check_int(value): try: int(value) return np.NaN except ValueError: return value . Our function will take a single value, and attempt to convert it to an integer inside a try block. If the int call is sucessful - i.e. the value is a valid integer - then the function will return np.NaN to represent missing data. If the call to int causes a ValueError - i.e. the value that we passed in isn&#39;t a valid integer - then it will trigger the except block and return the value itself. . To see how this works, let&#39;s try it on a few different inputs. Passing in a valid integer results in missing data: . check_int(&#39;42&#39;), check_int(1970.0) . (nan, nan) . Whereas passing in anything that can&#39;t be converted to an integer results in the original value: . check_int(&#39;banana&#39;), check_int(&#39;1970.0&#39;), check_int(&#39;2005-12-15&#39;) . (&#39;banana&#39;, &#39;1970.0&#39;, &#39;2005-12-15&#39;) . Notice a subtlety here in the handling of strings vs. floats: in Python calling int on a floating point number truncates it: . int(3.1415) . 3 . whereas calling int on a string containing a floating point number causes an error: . int(&#39;3.1415&#39;) . ValueError Traceback (most recent call last) &lt;ipython-input-15-388b05e53588&gt; in &lt;module&gt; -&gt; 1 int(&#39;3.1415&#39;) ValueError: invalid literal for int() with base 10: &#39;3.1415&#39; . So how does this function help us find our invalid values? If we use apply to call it on each value of the accession year column: . met[&#39;AccessionYear&#39;].apply(check_int) . 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN ... 474519 NaN 474520 NaN 474521 NaN 474522 NaN 474525 NaN Name: AccessionYear, Length: 470560, dtype: object . We get back a series of the same length where values that were sucessfully converted to integers are missing, and invalid values are the same. So we can easily find all of the invalid values by dropping missing data: . met[&#39;AccessionYear&#39;].apply(check_int).dropna() . 179190 2005-02-15 390850 2020-03-23 Name: AccessionYear, dtype: object . Here we have our answer: there are two values in this column that have been writted as complete dates. . Now that we have the invalid values we can figure out how to fix them. In this case, because both invalid values have the same date format we can do this by splitting on - and taking the first element of the resulting list: . fixed_values = met[&#39;AccessionYear&#39;].apply(check_int).dropna().str.split(&#39;-&#39;).str.get(0) fixed_values . 179190 2005 390850 2020 Name: AccessionYear, dtype: object . See chapter 3 of the Drawing from Data book for more about string methods and the str accessor. . Having fixed these two values, we can use the fixed values to update the original series: . met[&#39;AccessionYear&#39;].update(fixed_values) . /home/martin/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy exec(code_obj, self.user_global_ns, self.user_ns) . Which now allows us to convert the data type to integer: . met[&#39;AccessionYear&#39;].astype(int) . 0 1979 1 1980 2 1967 3 1967 4 1967 ... 474519 1964 474520 1978 474521 1907 474522 1969 474525 1961 Name: AccessionYear, Length: 470560, dtype: int64 . and carry out our filter: . met[ met[&#39;AccessionYear&#39;].astype(int) &lt; 1950 ] . Object Number Is Highlight ... Tags AAT URL Tags Wikidata URL . 14 16.74.49 | False | ... | NaN | NaN | . 15 16.74.27 | False | ... | NaN | NaN | . 16 16.74.28 | False | ... | NaN | NaN | . 17 16.74.29 | False | ... | NaN | NaN | . 18 16.74.30 | False | ... | NaN | NaN | . ... ... | ... | ... | ... | ... | . 474498 91.16.59 | False | ... | NaN | NaN | . 474499 93.13.6 | False | ... | NaN | NaN | . 474500 93.13.92 | False | ... | http://vocab.getty.edu/page/aat/300025943|http... | https://www.wikidata.org/wiki/Q467|https://www... | . 474504 13.225.13–.14, .16–.28 | False | ... | http://vocab.getty.edu/page/aat/300025928|http... | https://www.wikidata.org/wiki/Q8441|https://ww... | . 474521 07.225.14b | False | ... | http://vocab.getty.edu/page/aat/300054534|http... | https://www.wikidata.org/wiki/Q333|https://www... | . 202888 rows × 54 columns . About 200,000 objects were acquired prior to 1950. . Dealing with many invalid values . Let&#39;s try this technique on a more complicated column. Say that we want to find all objects that were acquired after the death of the artist. There&#39;s a column called Artist End Date that has values for about half of the rows: . met[&#39;Artist End Date&#39;].dropna() . 0 1869 1 1844 9 1917 10 1844 11 1869 ... 474492 2047 474503 1925 474512 1900 474519 1776 474522 1839 Name: Artist End Date, Length: 234232, dtype: object . From a glance at the values it looks like this is a series of year integers, but looking at the dtype tells us that it has ended up as type object. Sure enough, when we try to convert it to an integer type: . met[&#39;Artist End Date&#39;].dropna().astype(int) . ValueError Traceback (most recent call last) &lt;ipython-input-34-77ad2746910f&gt; in &lt;module&gt; -&gt; 1 met[&#39;Artist End Date&#39;].dropna().astype(int) ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/generic.py in astype(self, dtype, copy, errors) 5870 else: 5871 # else, only a single dtype is given -&gt; 5872 new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors) 5873 return self._constructor(new_data).__finalize__(self, method=&#34;astype&#34;) 5874 ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/managers.py in astype(self, dtype, copy, errors) 629 self, dtype, copy: bool = False, errors: str = &#34;raise&#34; 630 ) -&gt; &#34;BlockManager&#34;: --&gt; 631 return self.apply(&#34;astype&#34;, dtype=dtype, copy=copy, errors=errors) 632 633 def convert( ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, ignore_failures, **kwargs) 425 applied = b.apply(f, **kwargs) 426 else: --&gt; 427 applied = getattr(b, f)(**kwargs) 428 except (TypeError, NotImplementedError): 429 if not ignore_failures: ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/blocks.py in astype(self, dtype, copy, errors) 671 vals1d = values.ravel() 672 try: --&gt; 673 values = astype_nansafe(vals1d, dtype, copy=True) 674 except (ValueError, TypeError): 675 # e.g. astype_nansafe can fail on object-dtype of strings ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna) 1072 # work around NumPy brokenness, #1987 1073 if np.issubdtype(dtype.type, np.integer): -&gt; 1074 return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape) 1075 1076 # if we have a datetime/timedelta array of objects pandas/_libs/lib.pyx in pandas._libs.lib.astype_intsafe() ValueError: invalid literal for int() with base 10: &#39;1927 |9999 &#39; . we can see that it has some invalid values. To keep things simple let&#39;s remove any rows with a missing artist end date: . met = met.dropna(subset=[&#39;Artist End Date&#39;]) . Then use our function to find the invalid values as before: . met[&#39;Artist End Date&#39;].apply(check_int).dropna() . 34 1927 |9999 111 1933 |1902 112 1933 |1902 203 1933 |1902 204 1933 |1902 ... 474468 1840 |1875 474469 1840 |1875 474470 1831 |1840 474472 1890 |1831 |1842 |1864 474473 1900 |1842 Name: Artist End Date, Length: 69630, dtype: object . Here we have a much more complicated situation than the acession date. Where previously we had only two invalid values to worry about, now we have neary 70,000. However, since these values are in a pandas series, it&#39;s easy to summarize them using value_counts: . met[&#39;Artist End Date&#39;].apply(check_int).dropna().value_counts() . 1635 |1661 879 1682 |1716 589 1897 |1889 520 9999 |9999 426 1870 |1848 |1848 379 ... 1815 |1808 |1850 |1787 1 1641 |1713 1 1780 |1699 |1766 |1695 |1789 1 1868 |1879 1 1917 |1660 1 Name: Artist End Date, Length: 24693, dtype: int64 . The pattern is a bit tricky to see here. Let&#39;s just take the single most common invalid value: . met[&#39;Artist End Date&#39;].apply(check_int).dropna().value_counts().index[0] . &#39;1635 |1661 &#39; . It turns out to be the string &#39;1635 |1661 &#39;. This looks like it&#39;s structured data that has been taken from a table and accidentally included two values. Let&#39;s get the first row that has a non-integer value in the artist end date column (row 34): . met.loc[34] . Object Number 04.1a–c Is Highlight True Is Timeline Work True Is Public Domain False Object ID 35 Gallery Number 706 Department The American Wing AccessionYear 1904.0 Object Name Vase Title The Adams Vase Culture American Period NaN Dynasty NaN Reign NaN Portfolio NaN Constiuent ID 108316253 Artist Role Designer|Manufacturer Artist Prefix Designed by|Manufactured by Artist Display Name Paulding Farnham|Tiffany &amp; Co. Artist Display Bio 1859–1927|1837–present Artist Suffix NaN Artist Alpha Sort Farnham, Paulding|Tiffany &amp; Co. Artist Nationality NaN Artist Begin Date 1859 |1837 Artist End Date 1927 |9999 Artist Gender | Artist ULAN URL http://vocab.getty.edu/page/ulan/500336597|htt... Artist Wikidata URL | Object Date 1893–95 Object Begin Date 1893 Object End Date 1895 Medium Gold, amethysts, spessartites, tourmalines, fr... Dimensions Overall: 19 7/16 x 13 x 9 1/4 in. (49.4 x 33 x... Credit Line Gift of Edward D. Adams, 1904 Geography Type Made in City New York State NaN County NaN Country United States Region NaN Subregion NaN Locale NaN Locus NaN Excavation NaN River NaN Classification Metal Rights and Reproduction NaN Link Resource http://www.metmuseum.org/art/collection/search/35 Object Wikidata URL https://www.wikidata.org/wiki/Q83545838 Metadata Date NaN Repository Metropolitan Museum of Art, New York, NY Tags Animals|Garlands|Birds|Men Tags AAT URL http://vocab.getty.edu/page/aat/300249525|http... Tags Wikidata URL https://www.wikidata.org/wiki/Q729|https://www... Name: 34, dtype: object . Now we can see what&#39;s going on - take a look at the artist roles and dates: . met.loc[34][[&#39;Artist Role&#39;, &#39;Artist Display Name&#39;, &#39;Artist End Date&#39;]] . Artist Role Designer|Manufacturer Artist Display Name Paulding Farnham|Tiffany &amp; Co. Artist End Date 1927 |9999 Name: 34, dtype: object . This vase has two listed artists - the designer (Paulding Farnman) and the manufacturer (Tiffany &amp; Co.). The designer died in 1927, but the manufacturer is still around. These data nicely illustrate two common features of messy curated data - multiple pieces of data being forced into a single field, and the use of arbitrary values (in this case the date 9999) to represent special cases. . How are we going to deal with these data? For this example, let&#39;s assume that for each object the most important contributor is listed first. So we will take our non-integer end dates: . met[&#39;Artist End Date&#39;].apply(check_int).dropna() . 34 1927 |9999 111 1933 |1902 112 1933 |1902 203 1933 |1902 204 1933 |1902 ... 474468 1840 |1875 474469 1840 |1875 474470 1831 |1840 474472 1890 |1831 |1842 |1864 474473 1900 |1842 Name: Artist End Date, Length: 69630, dtype: object . and fix them by splitting on the tab character and taking the first element of the resulting string: . fixed_values = met[&#39;Artist End Date&#39;].apply(check_int).dropna().str.split(&#39; |&#39;).str.get(0) fixed_values . 34 1927 111 1933 112 1933 203 1933 204 1933 ... 474468 1840 474469 1840 474470 1831 474472 1890 474473 1900 Name: Artist End Date, Length: 69630, dtype: object . Just as before, we will update the series using these fixed values: . met[&#39;Artist End Date&#39;].update(fixed_values) . Now we can try to change the series to an int: . met[&#39;Artist End Date&#39;].astype(int) . ValueError Traceback (most recent call last) &lt;ipython-input-62-4702df368e1d&gt; in &lt;module&gt; -&gt; 1 met[&#39;Artist End Date&#39;].astype(int) ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/generic.py in astype(self, dtype, copy, errors) 5870 else: 5871 # else, only a single dtype is given -&gt; 5872 new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors) 5873 return self._constructor(new_data).__finalize__(self, method=&#34;astype&#34;) 5874 ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/managers.py in astype(self, dtype, copy, errors) 629 self, dtype, copy: bool = False, errors: str = &#34;raise&#34; 630 ) -&gt; &#34;BlockManager&#34;: --&gt; 631 return self.apply(&#34;astype&#34;, dtype=dtype, copy=copy, errors=errors) 632 633 def convert( ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, ignore_failures, **kwargs) 425 applied = b.apply(f, **kwargs) 426 else: --&gt; 427 applied = getattr(b, f)(**kwargs) 428 except (TypeError, NotImplementedError): 429 if not ignore_failures: ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/blocks.py in astype(self, dtype, copy, errors) 671 vals1d = values.ravel() 672 try: --&gt; 673 values = astype_nansafe(vals1d, dtype, copy=True) 674 except (ValueError, TypeError): 675 # e.g. astype_nansafe can fail on object-dtype of strings ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna) 1072 # work around NumPy brokenness, #1987 1073 if np.issubdtype(dtype.type, np.integer): -&gt; 1074 return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape) 1075 1076 # if we have a datetime/timedelta array of objects pandas/_libs/lib.pyx in pandas._libs.lib.astype_intsafe() ValueError: invalid literal for int() with base 10: &#39;2005-08-01&#39; . But we get the same error as before! Just like with the accession date column, the artist end date column has some dates written in year-month-day format. We can find them with one more trip through our check_int function: . met[&#39;Artist End Date&#39;].apply(check_int).dropna() . 65537 2005-08-01 65632 1972-03-23 65645 2004-09-28 65675 2002-06-12 65692 2004-09-28 ... 471268 1971-07-26 471269 1988-11-15 471270 1971-07-26 472106 1971-07-26 472186 2007-06-22 Name: Artist End Date, Length: 1775, dtype: object . Then fix them like we did before: . fixed_values = met[&#39;Artist End Date&#39;].apply(check_int).dropna().str.split(&#39;-&#39;).str.get(0) fixed_values . 65537 2005 65632 1972 65645 2004 65675 2002 65692 2004 ... 471268 1971 471269 1988 471270 1971 472106 1971 472186 2007 Name: Artist End Date, Length: 1775, dtype: object . update the series once more: . met[&#39;Artist End Date&#39;].update(fixed_values) . and turn it into an integer: . met[&#39;Artist End Date&#39;] = met[&#39;Artist End Date&#39;].astype(int) . Now we can finally do our query: which objects were acquired after the death of the artist? . met[ met[&#39;AccessionYear&#39;].astype(int) &gt; met[&#39;Artist End Date&#39;] ] . Object Number Is Highlight ... Tags AAT URL Tags Wikidata URL . 0 1979.486.1 | False | ... | NaN | NaN | . 1 1980.264.5 | False | ... | NaN | NaN | . 9 1979.486.3 | False | ... | NaN | NaN | . 10 1979.486.2 | False | ... | NaN | NaN | . 11 1979.486.7 | False | ... | NaN | NaN | . ... ... | ... | ... | ... | ... | . 474473 69.524.28 | False | ... | http://vocab.getty.edu/page/aat/300025943 | https://www.wikidata.org/wiki/Q467 | . 474503 TR.124.2020 | False | ... | NaN | NaN | . 474512 2002.233.25 | False | ... | http://vocab.getty.edu/page/aat/300164595 | https://www.wikidata.org/wiki/Q335261 | . 474519 64.101.433a, b | False | ... | http://vocab.getty.edu/page/aat/300250047|http... | https://www.wikidata.org/wiki/Q3736439|https:/... | . 474522 69.292.12 | False | ... | http://vocab.getty.edu/page/aat/300250049|http... | https://www.wikidata.org/wiki/Q2092297|https:/... | . 183125 rows × 54 columns . Extending to other data types . We can use the same kind of helper function to find value that can&#39;t be converted to other data types. All we have to do is change the conversion code inside the function. So, for example, to find values that can&#39;t be converted to floats, we would write this: . def check_float(value): try: float(value) return np.NaN except ValueError: return value . And then see how it works, remembering that NaN in the output indicates values that were converted without any problem: . test = pd.Series([42, 3.1415, &#39;banana&#39;]) test.apply(check_float) . 0 NaN 1 NaN 2 banana dtype: object . With some pandas data types we have to be a bit careful. In particular, we need to remember that the nullable integer data type Int64 (with an uppercase I) is more strict than the regular int64. In particular, int64 will happily convert a floating point number to an integer by truncating it: . pd.Series([3.1415]).astype(&#39;int64&#39;) . 0 3 dtype: int64 . and will work with string representations of integers: . pd.Series([&#39;42&#39;]).astype(&#39;int64&#39;) . 0 42 dtype: int64 . In contrast, Int64 will raise an error when trying to convert either an integer represented as a string: . pd.Series([&#39;42&#39;]).astype(&#39;Int64&#39;) . TypeError Traceback (most recent call last) &lt;ipython-input-122-5bf16a32e0d4&gt; in &lt;module&gt; -&gt; 1 pd.Series([&#39;42&#39;]).astype(&#39;Int64&#39;) ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/generic.py in astype(self, dtype, copy, errors) 5870 else: 5871 # else, only a single dtype is given -&gt; 5872 new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors) 5873 return self._constructor(new_data).__finalize__(self, method=&#34;astype&#34;) 5874 ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/managers.py in astype(self, dtype, copy, errors) 629 self, dtype, copy: bool = False, errors: str = &#34;raise&#34; 630 ) -&gt; &#34;BlockManager&#34;: --&gt; 631 return self.apply(&#34;astype&#34;, dtype=dtype, copy=copy, errors=errors) 632 633 def convert( ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, ignore_failures, **kwargs) 425 applied = b.apply(f, **kwargs) 426 else: --&gt; 427 applied = getattr(b, f)(**kwargs) 428 except (TypeError, NotImplementedError): 429 if not ignore_failures: ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/blocks.py in astype(self, dtype, copy, errors) 671 vals1d = values.ravel() 672 try: --&gt; 673 values = astype_nansafe(vals1d, dtype, copy=True) 674 except (ValueError, TypeError): 675 # e.g. astype_nansafe can fail on object-dtype of strings ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna) 1017 # dispatch on extension dtype if needed 1018 if is_extension_array_dtype(dtype): -&gt; 1019 return dtype.construct_array_type()._from_sequence(arr, dtype=dtype, copy=copy) 1020 1021 if not isinstance(dtype, np.dtype): ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/arrays/integer.py in _from_sequence(cls, scalars, dtype, copy) 361 cls, scalars, *, dtype=None, copy: bool = False 362 ) -&gt; &#34;IntegerArray&#34;: --&gt; 363 return integer_array(scalars, dtype=dtype, copy=copy) 364 365 @classmethod ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/arrays/integer.py in integer_array(values, dtype, copy) 141 TypeError if incompatible types 142 &#34;&#34;&#34; --&gt; 143 values, mask = coerce_to_array(values, dtype=dtype, copy=copy) 144 return IntegerArray(values, mask) 145 ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/arrays/integer.py in coerce_to_array(values, dtype, mask, copy) 225 &#34;mixed-integer-float&#34;, 226 ]: --&gt; 227 raise TypeError(f&#34;{values.dtype} cannot be converted to an IntegerDtype&#34;) 228 229 elif is_bool_dtype(values) and is_integer_dtype(dtype): TypeError: object cannot be converted to an IntegerDtype . or when trying to convert a floating point number: . pd.Series([3.1415]).astype(&#39;Int64&#39;) . TypeError Traceback (most recent call last) ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/arrays/integer.py in safe_cast(values, dtype, copy) 154 try: --&gt; 155 return values.astype(dtype, casting=&#34;safe&#34;, copy=copy) 156 except TypeError as err: TypeError: Cannot cast array data from dtype(&#39;float64&#39;) to dtype(&#39;int64&#39;) according to the rule &#39;safe&#39; The above exception was the direct cause of the following exception: TypeError Traceback (most recent call last) &lt;ipython-input-123-3ee8009491ea&gt; in &lt;module&gt; -&gt; 1 pd.Series([3.1415]).astype(&#39;Int64&#39;) ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/generic.py in astype(self, dtype, copy, errors) 5870 else: 5871 # else, only a single dtype is given -&gt; 5872 new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors) 5873 return self._constructor(new_data).__finalize__(self, method=&#34;astype&#34;) 5874 ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/managers.py in astype(self, dtype, copy, errors) 629 self, dtype, copy: bool = False, errors: str = &#34;raise&#34; 630 ) -&gt; &#34;BlockManager&#34;: --&gt; 631 return self.apply(&#34;astype&#34;, dtype=dtype, copy=copy, errors=errors) 632 633 def convert( ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, ignore_failures, **kwargs) 425 applied = b.apply(f, **kwargs) 426 else: --&gt; 427 applied = getattr(b, f)(**kwargs) 428 except (TypeError, NotImplementedError): 429 if not ignore_failures: ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/internals/blocks.py in astype(self, dtype, copy, errors) 671 vals1d = values.ravel() 672 try: --&gt; 673 values = astype_nansafe(vals1d, dtype, copy=True) 674 except (ValueError, TypeError): 675 # e.g. astype_nansafe can fail on object-dtype of strings ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna) 1017 # dispatch on extension dtype if needed 1018 if is_extension_array_dtype(dtype): -&gt; 1019 return dtype.construct_array_type()._from_sequence(arr, dtype=dtype, copy=copy) 1020 1021 if not isinstance(dtype, np.dtype): ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/arrays/integer.py in _from_sequence(cls, scalars, dtype, copy) 361 cls, scalars, *, dtype=None, copy: bool = False 362 ) -&gt; &#34;IntegerArray&#34;: --&gt; 363 return integer_array(scalars, dtype=dtype, copy=copy) 364 365 @classmethod ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/arrays/integer.py in integer_array(values, dtype, copy) 141 TypeError if incompatible types 142 &#34;&#34;&#34; --&gt; 143 values, mask = coerce_to_array(values, dtype=dtype, copy=copy) 144 return IntegerArray(values, mask) 145 ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/arrays/integer.py in coerce_to_array(values, dtype, mask, copy) 258 values = safe_cast(values, dtype, copy=False) 259 else: --&gt; 260 values = safe_cast(values, dtype, copy=False) 261 262 return values, mask ~/.virtualenvs/drawingfromdata/lib/python3.8/site-packages/pandas/core/arrays/integer.py in safe_cast(values, dtype, copy) 160 return casted 161 --&gt; 162 raise TypeError( 163 f&#34;cannot safely cast non-equivalent {values.dtype} to {np.dtype(dtype)}&#34; 164 ) from err TypeError: cannot safely cast non-equivalent float64 to int64 . So if we want to write a function that will identify values that can&#39;t be changed to Int64, we have to construct a series inside the try block and force the Int64 dtype: . def check_nullable_int(value): try: pd.Series([value], dtype=&#39;Int64&#39;) return np.NaN except: return value . Testing it shows the behaviour: . pd.Series([42, &#39;42&#39;, 3.1415, &#39;3.1415&#39;, &#39;banana&#39;]).apply(check_nullable_int) . 0 NaN 1 42 2 3.141500 3 3.1415 4 banana dtype: object . Unsurprisingly, this approach is much slower: . %timeit met[&#39;Artist End Date&#39;].apply(check_int) %timeit met[&#39;Artist End Date&#39;].apply(check_nullable_int) . 45.1 ms ± 452 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) 17.7 s ± 184 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . So for simple cases we should probably use the int and float functions. . Conclusion . This type of messy data, where we have multiple different representations in the same column, occurs very frequently in real world manually curated datasets. Writing a small helper function like our check_int can make it much easier to find the values that are causing problems, and figure out how to fix them. We need to watch out for slight differences between data types. . If you&#39;ve made it this far, you should definitely subscribe to the Drawing from Data newsletter, follow me on Twitter, or buy the Drawing from Data book! .",
            "url": "https://drawingfromdata.com/pandas/dtypes/finding-invalid-values-in-numerical-columns.html",
            "relUrl": "/pandas/dtypes/finding-invalid-values-in-numerical-columns.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Why does my memory usage explode when concatenating dataframes?",
            "content": "tldr: concatenating categorical Series with nonidentical categories gives an object dtype in the result, with severe memory implications. . Introduction . In a library as large and featureful as pandas, there are bound to be surprising behaviours. In this article we will take a look at a memory issue that I&#39;ve run into multiple times in real life datasets - an unexpected increase in memory usage when concatenating multiple dataframes. . As always, to keep this article to a reasonable size we won&#39;t be going in to too much detail on the methods involved. All of the methods we&#39;ll use here are covered in detail in the Drawing from Data book. In particular, check out chapter 2 for a discussion of data types and chapter 16 for a discussion of the memory implications. . If you&#39;re interested in more articles and tips on data exploration with Python, you should subscribe to the Drawing from Data newsletter or follow me on Twitter. . Saving memory on a single data file . As an example, we&#39;ll be using a slightly modified version of this car accident dataset: . https://www.kaggle.com/sobhanmoosavi/us-accidents . This dataset contains records of car accidents in the USA, and for the purposes of this story we have one file per month. Our goal is to combine all of these to make one large dataframe that we can use for analysis. Let&#39;s load up the first file to see the structure: . import pandas as pd # load just the data for January df = pd.read_csv(&#39;January_car_accidents.csv&#39;) df . City State month . 0 El Cerrito | CA | January | . 1 Berkeley | CA | January | . 2 Oakley | CA | January | . 3 Richmond | CA | January | . 4 El Cerrito | CA | January | . ... ... | ... | ... | . 301919 Sun Valley | CA | January | . 301920 Costa Mesa | CA | January | . 301921 Costa Mesa | CA | January | . 301922 Madras | OR | January | . 301923 Jordan Valley | OR | January | . 301924 rows × 3 columns . This is pretty straightforward - we have columns for City, State and Month, and around 300,000 rows. Each row represents a single car accident. In the original dataset, of course, there are many more columns. For this article we are interested in the State column. . Let&#39;s assume that because this is a large dataset, we&#39;re worried about running out of memory. We know that for columns with a small number of values, storing them as a categorical data type can save a lot of memory (see chapter 16 in the Drawing from Data book for a full discussion of memory issues). Let&#39;s check how much memory the State column uses when stored as object. Because memory_usage returns the result in bytes, we&#39;ll divide by 1e6 to get an answer in megabytes: . df[&#39;State&#39;].memory_usage(deep=True) / 1e6 . 17.813644 . Now the same column as a category: . df[&#39;State&#39;].astype(&#39;category&#39;).memory_usage(deep=True) / 1e6 . 0.305956 . As expected, a considerable saving. Once we&#39;ve figured this out, it&#39;s probably a good habit to specify a categorical data types when reading in the file: . df = pd.read_csv(&#39;January_car_accidents.csv&#39;, dtype={&#39;State&#39; : &#39;category&#39;}) df.memory_usage(deep=True) / 1e6 . Index 0.000128 City 19.861495 State 0.305828 month 19.323136 dtype: float64 . Incidentally, the City and month columns would also be better stored as categories, but we will ignore that for now. . Concatenating multiple data files . Let&#39;s now make a loop to read all 12 data files, and check the memory usage for each, remembering to keep the &#39;category&#39; dtype: . import glob for filename in glob.glob(&#39;*car_accidents.csv&#39;): month_df = pd.read_csv(filename, dtype = {&#39;State&#39; : &#39;category&#39;}) print(filename, month_df[&#39;State&#39;].memory_usage(deep=True) / 1e6) . March_car_accidents.csv 0.29748 April_car_accidents.csv 0.30353 September_car_accidents.csv 0.296511 February_car_accidents.csv 0.288488 May_car_accidents.csv 0.300636 January_car_accidents.csv 0.305956 July_car_accidents.csv 0.227059 June_car_accidents.csv 0.314354 November_car_accidents.csv 0.303147 December_car_accidents.csv 0.303714 October_car_accidents.csv 0.328636 August_car_accidents.csv 0.293021 . Obviously the exact numbers differ, but each of our single-month data files takes around 0.3 Mb for the State column. Our estimate for the memory usage of the State column in our combined dataframe, therefore, is 0.3 * 12: around 3.6 Mb. . The best way to combine these data files is to make a list of dataframes then concatenate them at the end: . all_dfs = [] for filename in glob.glob(&#39;*car_accidents.csv&#39;): month_df = pd.read_csv(filename, dtype = {&#39;State&#39; : &#39;category&#39;}) all_dfs.append(month_df) big_df = pd.concat(all_dfs) big_df . City State month . 0 Columbus | OH | March | . 1 Miamisburg | OH | March | . 2 Dayton | OH | March | . 3 Columbus | OH | March | . 4 Columbus | OH | March | . ... ... | ... | ... | . 288925 Riverside | CA | August | . 288926 San Diego | CA | August | . 288927 Orange | CA | August | . 288928 Culver City | CA | August | . 288929 Highland | CA | August | . 3513617 rows × 3 columns . All looks fine here: for our combined dataset we have around 3.5 million rows. But take a look at the memory usage: . big_df[&#39;State&#39;].memory_usage(deep=True) / 1e6 . 235.412339 . Far from taking up less than 4 Mb as expected, our State column is taking up more than 200 Mb. What has happened? The clue is in the data type: . big_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 3513617 entries, 0 to 288929 Data columns (total 3 columns): # Column Dtype -- 0 City object 1 State object 2 month object dtypes: object(3) memory usage: 107.2+ MB . We have lost our categorical type; the State colum in the big dataframe has been turned back into object. The reason: not every state is present in every single-month data file: . big_df.groupby(&#39;month&#39;)[&#39;State&#39;].nunique() . month April 48 August 49 December 49 February 49 January 48 July 49 June 48 March 49 May 49 November 49 October 49 September 49 Name: State, dtype: int64 . Most months have records for 49 states, but a few have records for only 48. We can find the missing states quite easily with a mixture of pandas and Python&#39;s built in set type: . april = big_df[big_df[&#39;month&#39;] == &#39;April&#39;] august = big_df[big_df[&#39;month&#39;] == &#39;August&#39;] # which state has records for August but not April set(august[&#39;State&#39;]) - set(april[&#39;State&#39;]) . {&#39;ND&#39;} . The missing states mean that the categories are slightly different between the individual month dataframes, and when we ask pandas to concatenate categorical datasets with different categories, it sets the data type back to object. We can see this behaviour with a simple example: . cat_series_one = pd.Series([&#39;apple&#39;, &#39;banana&#39;, &#39;orange&#39;]).astype(&#39;category&#39;) cat_series_two = pd.Series([&#39;apple&#39;, &#39;banana&#39;, &#39;banana&#39;]).astype(&#39;category&#39;) combined_series = pd.concat([cat_series_one, cat_series_two]) # look at the dtype for the combined series combined_series . 0 apple 1 banana 2 orange 0 apple 1 banana 2 banana dtype: object . We will encounter this problem any time we use pd.concat on categorical columns where the categories are not exactly identical. One such scenario is the one we&#39;ve just looked at: when we need to combine multiple data files. . Concatenation and chunking . Another is when we use chunking to read a data file one piece at a time. The canonical way to read a large CSV file in chunks is to append the chunks to a list then use pd.concat on the list: . all_dfs = [] # process 100,000 rows at a time for chunk in pd.read_csv(&#39;January_car_accidents.csv&#39;, dtype={&#39;State&#39; : &#39;category&#39;}, chunksize=100000): # possibly do some processing on chunk all_dfs.append(chunk) pd.concat(all_dfs).info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 301924 entries, 0 to 301923 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 City 301914 non-null object 1 State 301924 non-null object 2 month 301924 non-null object dtypes: object(3) memory usage: 6.9+ MB . Here we run into the same issue. Although we have specified a categorical data type for the State column for each chunk, when we combine them the State column gets turned back into an object, since each chunk has a slightly different set of unique states in it. . This version of the problem is especially harsh, as the most common reason for chunking input files in the first place is to reduce peak memory usage! So it&#39;s an especially nasty surprise when the memory usage of the State column turns out to be so much higher than our estimate. . Fixing the problem . We can get round this problem in a number of ways. If we have enough memory, we can simply take our combined dataframe and change the State column to a category after it&#39;s been assembled: . big_df[&#39;State&#39;] = big_df[&#39;State&#39;].astype(&#39;category&#39;) big_df.memory_usage(deep=True) / 1e6 . Index 28.108936 City 231.140089 State 3.517580 month 222.078650 dtype: float64 . That gets the State column memory usage back down to our estimate of around 3.5 Mb. . If we haven&#39;t got enough memory to do this, we have to force all the single-month dataframe State columns to have identical categories. We can do this by explicitly creating a categorical data type and listing the categories that we want: . from pandas.api.types import CategoricalDtype # just a list of Python strings states = [&quot;AL&quot;, &quot;AK&quot;, &quot;AZ&quot;, &quot;AR&quot;, &quot;CA&quot;, &quot;CO&quot;, &quot;CT&quot;, &quot;DC&quot;, &quot;DE&quot;, &quot;FL&quot;, &quot;GA&quot;, &quot;HI&quot;, &quot;ID&quot;, &quot;IL&quot;, &quot;IN&quot;, &quot;IA&quot;, &quot;KS&quot;, &quot;KY&quot;, &quot;LA&quot;, &quot;ME&quot;, &quot;MD&quot;, &quot;MA&quot;, &quot;MI&quot;, &quot;MN&quot;, &quot;MS&quot;, &quot;MO&quot;, &quot;MT&quot;, &quot;NE&quot;, &quot;NV&quot;, &quot;NH&quot;, &quot;NJ&quot;, &quot;NM&quot;, &quot;NY&quot;, &quot;NC&quot;, &quot;ND&quot;, &quot;OH&quot;, &quot;OK&quot;, &quot;OR&quot;, &quot;PA&quot;, &quot;RI&quot;, &quot;SC&quot;, &quot;SD&quot;, &quot;TN&quot;, &quot;TX&quot;, &quot;UT&quot;, &quot;VT&quot;, &quot;VA&quot;, &quot;WA&quot;, &quot;WV&quot;, &quot;WI&quot;, &quot;WY&quot;] state_type = CategoricalDtype(categories=states) state_type . CategoricalDtype(categories=[&#39;AL&#39;, &#39;AK&#39;, &#39;AZ&#39;, &#39;AR&#39;, &#39;CA&#39;, &#39;CO&#39;, &#39;CT&#39;, &#39;DC&#39;, &#39;DE&#39;, &#39;FL&#39;, &#39;GA&#39;, &#39;HI&#39;, &#39;ID&#39;, &#39;IL&#39;, &#39;IN&#39;, &#39;IA&#39;, &#39;KS&#39;, &#39;KY&#39;, &#39;LA&#39;, &#39;ME&#39;, &#39;MD&#39;, &#39;MA&#39;, &#39;MI&#39;, &#39;MN&#39;, &#39;MS&#39;, &#39;MO&#39;, &#39;MT&#39;, &#39;NE&#39;, &#39;NV&#39;, &#39;NH&#39;, &#39;NJ&#39;, &#39;NM&#39;, &#39;NY&#39;, &#39;NC&#39;, &#39;ND&#39;, &#39;OH&#39;, &#39;OK&#39;, &#39;OR&#39;, &#39;PA&#39;, &#39;RI&#39;, &#39;SC&#39;, &#39;SD&#39;, &#39;TN&#39;, &#39;TX&#39;, &#39;UT&#39;, &#39;VT&#39;, &#39;VA&#39;, &#39;WA&#39;, &#39;WV&#39;, &#39;WI&#39;, &#39;WY&#39;], , ordered=False) . Now when we create a series with this type, it will always have all the states as its categories regardless of which states are actually in the data: . pd.Series([&#39;KS&#39;, &#39;TX&#39;, &#39;VT&#39;]).astype(state_type) . 0 KS 1 TX 2 VT dtype: category Categories (51, object): [&#39;AL&#39;, &#39;AK&#39;, &#39;AZ&#39;, &#39;AR&#39;, ..., &#39;WA&#39;, &#39;WV&#39;, &#39;WI&#39;, &#39;WY&#39;] . Notice that the above series has all 51 states in its categories even though there are only 3 in the data. . Using this new type in our input loop results in the categorical data type being preserved after concatenation: . all_dfs = [] for filename in glob.glob(&#39;*car_accidents.csv&#39;): month_df = pd.read_csv(filename, dtype = {&#39;State&#39; : state_type}) all_dfs.append(month_df) big_df = pd.concat(all_dfs) big_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 3513617 entries, 0 to 288929 Data columns (total 3 columns): # Column Dtype -- 0 City object 1 State category 2 month object dtypes: category(1), object(2) memory usage: 83.8+ MB . In this case I&#39;ve just copied an existing Python list of state abbreviations that I found online. In other situations it might be possible to generate the list by reading all the input files one line at a time: . all_states = set() for filename in glob.glob(&#39;*car_accidents.csv&#39;): for line in open(filename): state = line.split(&#39;,&#39;)[1] all_states.add(state) # remove the column name all_states.remove(&#39;State&#39;) print(all_states) . {&#39;WY&#39;, &#39;NY&#39;, &#39;MS&#39;, &#39;IL&#39;, &#39;NJ&#39;, &#39;MA&#39;, &#39;DC&#39;, &#39;DE&#39;, &#39;UT&#39;, &#39;WA&#39;, &#39;OK&#39;, &#39;RI&#39;, &#39;CO&#39;, &#39;IN&#39;, &#39;TX&#39;, &#39;AZ&#39;, &#39;WI&#39;, &#39;TN&#39;, &#39;FL&#39;, &#39;IA&#39;, &#39;SC&#39;, &#39;ME&#39;, &#39;ND&#39;, &#39;NC&#39;, &#39;MD&#39;, &#39;SD&#39;, &#39;NE&#39;, &#39;KS&#39;, &#39;PA&#39;, &#39;GA&#39;, &#39;OR&#39;, &#39;MT&#39;, &#39;KY&#39;, &#39;NH&#39;, &#39;ID&#39;, &#39;MN&#39;, &#39;OH&#39;, &#39;VA&#39;, &#39;MI&#39;, &#39;NM&#39;, &#39;CT&#39;, &#39;CA&#39;, &#39;MO&#39;, &#39;AL&#39;, &#39;WV&#39;, &#39;AR&#39;, &#39;NV&#39;, &#39;LA&#39;, &#39;VT&#39;} . Although this takes a while to run, it&#39;s very memory-friendly as we only have to store a single row of each input file in memory at a time. . If you&#39;ve made it this far, you should definitely subscribe to the Drawing from Data newsletter, follow me on Twitter, or buy the Drawing from Data book! .",
            "url": "https://drawingfromdata.com/pandas/concat/memory/exploding-memory-usage-with-concat-and-categories.html",
            "relUrl": "/pandas/concat/memory/exploding-memory-usage-with-concat-and-categories.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Making a difficult data analysis question easy",
            "content": "Introduction . One of the things that makes data exploration such a powerful skill to have is the ease with which we can find interesting datasets online. We can search for data that will help us to answer real world questions, or just browse for interesting collections of data (that&#39;s how I picked all the example datasets for the videos that accompany the Drawing from Data book). . One downside, however, of finding existing datasets to work with is that we have no control over the format of the data. When we hit the download button on a new dataset we might find all sorts of things that will make our analysis difficult. Some of these I&#39;ve already written about - invalid data, missing data, data spread over multiple files or with obvious errors. . A more subtle problem is when the data are not in the structure that we would like. This issue is bit more complex than the others. If a dataset has annotation errors then we could all agree that that is bad. But the specific way that a data file is structured isn&#39;t good or bad in itself - it&#39;s just that a given structure makes some problems easy and some problems hard. . Thankfully, pandas has very powerful tools for reshaping and restructuring data. In this article we&#39;ll try to answer a problem that is difficult given the structure of our data file, but becomes very easy when we reshape the data. . As always, to keep this article to a reasonable size we won&#39;t be going in to too much detail on the methods involved. All of the methods we&#39;ll use here are covered in detail in the Drawing from Data book. In particular, check out chapter 3 for a discussion of calculations on series and chapter 14 for the reshaping tools. . If you&#39;re interested in more articles and tips on data exploration with Python, you should subscribe to the Drawing from Data newsletter or follow me on Twitter. . Taking a look at the dataset . Our starting point for this analysis is a data file giving the highest and lowest mean daily temperature in Farenheit for each year for a bunch of different cities. Let&#39;s take a look at it: . import pandas as pd pd.options.display.max_columns=6 pd.options.display.max_rows=10 cities = pd.read_csv(&#39;city_min_max.csv&#39;) cities . Year Abidjan_max Abidjan_min ... Youngstown_min Zurich_max Zurich_min . 0 1995 | 86.7 | 73.0 | ... | 4.8 | 77.6 | 17.1 | . 1 1996 | 86.1 | 72.2 | ... | -2.1 | 74.0 | 10.7 | . 2 1997 | 85.1 | 73.0 | ... | 2.5 | 73.4 | 14.2 | . 3 1998 | 87.2 | 73.4 | ... | 13.2 | 78.6 | 19.7 | . 4 1999 | 87.2 | 73.4 | ... | 3.3 | 75.8 | 16.3 | . ... ... | ... | ... | ... | ... | ... | ... | . 21 2016 | 86.9 | 74.9 | ... | 7.0 | 76.4 | 20.0 | . 22 2017 | 86.4 | 74.6 | ... | 7.9 | 80.3 | 17.4 | . 23 2018 | 85.8 | 74.6 | ... | 2.0 | 80.5 | 14.5 | . 24 2019 | 87.0 | 74.5 | ... | -4.0 | 81.7 | 26.1 | . 25 2020 | 86.4 | 77.4 | ... | 15.4 | 67.1 | 27.0 | . 26 rows × 547 columns . Hopefully it&#39;s not too hard to see the structure. We have a year column, then a bunch of paired minimum and maximum columns for each city. Notice that this is a very wide dataframe, with many more columns than rows. We might call it a summary table. . This format makes it very easy to answer some types of question. For example, in which years was the coldest day in Singapore warmer than the warmest day in Stockholm? To find out, we just have to filter on these two columns then take the year column: . cities[cities[&#39;Stockholm_max&#39;] &lt; cities[&#39;Singapore_min&#39;]][&#39;Year&#39;] . 1 1996 2 1997 3 1998 5 2000 7 2002 ... 18 2013 20 2015 21 2016 22 2017 25 2020 Name: Year, Length: 16, dtype: int64 . The challenge . Our goal for this analysis is simply to calculate the different between the minimum and maximum for each city in each year. Let&#39;s call this the temperature range. We would like to end up with a very simple table that has three columns: city, year and temperature range. . Now, in many ways the above dataframe looks like it will be very nice to work with. We have no missing data, no invalid values, all the columns are floating point with the exception of year. So this should be an easy calculation to carry out. . Option one: working with the data in the existing form . Let&#39;s start with something simple - can we calculate the temperature range for a single city? Given the column names, that&#39;s easy - we just subtract the two series: . cities[&#39;London_max&#39;] - cities[&#39;London_min&#39;] . 0 52.0 1 50.4 2 49.9 3 42.7 4 46.8 ... 21 45.5 22 47.3 23 55.3 24 52.0 25 28.0 Length: 26, dtype: float64 . These numbers aren&#39;t much use without the year column, so let&#39;s add that. The easiest way is to set year as the index, so that it gets propagated to the resulting series: . cities.set_index(&#39;Year&#39;)[&#39;London_max&#39;] - cities.set_index(&#39;Year&#39;)[&#39;London_min&#39;] . Year 1995 52.0 1996 50.4 1997 49.9 1998 42.7 1999 46.8 ... 2016 45.5 2017 47.3 2018 55.3 2019 52.0 2020 28.0 Length: 26, dtype: float64 . OK, now let&#39;s try multiple cities. If we just manually pick a few cities and put the calculation in a loop, it&#39;s fairly straightforward to create the series. For each city name we can figure out the column names by appending _min and _max, then calculate the temperature range by subtracting them like before: . for city in [&#39;Birmingham&#39;, &#39;Delhi&#39;, &#39;Osaka&#39;]: temp_range = cities.set_index(&#39;Year&#39;)[f&#39;{city}_max&#39;] - cities.set_index(&#39;Year&#39;)[f&#39;{city}_min&#39;] print(city, temp_range) . Birmingham Year 1995 65.8 1996 74.1 1997 64.8 1998 57.4 1999 69.0 ... 2016 61.1 2017 62.8 2018 65.1 2019 58.2 2020 46.3 Length: 26, dtype: float64 Delhi Year 1995 52.2 1996 47.5 1997 51.9 1998 54.6 1999 50.7 ... 2016 48.8 2017 52.1 2018 48.5 2019 58.1 2020 41.5 Length: 26, dtype: float64 Osaka Year 1995 55.1 1996 56.1 1997 52.3 1998 56.5 1999 57.0 ... 2016 61.0 2017 55.5 2018 61.2 2019 54.2 2020 38.8 Length: 26, dtype: float64 . Although the calculation is not too hard, in the above code we&#39;re just printing the series. Life gets a lot more complicated when we want to turn those numbers into a new dataframe. Now we have to turn each city series into a dataframe, add the city name as a column, store all the individual city dataframes in a list, and concatenate them at the end: . dfs = [] for city in [&#39;Birmingham&#39;, &#39;Delhi&#39;, &#39;Osaka&#39;]: # calculate the series as before city_series = cities.set_index(&#39;Year&#39;)[f&#39;{city}_max&#39;] - cities.set_index(&#39;Year&#39;)[f&#39;{city}_min&#39;] # turn it into a dataframe and move the year into a column city_df = city_series.to_frame(&#39;temp range&#39;).reset_index() # add the city name column city_df[&#39;city&#39;] = city dfs.append(city_df) # after the loop, concatenate all of the individual city dataframes pd.concat(dfs) . Year temp range city . 0 1995 | 65.8 | Birmingham | . 1 1996 | 74.1 | Birmingham | . 2 1997 | 64.8 | Birmingham | . 3 1998 | 57.4 | Birmingham | . 4 1999 | 69.0 | Birmingham | . ... ... | ... | ... | . 21 2016 | 61.0 | Osaka | . 22 2017 | 55.5 | Osaka | . 23 2018 | 61.2 | Osaka | . 24 2019 | 54.2 | Osaka | . 25 2020 | 38.8 | Osaka | . 78 rows × 3 columns . This is starting to look pretty complicated, but at least now most of the work is done. Now we just have to replace our manual list of city names with a list of all the city names. Unfortunately, that will involve quite a bit of processing as well. We need to take all the column names, get the part before the underscore, discard any duplicates (otherwise we will get each city name twice) and remember to exclude the year column. To keep it brief let&#39;s just see the final expression (for an explanation of the string processing methods we are using, take a look at chapter 3 in the Drawing from Data book): . cities.columns.str.split(&#39;_&#39;).str.get(0).unique()[1:] . Index([&#39;Abidjan&#39;, &#39;Abilene&#39;, &#39;Abu Dhabi&#39;, &#39;Addis Ababa&#39;, &#39;Akron Canton&#39;, &#39;Albany&#39;, &#39;Albuquerque&#39;, &#39;Algiers&#39;, &#39;Allentown&#39;, &#39;Almaty&#39;, ... &#39;Washington DC&#39;, &#39;Washington&#39;, &#39;West Palm Beach&#39;, &#39;Wichita Falls&#39;, &#39;Wichita&#39;, &#39;Wilkes Barre&#39;, &#39;Windhoek&#39;, &#39;Yakima&#39;, &#39;Youngstown&#39;, &#39;Zurich&#39;], dtype=&#39;object&#39;, length=273) . Technically the result of the above expression is an Index object, not a list, but it will behave like a list when we iterate over it in a loop. . Plugging this expression into our calculation loop gives us the following code: . dfs = [] for city in cities.columns.str.split(&#39;_&#39;).str.get(0).unique()[1:]: city_series = cities.set_index(&#39;Year&#39;)[f&#39;{city}_max&#39;] - cities.set_index(&#39;Year&#39;)[f&#39;{city}_min&#39;] city_df = city_series.to_frame(&#39;temp range&#39;).reset_index() city_df[&#39;city&#39;] = city dfs.append(city_df) pd.concat(dfs) . Year temp range city . 0 1995 | 13.7 | Abidjan | . 1 1996 | 13.9 | Abidjan | . 2 1997 | 12.1 | Abidjan | . 3 1998 | 13.8 | Abidjan | . 4 1999 | 13.8 | Abidjan | . ... ... | ... | ... | . 21 2016 | 56.4 | Zurich | . 22 2017 | 62.9 | Zurich | . 23 2018 | 66.0 | Zurich | . 24 2019 | 55.6 | Zurich | . 25 2020 | 40.1 | Zurich | . 7098 rows × 3 columns . Even for experienced pandas users, this would take quite a bit of effort to understand! But notice that the actual calculation is very simple - we&#39;re just subtracting one column from another. All of the complexity is to deal with getting the information we want into the right format. The most important bit of the code - the subtraction - is hard to spot, as it&#39;s somewhat hidden inside our complicated for loop. . Option two: reshaping the data . Since all of the complexity in our above code is due to dealing with the format of the data, it&#39;s possible to imagine a different format that would make life much easier. Imagine that we had a dataframe with four columns - city, year, min and max. In that case, we would just subtract the min column from the max column and our work would be done. . We don&#39;t have a dataframe in that format, but let&#39;s see if it&#39;s possible to make one. The first step is to convert our existing dataframe into long format using melt: . tidy_cities = cities.melt(id_vars=[&#39;Year&#39;]) tidy_cities . Year variable value . 0 1995 | Abidjan_max | 86.7 | . 1 1996 | Abidjan_max | 86.1 | . 2 1997 | Abidjan_max | 85.1 | . 3 1998 | Abidjan_max | 87.2 | . 4 1999 | Abidjan_max | 87.2 | . ... ... | ... | ... | . 14191 2016 | Zurich_min | 20.0 | . 14192 2017 | Zurich_min | 17.4 | . 14193 2018 | Zurich_min | 14.5 | . 14194 2019 | Zurich_min | 26.1 | . 14195 2020 | Zurich_min | 27.0 | . 14196 rows × 3 columns . Since this is only a temporary dataframe, we will just allow melt to use the default variable and value names. . Next we can take the variable column and, using the same string processing tools as before, turn it into two columns based on the underscore: . tidy_cities[&#39;variable&#39;].str.split(&#39;_&#39;, expand=True) . 0 1 . 0 Abidjan | max | . 1 Abidjan | max | . 2 Abidjan | max | . 3 Abidjan | max | . 4 Abidjan | max | . ... ... | ... | . 14191 Zurich | min | . 14192 Zurich | min | . 14193 Zurich | min | . 14194 Zurich | min | . 14195 Zurich | min | . 14196 rows × 2 columns . We can add these two new columns by just setting new columns on the existing dataframe. We&#39;ll also drop the old variable column as we don&#39;t need it anymore: . tidy_cities[[&#39;city&#39;, &#39;measurement&#39;]] = tidy_cities[&#39;variable&#39;].str.split(&#39;_&#39;, expand=True) tidy_cities.drop(columns=[&#39;variable&#39;], inplace=True) tidy_cities . Year value city measurement . 0 1995 | 86.7 | Abidjan | max | . 1 1996 | 86.1 | Abidjan | max | . 2 1997 | 85.1 | Abidjan | max | . 3 1998 | 87.2 | Abidjan | max | . 4 1999 | 87.2 | Abidjan | max | . ... ... | ... | ... | ... | . 14191 2016 | 20.0 | Zurich | min | . 14192 2017 | 17.4 | Zurich | min | . 14193 2018 | 14.5 | Zurich | min | . 14194 2019 | 26.1 | Zurich | min | . 14195 2020 | 27.0 | Zurich | min | . 14196 rows × 4 columns . Now we just have to get the min and max as separate columns. The easiest way to do this is to set a multilevel index, use unstack to turn the measurement values as columns, then reset the index. In this step we&#39;ll also rename the columns: . city_summary = tidy_cities.set_index([&#39;Year&#39;, &#39;city&#39;, &#39;measurement&#39;]).unstack().reset_index() city_summary.columns = [&#39;year&#39;, &#39;city&#39;, &#39;max temp&#39;, &#39;min temp&#39;] city_summary . year city max temp min temp . 0 1995 | Abidjan | 86.7 | 73.0 | . 1 1995 | Abilene | 89.9 | 24.6 | . 2 1995 | Abu Dhabi | 104.4 | 62.4 | . 3 1995 | Addis Ababa | 71.8 | 55.4 | . 4 1995 | Akron Canton | 84.4 | 2.9 | . ... ... | ... | ... | ... | . 7093 2020 | Wilkes Barre | 67.2 | 15.6 | . 7094 2020 | Windhoek | 86.3 | 58.0 | . 7095 2020 | Yakima | 70.0 | 16.1 | . 7096 2020 | Youngstown | 67.3 | 15.4 | . 7097 2020 | Zurich | 67.1 | 27.0 | . 7098 rows × 4 columns . Now we have our data in the exact same format that we imagined. And, as promised, the calculation we need to do becomes very easy: . city_summary[&#39;max temp&#39;] - city_summary[&#39;min temp&#39;] . 0 13.7 1 65.3 2 42.0 3 16.4 4 81.5 ... 7093 51.6 7094 28.3 7095 53.9 7096 51.9 7097 40.1 Length: 7098, dtype: float64 . Adding this series as a new column gives us our final dataframe: . city_summary[&#39;temp range&#39;] = city_summary[&#39;max temp&#39;] - city_summary[&#39;min temp&#39;] city_summary . year city max temp min temp temp range . 0 1995 | Abidjan | 86.7 | 73.0 | 13.7 | . 1 1995 | Abilene | 89.9 | 24.6 | 65.3 | . 2 1995 | Abu Dhabi | 104.4 | 62.4 | 42.0 | . 3 1995 | Addis Ababa | 71.8 | 55.4 | 16.4 | . 4 1995 | Akron Canton | 84.4 | 2.9 | 81.5 | . ... ... | ... | ... | ... | ... | . 7093 2020 | Wilkes Barre | 67.2 | 15.6 | 51.6 | . 7094 2020 | Windhoek | 86.3 | 58.0 | 28.3 | . 7095 2020 | Yakima | 70.0 | 16.1 | 53.9 | . 7096 2020 | Youngstown | 67.3 | 15.4 | 51.9 | . 7097 2020 | Zurich | 67.1 | 27.0 | 40.1 | . 7098 rows × 5 columns . Comparing the two solutions . For the sake of comparison, let&#39;s see the two sets of code together, without comments: . # looping dfs = [] for city in cities.columns.str.split(&#39;_&#39;).str.get(0).unique()[1:]: city_series = cities.set_index(&#39;Year&#39;)[f&#39;{city}_max&#39;] - cities.set_index(&#39;Year&#39;)[f&#39;{city}_min&#39;] city_df = city_series.to_frame(&#39;temp range&#39;).reset_index() city_df[&#39;city&#39;] = city dfs.append(city_df) city_summary = pd.concat(dfs) # reshaping tidy_cities = cities.melt(id_vars=[&#39;Year&#39;]) tidy_cities[[&#39;city&#39;, &#39;measurement&#39;]] = tidy_cities[&#39;variable&#39;].str.split(&#39;_&#39;, expand=True) tidy_cities.drop(columns=[&#39;variable&#39;], inplace=True) city_summary = tidy_cities.set_index([&#39;Year&#39;, &#39;city&#39;, &#39;measurement&#39;]).unstack().reset_index() city_summary.columns = [&#39;year&#39;, &#39;city&#39;, &#39;max temp&#39;, &#39;min temp&#39;] city_summary[&#39;temp range&#39;] = city_summary[&#39;max temp&#39;] - city_summary[&#39;min temp&#39;] . 246 ms ± 2.87 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . As an aside, the solution involving reshaping is about 10x faster on my machine, but let&#39;s ignore that for now - this article isn&#39;t about performance, and we haven&#39;t made any attempt to optimize either piece of code. . Comparing the two solutions, we might note that they are about equal in complexity. If anything, the reshaping solution looks like more typing - though that&#39;s partly because if involves more column names. So why do I strongly prefer the second solution? . Think about it from the perspective of someone reading the code for the first time and trying to understand what the code is doing. In the first solution, you have to comprehend the whole for loop in one go. One can imagine adding some print statements to look at the intermediate variables in order to understand the whole thing. . The second solution has an interesting property, however - you can still understand the essential calculation even if you have no idea how the reshaping works. All the code involved in reshaping - everything up to this point: . tidy_cities = cities.melt(id_vars=[&#39;Year&#39;]) tidy_cities[[&#39;city&#39;, &#39;measurement&#39;]] = tidy_cities[&#39;variable&#39;].str.split(&#39;_&#39;, expand=True) tidy_cities.drop(columns=[&#39;variable&#39;], inplace=True) city_summary = tidy_cities.set_index([&#39;Year&#39;, &#39;city&#39;, &#39;measurement&#39;]).unstack().reset_index() city_summary.columns = [&#39;year&#39;, &#39;city&#39;, &#39;max temp&#39;, &#39;min temp&#39;] city_summary . year city max temp min temp . 0 1995 | Abidjan | 86.7 | 73.0 | . 1 1995 | Abilene | 89.9 | 24.6 | . 2 1995 | Abu Dhabi | 104.4 | 62.4 | . 3 1995 | Addis Ababa | 71.8 | 55.4 | . 4 1995 | Akron Canton | 84.4 | 2.9 | . ... ... | ... | ... | ... | . 7093 2020 | Wilkes Barre | 67.2 | 15.6 | . 7094 2020 | Windhoek | 86.3 | 58.0 | . 7095 2020 | Yakima | 70.0 | 16.1 | . 7096 2020 | Youngstown | 67.3 | 15.4 | . 7097 2020 | Zurich | 67.1 | 27.0 | . 7098 rows × 4 columns . can be pretty much ignored as long as you understand the structure of the city_summary dataframe. And as we already mentioned, once the reshaping is done, the actual calculation is very straightforward: . city_summary[&#39;temp range&#39;] = city_summary[&#39;max temp&#39;] - city_summary[&#39;min temp&#39;] city_summary . year city max temp min temp temp range . 0 1995 | Abidjan | 86.7 | 73.0 | 13.7 | . 1 1995 | Abilene | 89.9 | 24.6 | 65.3 | . 2 1995 | Abu Dhabi | 104.4 | 62.4 | 42.0 | . 3 1995 | Addis Ababa | 71.8 | 55.4 | 16.4 | . 4 1995 | Akron Canton | 84.4 | 2.9 | 81.5 | . ... ... | ... | ... | ... | ... | . 7093 2020 | Wilkes Barre | 67.2 | 15.6 | 51.6 | . 7094 2020 | Windhoek | 86.3 | 58.0 | 28.3 | . 7095 2020 | Yakima | 70.0 | 16.1 | 53.9 | . 7096 2020 | Youngstown | 67.3 | 15.4 | 51.9 | . 7097 2020 | Zurich | 67.1 | 27.0 | 40.1 | . 7098 rows × 5 columns . As a side benefit, the complete reshaped dataframe is likely to be useful in answering other weather-related questions. Remember, though, that although the reshaped dataframe makes it easier to answer our particular question, it makes it much harder to answer different questions. Look how much more complicated the code is to find the years when the warmest day in Stockholm was colder than the coldest day in Singapore: . ( city_summary .groupby(&#39;year&#39;) .filter( lambda x : x[x[&#39;city&#39;] == &#39;Stockholm&#39;][&#39;max temp&#39;].mean() &lt; x[x[&#39;city&#39;] == &#39;Singapore&#39;][&#39;min temp&#39;].mean() ) [&#39;year&#39;] .unique() ) . array([1996, 1997, 1998, 2000, 2002, 2004, 2007, 2008, 2009, 2011, 2012, 2013, 2015, 2016, 2017, 2020]) . This is so complicated it would make this article twice as long to explain it - recall that this was a single line for the data in the original format. . Summary . In this article we have looked at two different approaches to solving a difficult data analysis problem in pandas. If we had to describe them at a high level, we might say that the first way is to write code to solve the difficult problem. The second is to write code to reshape the data so that the difficult problem becomes an easy problem, then solve the easy problem. . In the real world, this can be a very powerful problem-solving technique when working with datasets. The key is the imagination step: given a data analysis question, imagine what data structure would make that question easy to answer. Like any aspect of programming, this becomes easier with practice. . In my experience teaching data exploration with Python, the reshaping tools are the parts of pandas that are most under-used. Partly this is because they can be tricky to understand, but partly it&#39;s a matter of mindset. It can be difficult to remember that the format of a dataset when we recieve it is not set in stone; it&#39;s simply how it happens to be stored at that time. And one of the big differences I notice between novice and experienced programmers is a willingness to rearrange the data to suit the question at hand. . So next time you are faced with a tricky data analysis problem, try this strategy. Before starting to write code, try to think of a structure for the data that would make the problem easier, and see if you can get your data into that form. . If you&#39;ve made it this far, you should definitely subscribe to the Drawing from Data newsletter, follow me on Twitter, or buy the Drawing from Data book! .",
            "url": "https://drawingfromdata.com/pandas/reshaping/Making-a-difficult-data-analysis-problem-trivial.html",
            "relUrl": "/pandas/reshaping/Making-a-difficult-data-analysis-problem-trivial.html",
            "date": " • Feb 11, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About Me . I’m a freelance trainer specialising in teaching Python with a strong focus on data analysis and exploration. I trained as a biologist and completed my PhD in large-scale phylogenetics in 2007, then held a number of academic positions at the University of Edinburgh ending in a two year stint as Lecturer in Bioinformatics. I’ve been teaching and writing freelance full-time since 2015. . Contact . Want to chat? Drop me an email: martin@drawingfromdata.com .",
          "url": "https://drawingfromdata.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": ". Time to get to grips with your data . With Python, pandas and seaborn in your toolbox, you too can develop data exploration superpowers. . We’re currently in a golden age of data. It’s never been easier to assemble large datasets to probe questions in almost any field. . But these large datasets come with their own problems: . How to clean and validate data? . | How to combine datasets from multiple sources? . | And how to look for patterns in large, complex datasets and display your findings? . | . These are the questions that we all ask when we start working with these rich data sources. . The solution to these problems comes in the form of Python’s scientific software stack. The combination of a friendly, expressive language and high quality packages makes a fantastic set of tools for data exploration. . But the packages themselves can be hard to get to grips with. It’s difficult to know where to get started, or which sets of tools will be most useful. . You may have already encountered this. If you look at the matplotlib website, it’s clear that it’s a powerful charting tool - but looking at the tutorial can be daunting. The same goes for pandas: it can carry out almost any type of data manipulation, but that same power makes it hard to get to grips with. . . . . Happily, learning to use Python effectively for data exploration is a superpower that you can learn. With a basic knowledge of Python, pandas (for data manipulation) and seaborn (for data visualization) you’ll be able to understand complex datasets quickly and mine them for insight. . You’ll be able to make beautiful, informative charts for posters, papers and presentations, and rapidly update them to reflect new data or test new hypotheses. . And you’ll be able to quickly make sense of datasets from other projects - millions of rows of data will no longer be a scary prospect! . In this book, I have drawn on years of teaching experience to give you the tools you need to answer your data exploration questions. Starting with the basics, you’ll learn how to use Python, pandas, seaborn and matplotlib effectively using real world examples throughout. . . . Just the best bits of the best data exploration packages . . Rather than overwhelm you with information, the book concentrates on the most useful parts of the relevant libraries. Full color illustrations show hundreds of examples covering dozens of different chart types, with complete code samples that you can tweak and use for your own work. . The book is designed to help you get over the most common obstacles that we encounter with real world data. . You’ll learn what to do if: . your input files have errors, or missing data. . | your input files are too big to fit in memory . | you need to combine data from multiple sources . | you have to visualize datasets with thousands of rows or millions of columns . | you need to make complex filters for your data . | your data are in the wrong format for the analysis you want to do . | . Once you understand the basics of pandas, seaborn and matplotlib, the chapters on visualization will show you how to make sophisticated charts with minimal code and how to best use color to make clear charts. . . . All the chart types you’ll need . . The book covers the most basic charts that we use every day - histograms, scatter plots and boxplots - and more exotic charts like clustermaps and violin plots - all with full working code and real world examples. . . . . For getting started with data exploration in Python, the step-by-step approach offered by the book beats everything else. But the real power of Python’s data processing stack becomes apparent when you combine the individual tools in a single project. . That’s where the videos come in. . In each of the four detailed videos, we’ll take a single real world dataset and walk through all the steps involved in analyzing it, starting with publicly available data files and ending with our final figures. The videos give us a chance to explore different types of data, demonstrate tips and techniques that don’t fit in the book, and use visualization to answer interesting questions on wide-ranging topics. . . . . . . Video 1: 5000 Movie dataset . In this video we look at a dataset of 5000 movies with genres, dates and budgets . In it you’ll learn: . how to clean crowd-sourced data | how to spot artefacts caused by human error | how to deal with structured data inside columns | how to quickly spot correlations between columns | how to view trends over time | . . . . . . Video 2: Car accident reports . In this video we explore reports of 3.5 million US car accidents . In it you’ll learn: . how to reduce memory usage of large files | what to look out for when dealing with automated data collection | how to use heatmaps for visualizing temporal data | how to use GeoPandas to deal with geographic data | how to visualize geographical data at multiple levels of detail | . . . . . . Video 3: Lego set components . In this video we explore Lego sets - bricks, colors and metadata . In it you’ll learn: . how to combine information that’s spread across multiple files | how to deal with complex many-to-many relationships | how to look for categorical patterns across time | how to use color thematically | how to visualize heirarchical categories | . . . . . . Video 4: Software developer survey . In this video we take a look at the results of a career survey of 100,000 developers . In it you’ll learn: . how to deal effectively with survey data | how to rapidly understand the structure of a new dataset | how to visualize multiple overlapping categories | how to visualize and interpret rank data | how to optimize small multiple charts for readability | . . Just like in the book, all the video files come with working code in Jupyter notebooks that you can use to run the same analysis live, or tweak for your own work. . . . . If you work with data of any size, getting to grips with data exploration in Python is one of the biggest boosts you can make to your work and your career. . Free your data from Excel! Transform your messy spreadsheets into clean, tidy tables! Understand your data better! . Ready to get started? Check out the packages below (and if you want to see a complete chapter list, scroll down to the bottom of the page). . All of the packages come with a no-questions-asked refund policy so you can buy with confidence. My goal with these books and videos is to make them directly useful to you - if they don’t work for you, you can get your money back right away. . You can pay securely with a credit card or with Paypal. . . . . The complete package: bookshelf plus videos . Everything for complete beginners: the Drawing from Data book, the Python for complete beginners book, and all the videos . . This is the complete package, intended for those who’ve never used Python before, or who want a refresher before diving in to the data exploration material. . It includes my first book Python for complete beginners, which will take you from the very basics of programming and Python to give you all the background you’ll need to follow everything in the Drawing from Data book. . These two books come as searchable, DRM-free PDF files that you can keep forever and read on any device, along with exercise and example files to practice on. . You’ll also get the dataset walkthrough videos as high definition files that you can download and watch on any device. The videos come with interactive Jupyter notebook files that you can run, edit, and reuse for your own datasets. . Your files will be delivered by email, so you can start reading (and watching) right away. And you’ll get free access to any updates to the books (for example, when code changes to reflect changes in the libraries) and videos (for example, when new videos are added). . . Buy the complete package now for $169 . . . . The bookshelf package . The Drawing from Data book plus the Python for complete beginners book, and all the videos . . Don’t like videos? This package includes Python for complete beginners along with the Drawing from Data book. . These two books come as searchable, DRM-free PDF files that you can keep forever and read on any device, along with exercise and example files to practice on. . Your files will be delivered by email, so you can start reading right away. And you’ll get free access to any updates to the books (for example, when code changes to reflect changes in the libraries). . . Buy the bookshelf package now for $79 . . . . Just the book . . Don’t like videos and already know the basic of Python? This package includes just the Drawing from Data book. . These two books come as searchable, DRM-free PDF files that you can keep forever and read on any device, along with exercise and example files to practice on. . Your files will be delivered by email, so you can start reading right away. And you’ll get free access to any updates to the book (for example, when code changes to reflect changes in the libraries). . Buy the book now for $39 . . . Frequently asked questions: . What’s covered in the book? . Rather than trying to give an exhaustive tour pandas, seaborn, matplotlib and numpy (each of which could fill several books), the book concentrates on the most useful parts of each library. Here’s a complete list of chapters and topics: . getting data into pandas: series and dataframes, read_csv, dtypes/info/describe/head/len, missing data, giving column names | working with series: getting a single column, descriptive statistics, value_counts, broadcasting operations, numpy vectorized ufuncs, string methods, selecting multiple columns, setting an index, dropna, | filtering and selecting: boolean masks, basic filtering, isin, selecting with strings, multiple conditions, filter/select/aggregate pattern | pandas examples: turning series into list, iterating over unique values | intro to seaborn and plotting distributions: imports, distplot, setting title, distplot in a loop, relplot, hue and size for numerical values, adding a new column and plotting | special types of scatter plots: alpha/size/sample/hexbin/contour for large numbers, lmplot for regressions, pairwise plots | conditioning charts on categories: hue with category, size for category, style for category, small multiples with row/column, multiples with relplot | categorical axes wtih seaborn: about different types of categories, strip plots, swarm plots, multiples, box plots, problems with order, boxplots hide distribution details, violin plots, boxen plots, bar plots, point plots, line charts, count plots | styling figures with seaborn: three levels (seaborn high. seaborn low, matplotlib), aspect/height, labels, styles and contexts, passing keywords, | working with colours in seaborn: setting single colours, setting palettes, sequential data, don’t use rainbows, custom sequential, categorical palettes, redundant information, diverging palettes, when to use colour, matching between charts, colour plus style, displaying redundant catfor book layoutegories, adding metadata, highlighting categories, using colours consistently, don’t reuse | working with groups in pandas: types of categories, uniqueness, groupby, grouping on multiple columns, aggregating, turning into dataframes, filtering, transforming, iterating over groups, sorting to get group names | binning data: simple binary with filter and replace, increased options for visualisation, using pd.cut, range, uneven bins, exponential bins, using size for ordered categories, turning categories into ordered ones. | Long vs wide form and indices: why long data is best for visualization, using unstack to make a summary, using melt to get data into tidy form, setting an index, index performance, multi indexing, index slicing and loc | Matrix charts: displaying summary tables, setting scales, annotation, missing data, lots of categories, diverging palettes, binning, custom annotation, clustering walkthrough, row/col cluster, normalizing, color annotation, bubble grids, plotting context | Tricky data files in pandas: awkward input files, skiprows, setting names, using columns, footers, comments, thousands/decimals, boolean values, method chaining, assign and pipe, concat, adding a column by assigning a series, merging, inner/outer, dealing with large datasets, skipping columns, categories, precision | Using facet grids directly: making a facetgrid, using map for simple things, using map_dataframe with custom functions, reusing custom functions, using hue for single charts, small multiple heat maps | Unexpected behaviours: missing groups in groupby, fixing it with categories, breaking it with multiple grouping, workarounds, unsorted categories after unstacking, min_count in sum, odd scales in seaborn | High performance pandas: looping vs. vectorization, looping vs. apply, apply on dataframes, caching, replace and memoization, sampling large datasets, categories, indices, unique indices | Further reading: datetimes, alternative syntax, plotting directly from pandas, jupyter widgets, bokeh, machine learning, statistics | . Do I need to know Python in order to follow the book? . Yes - you don’t need to be a Python expert, but you do need to know the basics: the book isn’t intended for complete beginners. If you’ve never used Python before, you’ll need to learn the core of the language first. You can do this by buying one of the bookshelf packages (which includes Python for complete beginners, which is intended, as the name suggests, for complete beginners), or following various introductory Python tutorials online. . Who are you? . Hi, I’m Martin - formerly a bioinformatics researcher and lecturer at the University of Edinburgh; now a freelance Python writer and trainer. I’ve been using and teaching Python, with an emphasis on data exploration, for about ten years now. . Can I buy a group license for my team/office/research group/startup? . Sure, drop me an email martin@drawingfromdata.com and we’ll work something out. .",
          "url": "https://drawingfromdata.com/book/",
          "relUrl": "/book/",
          "date": ""
      }
      
  

  

  
      ,"page4": {
          "title": "Newsletter",
          "content": "Get the Drawing from Data newsletter . Sign up below to get an email with links to useful Python data science articles here and elsewhere. No spam ever, and an email every few weeks. . Get Python data analysis links and articles in your inbox . . . Email address Sign up",
          "url": "https://drawingfromdata.com/newsletter/",
          "relUrl": "/newsletter/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://drawingfromdata.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}