{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Why does my memory usage explode when concatenating dataframes?\"\n",
    "> \"Storing string columns as categories can result in massive memory savings when working with large dataframes. However, those savings can surprisingly disappear when we start concatenating dataframes.\"\n",
    "\n",
    "- toc: true\n",
    "- badges: false\n",
    "- comments: false\n",
    "- author: Martin\n",
    "- categories: [pandas, concat, memory]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tldr: concatenating categorical `Series` with nonidentical categories gives an `object` dtype in the result, with severe memory implications. \n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In a library as large and featureful as pandas, there are bound to be surprising behaviours. In this article we will take a look at a memory issue that I've run into multiple times in real life datasets - an unexpected increase in memory usage when concatenating multiple dataframes. \n",
    "\n",
    "As always, to keep this article to a reasonable size we won't be going in to too much detail on the methods involved. All of the methods we'll use here are covered in detail in the [Drawing from Data book](https://drawingfromdata.com/book/). In particular, check out chapter 2 for a discussion of data types and chapter 16 for a discussion of the memory implications.\n",
    "\n",
    "If you're interested in more articles and tips on data exploration with Python, you should subscribe to the [Drawing from Data newsletter](https://drawingfromdata.com/newsletter/) or [follow me on Twitter](https://twitter.com/DataDrawing).\n",
    "\n",
    "### Saving memory on a single data file\n",
    "\n",
    "As an example, we'll be using a slightly modified version of this car accident dataset:\n",
    "\n",
    "https://www.kaggle.com/sobhanmoosavi/us-accidents\n",
    "\n",
    "This dataset contains records of car accidents in the USA, and for the purposes of this story we have one file per month. Our goal is to combine all of these to make one large dataframe that we can use for analysis. Let's load up the first file to see the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>El Cerrito</td>\n",
       "      <td>CA</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Berkeley</td>\n",
       "      <td>CA</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oakley</td>\n",
       "      <td>CA</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Richmond</td>\n",
       "      <td>CA</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El Cerrito</td>\n",
       "      <td>CA</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301919</th>\n",
       "      <td>Sun Valley</td>\n",
       "      <td>CA</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301920</th>\n",
       "      <td>Costa Mesa</td>\n",
       "      <td>CA</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301921</th>\n",
       "      <td>Costa Mesa</td>\n",
       "      <td>CA</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301922</th>\n",
       "      <td>Madras</td>\n",
       "      <td>OR</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301923</th>\n",
       "      <td>Jordan Valley</td>\n",
       "      <td>OR</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301924 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 City State    month\n",
       "0          El Cerrito    CA  January\n",
       "1            Berkeley    CA  January\n",
       "2              Oakley    CA  January\n",
       "3            Richmond    CA  January\n",
       "4          El Cerrito    CA  January\n",
       "...               ...   ...      ...\n",
       "301919     Sun Valley    CA  January\n",
       "301920     Costa Mesa    CA  January\n",
       "301921     Costa Mesa    CA  January\n",
       "301922         Madras    OR  January\n",
       "301923  Jordan Valley    OR  January\n",
       "\n",
       "[301924 rows x 3 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load just the data for January\n",
    "df = pd.read_csv('January_car_accidents.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty straightforward - we have columns for City, State and Month, and around 300,000 rows. Each row represents a single car accident. In the original dataset, of course, there are many more columns. For this article we are interested in the State column. \n",
    "\n",
    "Let's assume that because this is a large dataset, we're worried about running out of memory. We know that for columns with a small number of values, storing them as a categorical data type can save a lot of memory (see chapter 16 in the [Drawing from Data book](drawingfromdata.com/book) for a full discussion of memory issues). Let's check how much memory the State column uses when stored as `object`. Because `memory_usage` returns the result in bytes, we'll divide by 1e6 to get an answer in megabytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.813644"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['State'].memory_usage(deep=True) / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same column as a category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.305956"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['State'].astype('category').memory_usage(deep=True) / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, a considerable saving. Once we've figured this out, it's probably a good habit to specify a categorical data types when reading in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index     0.000128\n",
       "City     19.861495\n",
       "State     0.305828\n",
       "month    19.323136\n",
       "dtype: float64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('January_car_accidents.csv', dtype={'State' : 'category'})\n",
    "df.memory_usage(deep=True) / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incidentally, the City and month columns would also be better stored as categories, but we will ignore that for now.\n",
    "\n",
    "## Concatenating multiple data files\n",
    "\n",
    "Let's now make a loop to read all 12 data files, and check the memory usage for each, remembering to keep the 'category' dtype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March_car_accidents.csv 0.29748\n",
      "April_car_accidents.csv 0.30353\n",
      "September_car_accidents.csv 0.296511\n",
      "February_car_accidents.csv 0.288488\n",
      "May_car_accidents.csv 0.300636\n",
      "January_car_accidents.csv 0.305956\n",
      "July_car_accidents.csv 0.227059\n",
      "June_car_accidents.csv 0.314354\n",
      "November_car_accidents.csv 0.303147\n",
      "December_car_accidents.csv 0.303714\n",
      "October_car_accidents.csv 0.328636\n",
      "August_car_accidents.csv 0.293021\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "for filename in glob.glob('*car_accidents.csv'):\n",
    "    month_df = pd.read_csv(filename, dtype = {'State' : 'category'})\n",
    "    print(filename, month_df['State'].memory_usage(deep=True) / 1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the exact numbers differ, but each of our single-month data files takes around 0.3 Mb for the State column. Our estimate for the memory usage of the State column in our combined dataframe, therefore, is 0.3 * 12: around 3.6 Mb. \n",
    "\n",
    "The best way to combine these data files is to make a list of dataframes then concatenate them at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Columbus</td>\n",
       "      <td>OH</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Miamisburg</td>\n",
       "      <td>OH</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dayton</td>\n",
       "      <td>OH</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Columbus</td>\n",
       "      <td>OH</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Columbus</td>\n",
       "      <td>OH</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288925</th>\n",
       "      <td>Riverside</td>\n",
       "      <td>CA</td>\n",
       "      <td>August</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288926</th>\n",
       "      <td>San Diego</td>\n",
       "      <td>CA</td>\n",
       "      <td>August</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288927</th>\n",
       "      <td>Orange</td>\n",
       "      <td>CA</td>\n",
       "      <td>August</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288928</th>\n",
       "      <td>Culver City</td>\n",
       "      <td>CA</td>\n",
       "      <td>August</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288929</th>\n",
       "      <td>Highland</td>\n",
       "      <td>CA</td>\n",
       "      <td>August</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3513617 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               City State   month\n",
       "0          Columbus    OH   March\n",
       "1        Miamisburg    OH   March\n",
       "2            Dayton    OH   March\n",
       "3          Columbus    OH   March\n",
       "4          Columbus    OH   March\n",
       "...             ...   ...     ...\n",
       "288925    Riverside    CA  August\n",
       "288926    San Diego    CA  August\n",
       "288927       Orange    CA  August\n",
       "288928  Culver City    CA  August\n",
       "288929     Highland    CA  August\n",
       "\n",
       "[3513617 rows x 3 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "for filename in glob.glob('*car_accidents.csv'):\n",
    "    month_df = pd.read_csv(filename, dtype = {'State' : 'category'})\n",
    "    all_dfs.append(month_df)\n",
    "    \n",
    "big_df = pd.concat(all_dfs)\n",
    "big_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All looks fine here: for our combined dataset we have around 3.5 million rows. But take a look at the memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.412339"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df['State'].memory_usage(deep=True) / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Far from taking up less than 4 Mb as expected, our State column is taking up more than 200 Mb. What has happened? The clue is in the data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3513617 entries, 0 to 288929\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   City    object\n",
      " 1   State   object\n",
      " 2   month   object\n",
      "dtypes: object(3)\n",
      "memory usage: 107.2+ MB\n"
     ]
    }
   ],
   "source": [
    "big_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have lost our categorical type; the State colum in the big dataframe has been turned back into `object`. The reason: **not every state is present in every single-month data file**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month\n",
       "April        48\n",
       "August       49\n",
       "December     49\n",
       "February     49\n",
       "January      48\n",
       "July         49\n",
       "June         48\n",
       "March        49\n",
       "May          49\n",
       "November     49\n",
       "October      49\n",
       "September    49\n",
       "Name: State, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df.groupby('month')['State'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most months have records for 49 states, but a few have records for only 48. We can find the missing states quite easily with a mixture of pandas and Python's built in set type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ND'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "april = big_df[big_df['month'] == 'April']\n",
    "august = big_df[big_df['month'] == 'August']\n",
    "\n",
    "# which state has records for August but not April\n",
    "set(august['State']) - set(april['State']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing states mean that the categories are slightly different between the individual month dataframes, and when we ask pandas to concatenate categorical datasets with different categories, it sets the data type back to `object`. We can see this behaviour with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     apple\n",
       "1    banana\n",
       "2    orange\n",
       "0     apple\n",
       "1    banana\n",
       "2    banana\n",
       "dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_series_one = pd.Series(['apple', 'banana', 'orange']).astype('category')\n",
    "cat_series_two = pd.Series(['apple', 'banana', 'banana']).astype('category')\n",
    "\n",
    "combined_series = pd.concat([cat_series_one, cat_series_two])\n",
    "\n",
    "# look at the dtype for the combined series\n",
    "combined_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will encounter this problem any time we use `pd.concat` on categorical columns where the categories are not exactly identical. One such scenario is the one we've just looked at: when we need to combine multiple data files. \n",
    "\n",
    "### Concatenation and chunking\n",
    "\n",
    "Another is when we use chunking to read a data file one piece at a time. The canonical way to read a large CSV file in chunks is to append the chunks to a list then use `pd.concat` on the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 301924 entries, 0 to 301923\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   City    301914 non-null  object\n",
      " 1   State   301924 non-null  object\n",
      " 2   month   301924 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "# process 100,000 rows at a time\n",
    "for chunk in pd.read_csv('January_car_accidents.csv', dtype={'State' : 'category'}, chunksize=100000):\n",
    "    # possibly do some processing on chunk\n",
    "    all_dfs.append(chunk)\n",
    "    \n",
    "pd.concat(all_dfs).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run into the same issue. Although we have specified a categorical data type for the State column for each chunk, when we combine them the State column gets turned back into an `object`, since each chunk has a slightly different set of unique states in it. \n",
    "\n",
    "This version of the problem is especially harsh, as the most common reason for chunking input files in the first place is to reduce peak memory usage! So it's an especially nasty surprise when the memory usage of the State column turns out to be so much higher than our estimate.\n",
    "\n",
    "\n",
    "## Fixing the problem\n",
    "\n",
    "We can get round this problem in a number of ways. If we have enough memory, we can simply take our combined dataframe and change the State column to a category after it's been assembled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index     28.108936\n",
       "City     231.140089\n",
       "State      3.517580\n",
       "month    222.078650\n",
       "dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df['State'] = big_df['State'].astype('category')\n",
    "big_df.memory_usage(deep=True) / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gets the State column memory usage back down to our estimate of around 3.5 Mb. \n",
    "\n",
    "If we haven't got enough memory to do this, we have to force all the single-month dataframe State columns to have identical categories. We can do this by explicitly creating a categorical data type and listing the categories that we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL',\n",
       "                  'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME',\n",
       "                  'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH',\n",
       "                  'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI',\n",
       "                  'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI',\n",
       "                  'WY'],\n",
       ", ordered=False)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# just a list of Python strings\n",
    "states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "\n",
    "state_type = CategoricalDtype(categories=states)\n",
    "state_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we create a series with this type, it will always have all the states as its categories regardless of which states are actually in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    KS\n",
       "1    TX\n",
       "2    VT\n",
       "dtype: category\n",
       "Categories (51, object): ['AL', 'AK', 'AZ', 'AR', ..., 'WA', 'WV', 'WI', 'WY']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['KS', 'TX', 'VT']).astype(state_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the above series has all 51 states in its categories even though there are only 3 in the data. \n",
    "\n",
    "Using this new type in our input loop results in the categorical data type being preserved after concatenation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3513617 entries, 0 to 288929\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Dtype   \n",
      "---  ------  -----   \n",
      " 0   City    object  \n",
      " 1   State   category\n",
      " 2   month   object  \n",
      "dtypes: category(1), object(2)\n",
      "memory usage: 83.8+ MB\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "for filename in glob.glob('*car_accidents.csv'):\n",
    "    month_df = pd.read_csv(filename, dtype = {'State' : state_type})\n",
    "    all_dfs.append(month_df)\n",
    "    \n",
    "big_df = pd.concat(all_dfs)\n",
    "big_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case I've just copied an existing Python [list of state abbreviations that I found online](https://gist.github.com/JeffPaine/3083347). In other situations it might be possible to generate the list by reading all the input files one line at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'WY', 'NY', 'MS', 'IL', 'NJ', 'MA', 'DC', 'DE', 'UT', 'WA', 'OK', 'RI', 'CO', 'IN', 'TX', 'AZ', 'WI', 'TN', 'FL', 'IA', 'SC', 'ME', 'ND', 'NC', 'MD', 'SD', 'NE', 'KS', 'PA', 'GA', 'OR', 'MT', 'KY', 'NH', 'ID', 'MN', 'OH', 'VA', 'MI', 'NM', 'CT', 'CA', 'MO', 'AL', 'WV', 'AR', 'NV', 'LA', 'VT'}\n"
     ]
    }
   ],
   "source": [
    "all_states = set()\n",
    "\n",
    "for filename in glob.glob('*car_accidents.csv'):\n",
    "    for line in open(filename):\n",
    "        state = line.split(',')[1]\n",
    "        all_states.add(state)\n",
    "\n",
    "# remove the column name\n",
    "all_states.remove('State')\n",
    "print(all_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this takes a while to run, it's very memory-friendly as we only have to store a single row of each input file in memory at a time. \n",
    "\n",
    "If you've made it this far, you should definitely subscribe to the [Drawing from Data newsletter](https://drawingfromdata.com/newsletter/), [follow me on Twitter](https://twitter.com/DataDrawing), or buy the [Drawing from Data book](https://drawingfromdata.com/book/)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drawingfromdata",
   "language": "python",
   "name": "drawingfromdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
